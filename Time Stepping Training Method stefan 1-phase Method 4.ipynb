{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4ac27d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.init as init\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8819113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_torch(arr):\n",
    "    \n",
    "    arr = torch.FloatTensor(arr)\n",
    "    arr = arr.unsqueeze(-1)\n",
    "    arr = arr.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def x_train_data(N_x, x_l, x_r, N_bc, s, t_test, del_t):\n",
    "    \n",
    "    x_train = np.linspace(x_l,x_r,N_x)   \n",
    "\n",
    "    for i in range(N_x):\n",
    "        if x_train[i]>s:\n",
    "            break\n",
    "\n",
    "#     x_bc1 = np.ones(N_bc)*x_l\n",
    "#     x_train = np.concatenate((x_train, x_bc1),0)\n",
    "    x_train = np_to_torch(x_train)\n",
    "    N_xl = torch.sum( torch.where(x_train == x_l,1,0) ).detach().numpy().item()\n",
    "    \n",
    "    return x_train, N_xl, i\n",
    "\n",
    "def initial_temp(N_x, N_bc, T_l, T_r, N_x_test, N_s, N_s_test):\n",
    "    \n",
    "    T_prev_1 = np.concatenate((np.linspace(T_l,T_r,N_s), np.ones(N_x - N_s)*T_r),0)\n",
    "    T_prev = np_to_torch(T_prev_1)\n",
    "#     T_prev_2 = np.ones(N_bc)*T_l\n",
    "#     T_prev = np.concatenate((T_prev_1, T_prev_2),0)\n",
    "#     T_prev = np_to_torch(T_prev)\n",
    "    \n",
    "    T_test_prev = np.concatenate((np.linspace(T_l,T_r,N_s_test), np.ones(N_x_test - N_s)*T_r),0)\n",
    "    T_test_prev = np.reshape(T_test_prev,(N_x_test, 1))\n",
    "    \n",
    "    return T_prev, T_test_prev\n",
    "\n",
    "def initial_interface(N, s):\n",
    "\n",
    "    s_interface = torch.full((N, 1), s)\n",
    "    s_interface = s_interface.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    return s_interface, s_interface\n",
    "            \n",
    "def xavier_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0)\n",
    "    \n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, layer_size):\n",
    "        super(ANN, self).__init__()\n",
    "        \n",
    "        # Fully conected model\n",
    "        modules = []\n",
    "        for i in range(len(layer_size) - 2):\n",
    "            modules.append(nn.Linear(layer_size[i], layer_size[i+1]))  \n",
    "            modules.append(nn.Tanh())\n",
    "        modules.append(nn.Linear(layer_size[-2], layer_size[-1])) \n",
    "#         modules.append(nn.ReLU())\n",
    "\n",
    "        self.fc = nn.Sequential(*modules)\n",
    "        for layer in self.fc.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                 layer.weight.data.normal_(mean=0, std=0.2)\n",
    "#         self.fc.apply(xavier_init)\n",
    "        \n",
    "    def forward(self, x_train, k2, del_t, s_ini, t_test, dTsds_prev):\n",
    "        op = self.fc( x_train )\n",
    "        op_x = torch.autograd.grad(op, x_train, grad_outputs=torch.ones_like(op), create_graph=True)[0]\n",
    "        op_x2 = torch.autograd.grad(op_x, x_train, grad_outputs=torch.ones_like(op_x), create_graph=True)[0]\n",
    "        \n",
    "        op_s = self.fc( s_ini )\n",
    "        op_s_x = torch.autograd.grad(op_s, s_ini, grad_outputs=torch.ones_like(op_s), create_graph=True)[0]\n",
    "        \n",
    "        c_prev = 0\n",
    "        c_new = 1 - c_prev\n",
    "            \n",
    "        s_new = s_ini - del_t*k2*(c_new*op_s_x + c_prev*dTsds_prev)\n",
    "        op_s_new = self.fc( s_new )\n",
    "        op_s_new_x = torch.autograd.grad(op_s_new, s_new, grad_outputs=torch.ones_like(op_s_new), create_graph=True)[0]\n",
    "        op_s_new_x2 = torch.autograd.grad(op_s_new_x, s_new, grad_outputs=torch.ones_like(op_s_new), create_graph=True)[0]\n",
    "\n",
    "        return op, op_x2, op_s_new, s_new, op_s_x, op_s_new_x2\n",
    "    \n",
    "def get_loss(x_train, k1, k2, N_tot, T_l, T_r, N_xl, x_l, x_r, T_prev, del_t, t_test, s_ini, dTsds_prev):\n",
    "    \n",
    "    mse = nn.MSELoss(reduction='sum')\n",
    "    w1 = 1\n",
    "    w2 = 1\n",
    "    w3 = 1\n",
    "        \n",
    "#     if(t_test == del_t):\n",
    "#         w2 = 4\n",
    "        \n",
    "    T, d2Tdx2, Ts, s_new, _, d2Tdx2_s_new  = model(x_train, k2, del_t, s_ini, t_test, dTsds_prev)\n",
    "    N1 = torch.sum(torch.where(x_train <= s_new,1,0)).detach().numpy().item()\n",
    "    eq1 = w1*( torch.sum( torch.square( torch.mul(torch.where(x_train <= s_new,1,0),T - T_prev - del_t*k1*d2Tdx2 ) ) ) )/(N1)\n",
    "    bc1 = w2*torch.sum( torch.square( torch.mul(torch.where(x_train == x_l,1,0),(T - T_l)) ) )/(N_xl)\n",
    "    bc2 = w3*torch.sum( torch.square( T_r - Ts ) )\n",
    "\n",
    "    loss = eq1 + bc1 + bc2   \n",
    "    \n",
    "    return loss, eq1, bc1, bc2\n",
    "\n",
    "def print_loss(epoch, loss, eq1, bc1, bc2):\n",
    "    print('epoch = ',epoch)\n",
    "    print('loss = ',loss.detach().numpy())\n",
    "    print('eq1_loss = ',eq1.detach().numpy())\n",
    "    print('bc1_loss = ',bc1.detach().numpy())\n",
    "    print('bc2_loss = ',bc2.detach().numpy())\n",
    "\n",
    "def interface_identifier(y_pred, T_r, N, x_test, s):\n",
    "    \n",
    "    for i in range(N):\n",
    "        if x_test[i]>s:\n",
    "            break\n",
    "            \n",
    "    for j in range(i,N):\n",
    "        y_pred[j] = T_r\n",
    "\n",
    "    return y_pred, s\n",
    "\n",
    "def lamb_analytical(k1, k2):\n",
    "    x = []\n",
    "    er = []\n",
    "    cnt = 0\n",
    "    for i in np.arange(0.1, 5, 0.001):\n",
    "        x.append(i)\n",
    "        er.append(math.erf(x[-1]))\n",
    "        cnt = cnt+1\n",
    "\n",
    "    x = np.array(x)\n",
    "    er = np.array(er)\n",
    "    y =[]\n",
    "    y = np.exp(-x*x)/(er*math.sqrt(math.pi))-x*k1/k2\n",
    "\n",
    "    for i in range(1,cnt):\n",
    "        if(y[i]*y[i-1]<0):\n",
    "            lam = x[i]\n",
    "            break\n",
    "    \n",
    "    return lam\n",
    "\n",
    "def analytical(N_x_test, x_test, t_test, T_r, k1, k2, T_l):\n",
    "\n",
    "    x_test = x_test.detach().numpy()\n",
    "    y_an = np.zeros((N_x_test, 1))\n",
    "    lam = lamb_analytical(k1, k2)\n",
    "    s = np.sqrt(k1*t_test)*2*lam\n",
    "    \n",
    "    for j in range(N_x_test):\n",
    "        if(x_test[j]<s):\n",
    "            y_an[j] = T_l - T_l*math.erf( x_test[j]/( 2*np.sqrt(k1*t_test) ) )/ math.erf(lam) \n",
    "        else:\n",
    "            y_an[j] = T_r\n",
    "            \n",
    "    y_an = np.reshape(y_an, (N_x_test, 1))\n",
    "    \n",
    "    return y_an, s\n",
    "    \n",
    "def train_model(model, optimiser1, epochs, T_r, T_l, k1, k2, N_x, x_l, x_r, N_t, N_bc, accuracy_cap, N_x_test, del_t, s_initial):\n",
    "    \n",
    "    loss_store = []\n",
    "    T_store_pred = []\n",
    "    s_store_pred = []\n",
    "    T_store_an = []\n",
    "    s_store_an = []\n",
    "    t_store = []\n",
    "    mse = nn.MSELoss(reduction='sum')\n",
    "    model.train()  \n",
    "    \n",
    "#     N_tot = N_x + N_bc\n",
    "    N_tot = N_x\n",
    "    print(\"N_tot = \", N_tot)\n",
    "    \n",
    "    t_test = 0\n",
    "    \n",
    "    t_store.append(0)\n",
    "    s_store_an.append(0)\n",
    "    for i in range(N_t):\n",
    "        \n",
    "        t_test = t_test + del_t\n",
    "        t_store.append(t_test)\n",
    "        print(\"t = \", t_test)\n",
    "        print(\" \")\n",
    "        \n",
    "        if(i==0):\n",
    "            x_train, N_xl, N_s = x_train_data(N_x, x_l, x_r, N_bc, s_initial, t_test, del_t)\n",
    "            print(\"Ns = \", N_s)\n",
    "            T_prev, _ = initial_temp(N_x, N_bc, T_l, T_r, N_x_test, N_s, N_s)\n",
    "            s_prev, _ = initial_interface(1, s_initial)\n",
    "            T_store_pred.append(T_prev.detach().numpy())\n",
    "            s_store_pred.append(s_prev.detach().numpy())\n",
    "            slope = (T_r - T_l)/s_initial\n",
    "            print(\"slope = \", slope)\n",
    "            dTsds_prev =torch.FloatTensor( np.ones((1, 1))*slope )\n",
    "        print(\"slope at interface = \", dTsds_prev)\n",
    "#         print(T_prev)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            #Backpropogation and optimisation\n",
    "            loss, eq1, bc1, bc2 = get_loss(x_train, k1, k2, N_tot, T_l, T_r, N_xl, x_l, x_r, T_prev, del_t, t_test, s_prev, dTsds_prev)\n",
    "            optimiser1.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser1.step()  \n",
    "            loss_store.append(loss.detach().numpy())\n",
    "            \n",
    "            if epoch%2000==0:\n",
    "                print_loss(epoch, loss, eq1, bc1, bc2)\n",
    "                print(\"\")\n",
    "\n",
    "            if loss<0.0004 :\n",
    "                print(\"loss limit attained, epoch = \", epoch)\n",
    "                print_loss(epoch, loss, eq1, bc1, bc2)\n",
    "                print(\"\")\n",
    "                break\n",
    "                    \n",
    "        # Store the results after each time step\n",
    "        T_prev,_,_,s_prev,dTsds_prev,_ = model(x_train, k2, del_t, s_prev, t_test, dTsds_prev) \n",
    "        dTsds_prev = dTsds_prev.clone().detach().requires_grad_(False)\n",
    "        T_prev = T_prev.detach().numpy()\n",
    "        T_prev, _ = interface_identifier(T_prev, T_r, N_tot, x_train.detach().numpy(), s_prev[0][0].detach().numpy())\n",
    "        s_prev = s_prev.detach().numpy()\n",
    "        print(\"interface_PINN = \", s_prev[0][0])\n",
    "        T_an, s_an = analytical(N_x, x_train, t_test, T_r, k1, k2, T_l)\n",
    "        print(\"interface_Analytical = \", s_an)\n",
    "        \n",
    "        T_store_pred.append(T_prev)\n",
    "        \n",
    "        T_store_an.append(T_an)\n",
    "        s_store_pred.append(s_prev)\n",
    "        s_store_an.append(s_an)\n",
    "        \n",
    "        T_prev = torch.FloatTensor(T_store_pred[-1]).clone().detach().requires_grad_(False)\n",
    "        s_prev = torch.FloatTensor(s_store_pred[-1]).clone().detach().requires_grad_(True)\n",
    "        \n",
    "        print(\"broke inner loop\")\n",
    "        print(\"\")\n",
    "\n",
    "    return loss_store, T_store_pred, T_store_an, x_train, s_store_pred, s_store_an, t_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa5eeff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=3, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=3, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Total trainable parameters in the model: 22\n",
      "N_tot =  3001\n",
      "t =  0.001\n",
      " \n",
      "Ns =  17\n",
      "slope =  -333.3333333333333\n",
      "slope at interface =  tensor([[-333.3333]])\n",
      "epoch =  0\n",
      "loss =  0.14887312\n",
      "eq1_loss =  0.023581414\n",
      "bc1_loss =  0.068620026\n",
      "bc2_loss =  0.056671683\n",
      "\n",
      "epoch =  2000\n",
      "loss =  0.14843692\n",
      "eq1_loss =  0.023437409\n",
      "bc1_loss =  0.062499806\n",
      "bc2_loss =  0.062499702\n",
      "\n",
      "epoch =  4000\n",
      "loss =  0.1484242\n",
      "eq1_loss =  0.023435405\n",
      "bc1_loss =  0.062493995\n",
      "bc2_loss =  0.062494792\n",
      "\n",
      "epoch =  6000\n",
      "loss =  0.14815116\n",
      "eq1_loss =  0.023392532\n",
      "bc1_loss =  0.062383942\n",
      "bc2_loss =  0.062374685\n",
      "\n",
      "epoch =  8000\n",
      "loss =  0.14588083\n",
      "eq1_loss =  0.023041045\n",
      "bc1_loss =  0.061425604\n",
      "bc2_loss =  0.061414186\n",
      "\n",
      "epoch =  10000\n",
      "loss =  0.14100185\n",
      "eq1_loss =  0.022313738\n",
      "bc1_loss =  0.05939669\n",
      "bc2_loss =  0.05929142\n",
      "\n",
      "epoch =  12000\n",
      "loss =  0.13494278\n",
      "eq1_loss =  0.023041427\n",
      "bc1_loss =  0.05822236\n",
      "bc2_loss =  0.053679\n",
      "\n",
      "epoch =  14000\n",
      "loss =  0.124274075\n",
      "eq1_loss =  0.022679502\n",
      "bc1_loss =  0.05491197\n",
      "bc2_loss =  0.046682604\n",
      "\n",
      "epoch =  16000\n",
      "loss =  0.108961836\n",
      "eq1_loss =  0.021918617\n",
      "bc1_loss =  0.05023259\n",
      "bc2_loss =  0.036810633\n",
      "\n",
      "epoch =  18000\n",
      "loss =  0.08732891\n",
      "eq1_loss =  0.019547813\n",
      "bc1_loss =  0.04196365\n",
      "bc2_loss =  0.025817448\n",
      "\n",
      "epoch =  20000\n",
      "loss =  0.06131158\n",
      "eq1_loss =  0.01636031\n",
      "bc1_loss =  0.030621782\n",
      "bc2_loss =  0.014329485\n",
      "\n",
      "epoch =  22000\n",
      "loss =  0.03575995\n",
      "eq1_loss =  0.013137675\n",
      "bc1_loss =  0.018041108\n",
      "bc2_loss =  0.0045811636\n",
      "\n",
      "epoch =  24000\n",
      "loss =  0.018992668\n",
      "eq1_loss =  0.011389563\n",
      "bc1_loss =  0.0074530123\n",
      "bc2_loss =  0.00015009406\n",
      "\n",
      "epoch =  26000\n",
      "loss =  0.015026432\n",
      "eq1_loss =  0.011441305\n",
      "bc1_loss =  0.0030066017\n",
      "bc2_loss =  0.00057852565\n",
      "\n",
      "epoch =  28000\n",
      "loss =  0.014641521\n",
      "eq1_loss =  0.011322768\n",
      "bc1_loss =  0.0025120438\n",
      "bc2_loss =  0.00080670917\n",
      "\n",
      "epoch =  30000\n",
      "loss =  0.014271413\n",
      "eq1_loss =  0.01107981\n",
      "bc1_loss =  0.0023885125\n",
      "bc2_loss =  0.0008030904\n",
      "\n",
      "epoch =  32000\n",
      "loss =  0.013721596\n",
      "eq1_loss =  0.010727417\n",
      "bc1_loss =  0.0022201012\n",
      "bc2_loss =  0.00077407795\n",
      "\n",
      "epoch =  34000\n",
      "loss =  0.012770489\n",
      "eq1_loss =  0.010069681\n",
      "bc1_loss =  0.0019876938\n",
      "bc2_loss =  0.00071311387\n",
      "\n",
      "epoch =  36000\n",
      "loss =  0.01133014\n",
      "eq1_loss =  0.009141733\n",
      "bc1_loss =  0.0016106003\n",
      "bc2_loss =  0.0005778062\n",
      "\n",
      "epoch =  38000\n",
      "loss =  0.0095283305\n",
      "eq1_loss =  0.008043603\n",
      "bc1_loss =  0.0010869706\n",
      "bc2_loss =  0.0003977565\n",
      "\n",
      "epoch =  40000\n",
      "loss =  0.007965244\n",
      "eq1_loss =  0.007011683\n",
      "bc1_loss =  0.0006783961\n",
      "bc2_loss =  0.00027516478\n",
      "\n",
      "epoch =  42000\n",
      "loss =  0.006557391\n",
      "eq1_loss =  0.0060722306\n",
      "bc1_loss =  0.00032448536\n",
      "bc2_loss =  0.00016067494\n",
      "\n",
      "epoch =  44000\n",
      "loss =  0.005572836\n",
      "eq1_loss =  0.0053989454\n",
      "bc1_loss =  0.000103591075\n",
      "bc2_loss =  7.029928e-05\n",
      "\n",
      "epoch =  46000\n",
      "loss =  0.005074395\n",
      "eq1_loss =  0.005045554\n",
      "bc1_loss =  1.150958e-05\n",
      "bc2_loss =  1.7330793e-05\n",
      "\n",
      "epoch =  48000\n",
      "loss =  0.0049383547\n",
      "eq1_loss =  0.0049361056\n",
      "bc1_loss =  8.2514475e-07\n",
      "bc2_loss =  1.423929e-06\n",
      "\n",
      "epoch =  50000\n",
      "loss =  0.0049280105\n",
      "eq1_loss =  0.004923027\n",
      "bc1_loss =  4.9513396e-06\n",
      "bc2_loss =  3.214518e-08\n",
      "\n",
      "epoch =  52000\n",
      "loss =  0.004926726\n",
      "eq1_loss =  0.0049228296\n",
      "bc1_loss =  3.7761454e-06\n",
      "bc2_loss =  1.2067e-07\n",
      "\n",
      "epoch =  54000\n",
      "loss =  0.0049255122\n",
      "eq1_loss =  0.004917489\n",
      "bc1_loss =  7.9048195e-06\n",
      "bc2_loss =  1.1852626e-07\n",
      "\n",
      "epoch =  56000\n",
      "loss =  0.004923666\n",
      "eq1_loss =  0.004920095\n",
      "bc1_loss =  3.4574668e-06\n",
      "bc2_loss =  1.13331225e-07\n",
      "\n",
      "epoch =  58000\n",
      "loss =  0.0049217097\n",
      "eq1_loss =  0.0049166274\n",
      "bc1_loss =  5.08217e-06\n",
      "bc2_loss =  1.2789769e-11\n",
      "\n",
      "epoch =  60000\n",
      "loss =  0.0049201907\n",
      "eq1_loss =  0.004915277\n",
      "bc1_loss =  4.9137443e-06\n",
      "bc2_loss =  4.604317e-12\n",
      "\n",
      "epoch =  62000\n",
      "loss =  0.0049186694\n",
      "eq1_loss =  0.004913619\n",
      "bc1_loss =  5.0472936e-06\n",
      "bc2_loss =  3.4399363e-09\n",
      "\n",
      "epoch =  64000\n",
      "loss =  0.004917534\n",
      "eq1_loss =  0.004910551\n",
      "bc1_loss =  6.8455774e-06\n",
      "bc2_loss =  1.3744881e-07\n",
      "\n",
      "epoch =  66000\n",
      "loss =  0.004915639\n",
      "eq1_loss =  0.004910829\n",
      "bc1_loss =  4.8028633e-06\n",
      "bc2_loss =  7.003166e-09\n",
      "\n",
      "epoch =  68000\n",
      "loss =  0.0049140984\n",
      "eq1_loss =  0.004909123\n",
      "bc1_loss =  4.955054e-06\n",
      "bc2_loss =  2.046363e-08\n",
      "\n",
      "epoch =  70000\n",
      "loss =  0.004912523\n",
      "eq1_loss =  0.004907713\n",
      "bc1_loss =  4.788244e-06\n",
      "bc2_loss =  2.1921153e-08\n",
      "\n",
      "epoch =  72000\n",
      "loss =  0.0049109003\n",
      "eq1_loss =  0.004906237\n",
      "bc1_loss =  4.638657e-06\n",
      "bc2_loss =  2.4536405e-08\n",
      "\n",
      "epoch =  74000\n",
      "loss =  0.004909235\n",
      "eq1_loss =  0.00490452\n",
      "bc1_loss =  4.6757027e-06\n",
      "bc2_loss =  3.9253848e-08\n",
      "\n",
      "epoch =  76000\n",
      "loss =  0.004907487\n",
      "eq1_loss =  0.0049024983\n",
      "bc1_loss =  4.9126875e-06\n",
      "bc2_loss =  7.583054e-08\n",
      "\n",
      "epoch =  78000\n",
      "loss =  0.0049056364\n",
      "eq1_loss =  0.004900886\n",
      "bc1_loss =  4.676218e-06\n",
      "bc2_loss =  7.413314e-08\n",
      "\n",
      "epoch =  80000\n",
      "loss =  0.0049037016\n",
      "eq1_loss =  0.004898864\n",
      "bc1_loss =  4.73414e-06\n",
      "bc2_loss =  1.03750665e-07\n",
      "\n",
      "epoch =  82000\n",
      "loss =  0.0049016643\n",
      "eq1_loss =  0.0048967353\n",
      "bc1_loss =  4.7898097e-06\n",
      "bc2_loss =  1.394003e-07\n",
      "\n",
      "epoch =  84000\n",
      "loss =  0.004899529\n",
      "eq1_loss =  0.0048940363\n",
      "bc1_loss =  5.2479704e-06\n",
      "bc2_loss =  2.445106e-07\n",
      "\n",
      "epoch =  86000\n",
      "loss =  0.00489709\n",
      "eq1_loss =  0.0048925667\n",
      "bc1_loss =  4.3699984e-06\n",
      "bc2_loss =  1.5307256e-07\n",
      "\n",
      "epoch =  88000\n",
      "loss =  0.004894571\n",
      "eq1_loss =  0.004890074\n",
      "bc1_loss =  4.3103946e-06\n",
      "bc2_loss =  1.8622472e-07\n",
      "\n",
      "epoch =  90000\n",
      "loss =  0.0048918356\n",
      "eq1_loss =  0.0048873713\n",
      "bc1_loss =  4.240392e-06\n",
      "bc2_loss =  2.2420159e-07\n",
      "\n",
      "interface_PINN =  0.00301695\n",
      "interface_Analytical =  0.0037820840815613846\n",
      "broke inner loop\n",
      "\n",
      "t =  0.002\n",
      " \n",
      "slope at interface =  tensor([[-166.3322]])\n",
      "epoch =  0\n",
      "loss =  0.05982088\n",
      "eq1_loss =  0.023778325\n",
      "bc1_loss =  4.2031616e-06\n",
      "bc2_loss =  0.03603835\n",
      "\n",
      "epoch =  2000\n",
      "loss =  0.0021925482\n",
      "eq1_loss =  0.0018045404\n",
      "bc1_loss =  0.00031552906\n",
      "bc2_loss =  7.247884e-05\n",
      "\n",
      "epoch =  4000\n",
      "loss =  0.0019313941\n",
      "eq1_loss =  0.0016327387\n",
      "bc1_loss =  0.00024974922\n",
      "bc2_loss =  4.8906157e-05\n",
      "\n",
      "epoch =  6000\n",
      "loss =  0.0015560426\n",
      "eq1_loss =  0.0013820983\n",
      "bc1_loss =  0.00016047555\n",
      "bc2_loss =  1.34687325e-05\n",
      "\n",
      "epoch =  8000\n",
      "loss =  0.001253801\n",
      "eq1_loss =  0.0011860264\n",
      "bc1_loss =  6.769711e-05\n",
      "bc2_loss =  7.7547156e-08\n",
      "\n",
      "epoch =  10000\n",
      "loss =  0.0011642571\n",
      "eq1_loss =  0.001133998\n",
      "bc1_loss =  2.1937354e-05\n",
      "bc2_loss =  8.321757e-06\n",
      "\n",
      "epoch =  12000\n",
      "loss =  0.001150172\n",
      "eq1_loss =  0.0011231618\n",
      "bc1_loss =  1.6660488e-05\n",
      "bc2_loss =  1.034974e-05\n",
      "\n",
      "epoch =  14000\n",
      "loss =  0.0011318886\n",
      "eq1_loss =  0.001106001\n",
      "bc1_loss =  1.649062e-05\n",
      "bc2_loss =  9.3970875e-06\n",
      "\n",
      "epoch =  16000\n",
      "loss =  0.0011079118\n",
      "eq1_loss =  0.0010825034\n",
      "bc1_loss =  1.8446952e-05\n",
      "bc2_loss =  6.961467e-06\n",
      "\n",
      "epoch =  18000\n",
      "loss =  0.0010776691\n",
      "eq1_loss =  0.0010556376\n",
      "bc1_loss =  1.5055511e-05\n",
      "bc2_loss =  6.9759426e-06\n",
      "\n",
      "epoch =  20000\n",
      "loss =  0.001041427\n",
      "eq1_loss =  0.0010224779\n",
      "bc1_loss =  1.3208381e-05\n",
      "bc2_loss =  5.740756e-06\n",
      "\n",
      "epoch =  22000\n",
      "loss =  0.0009991524\n",
      "eq1_loss =  0.000982776\n",
      "bc1_loss =  1.3077002e-05\n",
      "bc2_loss =  3.2992798e-06\n",
      "\n",
      "epoch =  24000\n",
      "loss =  0.00095237873\n",
      "eq1_loss =  0.0009410983\n",
      "bc1_loss =  8.960829e-06\n",
      "bc2_loss =  2.3195835e-06\n",
      "\n",
      "epoch =  26000\n",
      "loss =  0.0009063418\n",
      "eq1_loss =  0.00089955074\n",
      "bc1_loss =  5.783106e-06\n",
      "bc2_loss =  1.0079771e-06\n",
      "\n",
      "epoch =  28000\n",
      "loss =  0.00086765626\n",
      "eq1_loss =  0.0008638622\n",
      "bc1_loss =  3.7276552e-06\n",
      "bc2_loss =  6.636357e-08\n",
      "\n",
      "epoch =  30000\n",
      "loss =  0.0008416651\n",
      "eq1_loss =  0.0008394655\n",
      "bc1_loss =  1.9884574e-06\n",
      "bc2_loss =  2.1118787e-07\n",
      "\n",
      "epoch =  32000\n",
      "loss =  0.0008290693\n",
      "eq1_loss =  0.0008269832\n",
      "bc1_loss =  9.33756e-07\n",
      "bc2_loss =  1.1523585e-06\n",
      "\n",
      "epoch =  34000\n",
      "loss =  0.00082519534\n",
      "eq1_loss =  0.0008225426\n",
      "bc1_loss =  4.8949516e-07\n",
      "bc2_loss =  2.163265e-06\n",
      "\n",
      "epoch =  36000\n",
      "loss =  0.0008240769\n",
      "eq1_loss =  0.000821081\n",
      "bc1_loss =  3.4497316e-07\n",
      "bc2_loss =  2.6509065e-06\n",
      "\n",
      "epoch =  38000\n",
      "loss =  0.0008233467\n",
      "eq1_loss =  0.0008202201\n",
      "bc1_loss =  3.137842e-07\n",
      "bc2_loss =  2.8128507e-06\n",
      "\n",
      "epoch =  40000\n",
      "loss =  0.00082269096\n",
      "eq1_loss =  0.0008195685\n",
      "bc1_loss =  2.9277942e-07\n",
      "bc2_loss =  2.82967e-06\n",
      "\n",
      "epoch =  42000\n",
      "loss =  0.0008220879\n",
      "eq1_loss =  0.0008189088\n",
      "bc1_loss =  3.0148664e-07\n",
      "bc2_loss =  2.8775971e-06\n",
      "\n",
      "epoch =  44000\n",
      "loss =  0.00082153035\n",
      "eq1_loss =  0.0008183641\n",
      "bc1_loss =  2.9355397e-07\n",
      "bc2_loss =  2.8727459e-06\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  46000\n",
      "loss =  0.00082102127\n",
      "eq1_loss =  0.00081779785\n",
      "bc1_loss =  3.072742e-07\n",
      "bc2_loss =  2.9161474e-06\n",
      "\n",
      "epoch =  48000\n",
      "loss =  0.00082054303\n",
      "eq1_loss =  0.00081744435\n",
      "bc1_loss =  2.6977102e-07\n",
      "bc2_loss =  2.828868e-06\n",
      "\n",
      "epoch =  50000\n",
      "loss =  0.00082009326\n",
      "eq1_loss =  0.0008169483\n",
      "bc1_loss =  2.8191658e-07\n",
      "bc2_loss =  2.8630557e-06\n",
      "\n",
      "epoch =  52000\n",
      "loss =  0.0008196662\n",
      "eq1_loss =  0.00081662426\n",
      "bc1_loss =  2.5259305e-07\n",
      "bc2_loss =  2.789308e-06\n",
      "\n",
      "epoch =  54000\n",
      "loss =  0.00081925525\n",
      "eq1_loss =  0.00081594096\n",
      "bc1_loss =  3.3056335e-07\n",
      "bc2_loss =  2.9837124e-06\n",
      "\n",
      "epoch =  56000\n",
      "loss =  0.00081884535\n",
      "eq1_loss =  0.00081566855\n",
      "bc1_loss =  2.9149078e-07\n",
      "bc2_loss =  2.8852867e-06\n",
      "\n",
      "epoch =  58000\n",
      "loss =  0.0008184321\n",
      "eq1_loss =  0.0008153469\n",
      "bc1_loss =  2.6754662e-07\n",
      "bc2_loss =  2.817651e-06\n",
      "\n",
      "epoch =  60000\n",
      "loss =  0.0008180357\n",
      "eq1_loss =  0.0008144234\n",
      "bc1_loss =  4.2722502e-07\n",
      "bc2_loss =  3.185091e-06\n",
      "\n",
      "epoch =  62000\n",
      "loss =  0.0008176016\n",
      "eq1_loss =  0.00081382226\n",
      "bc1_loss =  4.8350853e-07\n",
      "bc2_loss =  3.2958162e-06\n",
      "\n",
      "epoch =  64000\n",
      "loss =  0.00081702974\n",
      "eq1_loss =  0.00081378984\n",
      "bc1_loss =  3.192838e-07\n",
      "bc2_loss =  2.9206276e-06\n",
      "\n",
      "epoch =  66000\n",
      "loss =  0.00081659365\n",
      "eq1_loss =  0.00081417704\n",
      "bc1_loss =  1.0825261e-07\n",
      "bc2_loss =  2.3083405e-06\n",
      "\n",
      "epoch =  68000\n",
      "loss =  0.000815797\n",
      "eq1_loss =  0.0008124551\n",
      "bc1_loss =  3.5456117e-07\n",
      "bc2_loss =  2.9874202e-06\n",
      "\n",
      "epoch =  70000\n",
      "loss =  0.00081501907\n",
      "eq1_loss =  0.00081180775\n",
      "bc1_loss =  3.1672926e-07\n",
      "bc2_loss =  2.8946088e-06\n",
      "\n",
      "epoch =  72000\n",
      "loss =  0.0008141034\n",
      "eq1_loss =  0.00081089436\n",
      "bc1_loss =  3.1605873e-07\n",
      "bc2_loss =  2.8929865e-06\n",
      "\n",
      "epoch =  74000\n",
      "loss =  0.0008130153\n",
      "eq1_loss =  0.00080979354\n",
      "bc1_loss =  3.1941852e-07\n",
      "bc2_loss =  2.902321e-06\n",
      "\n",
      "epoch =  76000\n",
      "loss =  0.000811715\n",
      "eq1_loss =  0.0008084567\n",
      "bc1_loss =  3.2796402e-07\n",
      "bc2_loss =  2.9304147e-06\n",
      "\n",
      "epoch =  78000\n",
      "loss =  0.0008101541\n",
      "eq1_loss =  0.0008066018\n",
      "bc1_loss =  4.1300837e-07\n",
      "bc2_loss =  3.1393024e-06\n",
      "\n",
      "epoch =  80000\n",
      "loss =  0.0008082405\n",
      "eq1_loss =  0.0008049434\n",
      "bc1_loss =  3.2905723e-07\n",
      "bc2_loss =  2.9680834e-06\n",
      "\n",
      "epoch =  82000\n",
      "loss =  0.00080592604\n",
      "eq1_loss =  0.0008026213\n",
      "bc1_loss =  3.222545e-07\n",
      "bc2_loss =  2.982477e-06\n",
      "\n",
      "epoch =  84000\n",
      "loss =  0.00080308033\n",
      "eq1_loss =  0.0007996159\n",
      "bc1_loss =  3.5754874e-07\n",
      "bc2_loss =  3.1068594e-06\n",
      "\n",
      "epoch =  86000\n",
      "loss =  0.0007995532\n",
      "eq1_loss =  0.00079617376\n",
      "bc1_loss =  3.1485354e-07\n",
      "bc2_loss =  3.06456e-06\n",
      "\n",
      "epoch =  88000\n",
      "loss =  0.0007951617\n",
      "eq1_loss =  0.00079162605\n",
      "bc1_loss =  3.360692e-07\n",
      "bc2_loss =  3.1995744e-06\n",
      "\n",
      "epoch =  90000\n",
      "loss =  0.00078962126\n",
      "eq1_loss =  0.00078593683\n",
      "bc1_loss =  3.5130347e-07\n",
      "bc2_loss =  3.333145e-06\n",
      "\n",
      "interface_PINN =  0.004075706\n",
      "interface_Analytical =  0.005348674602179501\n",
      "broke inner loop\n",
      "\n",
      "t =  0.003\n",
      " \n",
      "slope at interface =  tensor([[-116.0917]])\n",
      "epoch =  0\n",
      "loss =  0.01756464\n",
      "eq1_loss =  0.008017326\n",
      "bc1_loss =  3.2388067e-07\n",
      "bc2_loss =  0.0095469905\n",
      "\n",
      "epoch =  2000\n",
      "loss =  0.00044032926\n",
      "eq1_loss =  0.00034554105\n",
      "bc1_loss =  7.2223316e-05\n",
      "bc2_loss =  2.2564891e-05\n",
      "\n",
      "loss limit attained, epoch =  2158\n",
      "epoch =  2158\n",
      "loss =  0.0003998737\n",
      "eq1_loss =  0.000322297\n",
      "bc1_loss =  5.8184018e-05\n",
      "bc2_loss =  1.9392664e-05\n",
      "\n",
      "interface_PINN =  0.0049201776\n",
      "interface_Analytical =  0.006550761787761791\n",
      "broke inner loop\n",
      "\n",
      "t =  0.004\n",
      " \n",
      "slope at interface =  tensor([[-92.5956]])\n",
      "epoch =  0\n",
      "loss =  0.00834853\n",
      "eq1_loss =  0.004568338\n",
      "bc1_loss =  5.8102207e-05\n",
      "bc2_loss =  0.0037220896\n",
      "\n",
      "loss limit attained, epoch =  1615\n",
      "epoch =  1615\n",
      "loss =  0.00039974833\n",
      "eq1_loss =  0.0002801665\n",
      "bc1_loss =  8.6885004e-05\n",
      "bc2_loss =  3.2696804e-05\n",
      "\n",
      "interface_PINN =  0.005636477\n",
      "interface_Analytical =  0.007564168163122769\n",
      "broke inner loop\n",
      "\n",
      "t =  0.005\n",
      " \n",
      "slope at interface =  tensor([[-78.5416]])\n",
      "epoch =  0\n",
      "loss =  0.0047339415\n",
      "eq1_loss =  0.002826439\n",
      "bc1_loss =  8.676282e-05\n",
      "bc2_loss =  0.00182074\n",
      "\n",
      "loss limit attained, epoch =  1133\n",
      "epoch =  1133\n",
      "loss =  0.00039957935\n",
      "eq1_loss =  0.00027056472\n",
      "bc1_loss =  8.5757514e-05\n",
      "bc2_loss =  4.3257125e-05\n",
      "\n",
      "interface_PINN =  0.0062649692\n",
      "interface_Analytical =  0.008456997102991115\n",
      "broke inner loop\n",
      "\n",
      "t =  0.006\n",
      " \n",
      "slope at interface =  tensor([[-68.9136]])\n",
      "epoch =  0\n",
      "loss =  0.003081177\n",
      "eq1_loss =  0.0020103015\n",
      "bc1_loss =  8.563612e-05\n",
      "bc2_loss =  0.0009852393\n",
      "\n",
      "loss limit attained, epoch =  813\n",
      "epoch =  813\n",
      "loss =  0.00039960808\n",
      "eq1_loss =  0.00026872568\n",
      "bc1_loss =  8.018779e-05\n",
      "bc2_loss =  5.0694587e-05\n",
      "\n",
      "interface_PINN =  0.006828554\n",
      "interface_Analytical =  0.009264176164128148\n",
      "broke inner loop\n",
      "\n",
      "t =  0.007\n",
      " \n",
      "slope at interface =  tensor([[-61.7966]])\n",
      "epoch =  0\n",
      "loss =  0.002182111\n",
      "eq1_loss =  0.0015348619\n",
      "bc1_loss =  8.007041e-05\n",
      "bc2_loss =  0.00056717865\n",
      "\n",
      "loss limit attained, epoch =  607\n",
      "epoch =  607\n",
      "loss =  0.0003997627\n",
      "eq1_loss =  0.00026893162\n",
      "bc1_loss =  7.626797e-05\n",
      "bc2_loss =  5.456311e-05\n",
      "\n",
      "interface_PINN =  0.007341796\n",
      "interface_Analytical =  0.010006453917347552\n",
      "broke inner loop\n",
      "\n",
      "t =  0.008\n",
      " \n",
      "slope at interface =  tensor([[-56.2766]])\n",
      "epoch =  0\n",
      "loss =  0.0016814652\n",
      "eq1_loss =  0.0012622661\n",
      "bc1_loss =  7.615141e-05\n",
      "bc2_loss =  0.00034304772\n",
      "\n",
      "loss limit attained, epoch =  494\n",
      "epoch =  494\n",
      "loss =  0.00039991955\n",
      "eq1_loss =  0.00027125955\n",
      "bc1_loss =  7.069764e-05\n",
      "bc2_loss =  5.7962356e-05\n",
      "\n",
      "interface_PINN =  0.007814633\n",
      "interface_Analytical =  0.010697349204359002\n",
      "broke inner loop\n",
      "\n",
      "t =  0.009000000000000001\n",
      " \n",
      "slope at interface =  tensor([[-51.8461]])\n",
      "epoch =  0\n",
      "loss =  0.0013655177\n",
      "eq1_loss =  0.0010832183\n",
      "bc1_loss =  7.0587426e-05\n",
      "bc2_loss =  0.00021171205\n",
      "\n",
      "loss limit attained, epoch =  415\n",
      "epoch =  415\n",
      "loss =  0.00039962935\n",
      "eq1_loss =  0.00027353983\n",
      "bc1_loss =  6.445502e-05\n",
      "bc2_loss =  6.163453e-05\n",
      "\n",
      "interface_PINN =  0.0082541695\n",
      "interface_Analytical =  0.011346252244684153\n",
      "broke inner loop\n",
      "\n",
      "t =  0.010000000000000002\n",
      " \n",
      "slope at interface =  tensor([[-48.1948]])\n",
      "epoch =  0\n",
      "loss =  0.001133591\n",
      "eq1_loss =  0.0009386778\n",
      "bc1_loss =  6.434404e-05\n",
      "bc2_loss =  0.00013056918\n",
      "\n",
      "loss limit attained, epoch =  336\n",
      "epoch =  336\n",
      "loss =  0.00039996524\n",
      "eq1_loss =  0.00027554802\n",
      "bc1_loss =  6.1128325e-05\n",
      "bc2_loss =  6.32889e-05\n",
      "\n",
      "interface_PINN =  0.00866561\n",
      "interface_Analytical =  0.011960000000000009\n",
      "broke inner loop\n",
      "\n",
      "t =  0.011000000000000003\n",
      " \n",
      "slope at interface =  tensor([[-45.1141]])\n",
      "epoch =  0\n",
      "loss =  0.0009915665\n",
      "eq1_loss =  0.00084905233\n",
      "bc1_loss =  6.101653e-05\n",
      "bc2_loss =  8.1497565e-05\n",
      "\n",
      "loss limit attained, epoch =  283\n",
      "epoch =  283\n",
      "loss =  0.00039966195\n",
      "eq1_loss =  0.00027679236\n",
      "bc1_loss =  5.8466245e-05\n",
      "bc2_loss =  6.440334e-05\n",
      "\n",
      "interface_PINN =  0.009052876\n",
      "interface_Analytical =  0.012543753824115022\n",
      "broke inner loop\n",
      "\n",
      "t =  0.012000000000000004\n",
      " \n",
      "slope at interface =  tensor([[-42.4634]])\n",
      "epoch =  0\n",
      "loss =  0.0008791665\n",
      "eq1_loss =  0.0007705351\n",
      "bc1_loss =  5.8356913e-05\n",
      "bc2_loss =  5.0274473e-05\n",
      "\n",
      "loss limit attained, epoch =  243\n",
      "epoch =  243\n",
      "loss =  0.00039992752\n",
      "eq1_loss =  0.00027804612\n",
      "bc1_loss =  5.605248e-05\n",
      "bc2_loss =  6.5828935e-05\n",
      "\n",
      "interface_PINN =  0.009419035\n",
      "interface_Analytical =  0.013101523575523586\n",
      "broke inner loop\n",
      "\n",
      "t =  0.013000000000000005\n",
      " \n",
      "slope at interface =  tensor([[-40.1489]])\n",
      "epoch =  0\n",
      "loss =  0.00078769395\n",
      "eq1_loss =  0.00070201047\n",
      "bc1_loss =  5.5941862e-05\n",
      "bc2_loss =  2.9741606e-05\n",
      "\n",
      "loss limit attained, epoch =  214\n",
      "epoch =  214\n",
      "loss =  0.0003999414\n",
      "eq1_loss =  0.00027888644\n",
      "bc1_loss =  5.356563e-05\n",
      "bc2_loss =  6.748933e-05\n",
      "\n",
      "interface_PINN =  0.009766612\n",
      "interface_Analytical =  0.013636498084185702\n",
      "broke inner loop\n",
      "\n",
      "t =  0.014000000000000005\n",
      " \n",
      "slope at interface =  tensor([[-38.1115]])\n",
      "epoch =  0\n",
      "loss =  0.0007256678\n",
      "eq1_loss =  0.0006557758\n",
      "bc1_loss =  5.3457497e-05\n",
      "bc2_loss =  1.6434513e-05\n",
      "\n",
      "loss limit attained, epoch =  191\n",
      "epoch =  191\n",
      "loss =  0.00039973287\n",
      "eq1_loss =  0.00027932596\n",
      "bc1_loss =  5.106192e-05\n",
      "bc2_loss =  6.9345006e-05\n",
      "\n",
      "interface_PINN =  0.010097688\n",
      "interface_Analytical =  0.014151262841174295\n",
      "broke inner loop\n",
      "\n",
      "t =  0.015000000000000006\n",
      " \n",
      "slope at interface =  tensor([[-36.3022]])\n",
      "epoch =  0\n",
      "loss =  0.00067117834\n",
      "eq1_loss =  0.0006121524\n",
      "bc1_loss =  5.095805e-05\n",
      "bc2_loss =  8.067871e-06\n",
      "\n",
      "loss limit attained, epoch =  165\n",
      "epoch =  165\n",
      "loss =  0.00039972964\n",
      "eq1_loss =  0.0002800563\n",
      "bc1_loss =  4.9982442e-05\n",
      "bc2_loss =  6.96909e-05\n",
      "\n",
      "interface_PINN =  0.010414015\n",
      "interface_Analytical =  0.014647948661843418\n",
      "broke inner loop\n",
      "\n",
      "t =  0.016000000000000007\n",
      " \n",
      "slope at interface =  tensor([[-34.6849]])\n",
      "epoch =  0\n",
      "loss =  0.00062791177\n",
      "eq1_loss =  0.00057452574\n",
      "bc1_loss =  4.9866205e-05\n",
      "bc2_loss =  3.5198104e-06\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss limit attained, epoch =  141\n",
      "epoch =  141\n",
      "loss =  0.0003996913\n",
      "eq1_loss =  0.00028049992\n",
      "bc1_loss =  4.9701346e-05\n",
      "bc2_loss =  6.9490015e-05\n",
      "\n",
      "interface_PINN =  0.010716911\n",
      "interface_Analytical =  0.01512833632624554\n",
      "broke inner loop\n",
      "\n",
      "t =  0.017000000000000008\n",
      " \n",
      "slope at interface =  tensor([[-33.2123]])\n",
      "epoch =  0\n",
      "loss =  0.0005955502\n",
      "eq1_loss =  0.00054483063\n",
      "bc1_loss =  4.9592152e-05\n",
      "bc2_loss =  1.1274132e-06\n",
      "\n",
      "loss limit attained, epoch =  124\n",
      "epoch =  124\n",
      "loss =  0.00039966672\n",
      "eq1_loss =  0.000280553\n",
      "bc1_loss =  4.9572005e-05\n",
      "bc2_loss =  6.9541704e-05\n",
      "\n",
      "interface_PINN =  0.0110075185\n",
      "interface_Analytical =  0.015593932153244752\n",
      "broke inner loop\n",
      "\n",
      "t =  0.01800000000000001\n",
      " \n",
      "slope at interface =  tensor([[-31.8648]])\n",
      "epoch =  0\n",
      "loss =  0.00056992413\n",
      "eq1_loss =  0.00052036426\n",
      "bc1_loss =  4.944954e-05\n",
      "bc2_loss =  1.10301755e-07\n",
      "\n",
      "loss limit attained, epoch =  111\n",
      "epoch =  111\n",
      "loss =  0.00039976573\n",
      "eq1_loss =  0.00028084643\n",
      "bc1_loss =  4.9218444e-05\n",
      "bc2_loss =  6.970085e-05\n",
      "\n",
      "interface_PINN =  0.011286846\n",
      "interface_Analytical =  0.016046023806538506\n",
      "broke inner loop\n",
      "\n",
      "t =  0.01900000000000001\n",
      " \n",
      "slope at interface =  tensor([[-30.6281]])\n",
      "epoch =  0\n",
      "loss =  0.00054874126\n",
      "eq1_loss =  0.0004996092\n",
      "bc1_loss =  4.903129e-05\n",
      "bc2_loss =  1.0077726e-07\n",
      "\n",
      "loss limit attained, epoch =  100\n",
      "epoch =  100\n",
      "loss =  0.00039977467\n",
      "eq1_loss =  0.00028111253\n",
      "bc1_loss =  4.8706283e-05\n",
      "bc2_loss =  6.9955866e-05\n",
      "\n",
      "interface_PINN =  0.011555784\n",
      "interface_Analytical =  0.016485722307499923\n",
      "broke inner loop\n",
      "\n",
      "t =  0.02000000000000001\n",
      " \n",
      "slope at interface =  tensor([[-29.4889]])\n",
      "epoch =  0\n",
      "loss =  0.00053122983\n",
      "eq1_loss =  0.0004817337\n",
      "bc1_loss =  4.8664693e-05\n",
      "bc2_loss =  8.314373e-07\n",
      "\n",
      "loss limit attained, epoch =  91\n",
      "epoch =  91\n",
      "loss =  0.00039985438\n",
      "eq1_loss =  0.0002804098\n",
      "bc1_loss =  4.8654714e-05\n",
      "bc2_loss =  7.078989e-05\n",
      "\n",
      "interface_PINN =  0.01181513\n",
      "interface_Analytical =  0.016913994205982233\n",
      "broke inner loop\n",
      "\n",
      "t =  0.02100000000000001\n",
      " \n",
      "slope at interface =  tensor([[-28.4370]])\n",
      "epoch =  0\n",
      "loss =  0.00051652343\n",
      "eq1_loss =  0.00046602654\n",
      "bc1_loss =  4.8365775e-05\n",
      "bc2_loss =  2.131124e-06\n",
      "\n",
      "loss limit attained, epoch =  84\n",
      "epoch =  84\n",
      "loss =  0.00039972906\n",
      "eq1_loss =  0.0002799611\n",
      "bc1_loss =  4.8229907e-05\n",
      "bc2_loss =  7.1538074e-05\n",
      "\n",
      "interface_PINN =  0.01206561\n",
      "interface_Analytical =  0.017331686588442585\n",
      "broke inner loop\n",
      "\n",
      "t =  0.022000000000000013\n",
      " \n",
      "slope at interface =  tensor([[-27.4649]])\n",
      "epoch =  0\n",
      "loss =  0.0005034469\n",
      "eq1_loss =  0.00045132724\n",
      "bc1_loss =  4.822163e-05\n",
      "bc2_loss =  3.898035e-06\n",
      "\n",
      "loss limit attained, epoch =  78\n",
      "epoch =  78\n",
      "loss =  0.00039965677\n",
      "eq1_loss =  0.0002818425\n",
      "bc1_loss =  4.726287e-05\n",
      "bc2_loss =  7.0551374e-05\n",
      "\n",
      "interface_PINN =  0.012307928\n",
      "interface_Analytical =  0.017739546781132844\n",
      "broke inner loop\n",
      "\n",
      "t =  0.023000000000000013\n",
      " \n",
      "slope at interface =  tensor([[-26.5700]])\n",
      "epoch =  0\n",
      "loss =  0.0004885356\n",
      "eq1_loss =  0.00043596877\n",
      "bc1_loss =  4.7185866e-05\n",
      "bc2_loss =  5.380978e-06\n",
      "\n",
      "loss limit attained, epoch =  63\n",
      "epoch =  63\n",
      "loss =  0.00039978186\n",
      "eq1_loss =  0.00028163003\n",
      "bc1_loss =  4.8854483e-05\n",
      "bc2_loss =  6.929736e-05\n",
      "\n",
      "interface_PINN =  0.012542541\n",
      "interface_Analytical =  0.018138238062171328\n",
      "broke inner loop\n",
      "\n",
      "t =  0.024000000000000014\n",
      " \n",
      "slope at interface =  tensor([[-25.7252]])\n",
      "epoch =  0\n",
      "loss =  0.00048234177\n",
      "eq1_loss =  0.00042622144\n",
      "bc1_loss =  4.9119808e-05\n",
      "bc2_loss =  7.000523e-06\n",
      "\n",
      "loss limit attained, epoch =  62\n",
      "epoch =  62\n",
      "loss =  0.0003996566\n",
      "eq1_loss =  0.00028169175\n",
      "bc1_loss =  4.841719e-05\n",
      "bc2_loss =  6.954766e-05\n",
      "\n",
      "interface_PINN =  0.012769912\n",
      "interface_Analytical =  0.018528352328256302\n",
      "broke inner loop\n",
      "\n",
      "t =  0.025000000000000015\n",
      " \n",
      "slope at interface =  tensor([[-24.9310]])\n",
      "epoch =  0\n",
      "loss =  0.00047189373\n",
      "eq1_loss =  0.00041422705\n",
      "bc1_loss =  4.8682992e-05\n",
      "bc2_loss =  8.983682e-06\n",
      "\n",
      "loss limit attained, epoch =  53\n",
      "epoch =  53\n",
      "loss =  0.00039992988\n",
      "eq1_loss =  0.00027989413\n",
      "bc1_loss =  5.081179e-05\n",
      "bc2_loss =  6.922395e-05\n",
      "\n",
      "interface_PINN =  0.012990515\n",
      "interface_Analytical =  0.018910420407806928\n",
      "broke inner loop\n",
      "\n",
      "t =  0.026000000000000016\n",
      " \n",
      "slope at interface =  tensor([[-24.1888]])\n",
      "epoch =  0\n",
      "loss =  0.00046668533\n",
      "eq1_loss =  0.00040647545\n",
      "bc1_loss =  4.9741695e-05\n",
      "bc2_loss =  1.0468197e-05\n",
      "\n",
      "loss limit attained, epoch =  51\n",
      "epoch =  51\n",
      "loss =  0.0003995403\n",
      "eq1_loss =  0.00027624005\n",
      "bc1_loss =  5.2470223e-05\n",
      "bc2_loss =  7.083001e-05\n",
      "\n",
      "interface_PINN =  0.01320475\n",
      "interface_Analytical =  0.019284920533930147\n",
      "broke inner loop\n",
      "\n",
      "t =  0.027000000000000017\n",
      " \n",
      "slope at interface =  tensor([[-23.4906]])\n",
      "epoch =  0\n",
      "loss =  0.0004616612\n",
      "eq1_loss =  0.0003969908\n",
      "bc1_loss =  5.1557185e-05\n",
      "bc2_loss =  1.3113238e-05\n",
      "\n",
      "loss limit attained, epoch =  51\n",
      "epoch =  51\n",
      "loss =  0.00039986486\n",
      "eq1_loss =  0.0002761624\n",
      "bc1_loss =  5.2263185e-05\n",
      "bc2_loss =  7.1439295e-05\n",
      "\n",
      "interface_PINN =  0.013413052\n",
      "interface_Analytical =  0.01965228536328538\n",
      "broke inner loop\n",
      "\n",
      "t =  0.028000000000000018\n",
      " \n",
      "slope at interface =  tensor([[-22.8402]])\n",
      "epoch =  0\n",
      "loss =  0.00045444886\n",
      "eq1_loss =  0.00038769687\n",
      "bc1_loss =  5.143913e-05\n",
      "bc2_loss =  1.5312853e-05\n",
      "\n",
      "loss limit attained, epoch =  46\n",
      "epoch =  46\n",
      "loss =  0.00039947944\n",
      "eq1_loss =  0.00027775895\n",
      "bc1_loss =  5.225629e-05\n",
      "bc2_loss =  6.946418e-05\n",
      "\n",
      "interface_PINN =  0.013615845\n",
      "interface_Analytical =  0.020012907834695108\n",
      "broke inner loop\n",
      "\n",
      "t =  0.02900000000000002\n",
      " \n",
      "slope at interface =  tensor([[-22.2360]])\n",
      "epoch =  0\n",
      "loss =  0.0004478789\n",
      "eq1_loss =  0.0003786302\n",
      "bc1_loss =  5.2553154e-05\n",
      "bc2_loss =  1.669554e-05\n",
      "\n",
      "loss limit attained, epoch =  40\n",
      "epoch =  40\n",
      "loss =  0.00039987918\n",
      "eq1_loss =  0.00028708088\n",
      "bc1_loss =  4.871793e-05\n",
      "bc2_loss =  6.408039e-05\n",
      "\n",
      "interface_PINN =  0.013813292\n",
      "interface_Analytical =  0.020367146093647998\n",
      "broke inner loop\n",
      "\n",
      "t =  0.03000000000000002\n",
      " \n",
      "slope at interface =  tensor([[-21.6499]])\n",
      "epoch =  0\n",
      "loss =  0.00043648138\n",
      "eq1_loss =  0.00037166986\n",
      "bc1_loss =  4.8932838e-05\n",
      "bc2_loss =  1.5878702e-05\n",
      "\n",
      "loss limit attained, epoch =  23\n",
      "epoch =  23\n",
      "loss =  0.00039920062\n",
      "eq1_loss =  0.0002937333\n",
      "bc1_loss =  5.5804638e-05\n",
      "bc2_loss =  4.9662696e-05\n",
      "\n",
      "interface_PINN =  0.014006174\n",
      "interface_Analytical =  0.020715327658523797\n",
      "broke inner loop\n",
      "\n",
      "t =  0.03100000000000002\n",
      " \n",
      "slope at interface =  tensor([[-21.1494]])\n",
      "epoch =  0\n",
      "loss =  0.0004384185\n",
      "eq1_loss =  0.00036922022\n",
      "bc1_loss =  5.7953283e-05\n",
      "bc2_loss =  1.1245005e-05\n",
      "\n",
      "loss limit attained, epoch =  20\n",
      "epoch =  20\n",
      "loss =  0.0003987563\n",
      "eq1_loss =  0.00030276162\n",
      "bc1_loss =  5.4765824e-05\n",
      "bc2_loss =  4.1228854e-05\n",
      "\n",
      "interface_PINN =  0.014194127\n",
      "interface_Analytical =  0.021057752966544198\n",
      "broke inner loop\n",
      "\n",
      "t =  0.03200000000000002\n",
      " \n",
      "slope at interface =  tensor([[-20.6089]])\n",
      "epoch =  0\n",
      "loss =  0.00042986646\n",
      "eq1_loss =  0.00036564647\n",
      "bc1_loss =  5.5797515e-05\n",
      "bc2_loss =  8.422476e-06\n",
      "\n",
      "loss limit attained, epoch =  11\n",
      "epoch =  11\n",
      "loss =  0.00039979222\n",
      "eq1_loss =  0.0002609128\n",
      "bc1_loss =  9.739911e-05\n",
      "bc2_loss =  4.14803e-05\n",
      "\n",
      "interface_PINN =  0.014378343\n",
      "interface_Analytical =  0.021394698408718014\n",
      "broke inner loop\n",
      "\n",
      "t =  0.03300000000000002\n",
      " \n",
      "slope at interface =  tensor([[-20.1991]])\n",
      "epoch =  0\n",
      "loss =  0.00046137598\n",
      "eq1_loss =  0.00036334162\n",
      "bc1_loss =  9.006035e-05\n",
      "bc2_loss =  7.974014e-06\n",
      "\n",
      "loss limit attained, epoch =  36\n",
      "epoch =  36\n",
      "loss =  0.00039945444\n",
      "eq1_loss =  0.0002726999\n",
      "bc1_loss =  6.18931e-05\n",
      "bc2_loss =  6.4861444e-05\n",
      "\n",
      "interface_PINN =  0.014556369\n",
      "interface_Analytical =  0.02172641894100362\n",
      "broke inner loop\n",
      "\n",
      "t =  0.03400000000000002\n",
      " \n",
      "slope at interface =  tensor([[-19.5204]])\n",
      "epoch =  0\n",
      "loss =  0.00043759495\n",
      "eq1_loss =  0.00035518705\n",
      "bc1_loss =  6.017025e-05\n",
      "bc2_loss =  2.2237647e-05\n",
      "\n",
      "loss limit attained, epoch =  34\n",
      "epoch =  34\n",
      "loss =  0.00039996533\n",
      "eq1_loss =  0.00027491534\n",
      "bc1_loss =  5.7299912e-05\n",
      "bc2_loss =  6.775008e-05\n",
      "\n",
      "interface_PINN =  0.014730181\n",
      "interface_Analytical =  0.02205315034184461\n",
      "broke inner loop\n",
      "\n",
      "t =  0.035000000000000024\n",
      " \n",
      "slope at interface =  tensor([[-19.0583]])\n",
      "epoch =  0\n",
      "loss =  0.00043000584\n",
      "eq1_loss =  0.00034899107\n",
      "bc1_loss =  5.564268e-05\n",
      "bc2_loss =  2.5372068e-05\n",
      "\n",
      "loss limit attained, epoch =  27\n",
      "epoch =  27\n",
      "loss =  0.00039983355\n",
      "eq1_loss =  0.00027624983\n",
      "bc1_loss =  5.933906e-05\n",
      "bc2_loss =  6.424463e-05\n",
      "\n",
      "interface_PINN =  0.014900333\n",
      "interface_Analytical =  0.022375111172908194\n",
      "broke inner loop\n",
      "\n",
      "t =  0.036000000000000025\n",
      " \n",
      "slope at interface =  tensor([[-18.6571]])\n",
      "epoch =  0\n",
      "loss =  0.0004296091\n",
      "eq1_loss =  0.0003438863\n",
      "bc1_loss =  6.0024235e-05\n",
      "bc2_loss =  2.5698564e-05\n",
      "\n",
      "loss limit attained, epoch =  27\n",
      "epoch =  27\n",
      "loss =  0.00039983913\n",
      "eq1_loss =  0.00027318412\n",
      "bc1_loss =  6.0702198e-05\n",
      "bc2_loss =  6.59528e-05\n",
      "\n",
      "interface_PINN =  0.01506681\n",
      "interface_Analytical =  0.022692504489368315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broke inner loop\n",
      "\n",
      "t =  0.037000000000000026\n",
      " \n",
      "slope at interface =  tensor([[-18.2541]])\n",
      "epoch =  0\n",
      "loss =  0.00042742293\n",
      "eq1_loss =  0.0003387489\n",
      "bc1_loss =  6.081556e-05\n",
      "bc2_loss =  2.785845e-05\n",
      "\n",
      "loss limit attained, epoch =  26\n",
      "epoch =  26\n",
      "loss =  0.00039985628\n",
      "eq1_loss =  0.00027585708\n",
      "bc1_loss =  5.9007103e-05\n",
      "bc2_loss =  6.4992084e-05\n",
      "\n",
      "interface_PINN =  0.015229781\n",
      "interface_Analytical =  0.023005519337758955\n",
      "broke inner loop\n",
      "\n",
      "t =  0.03800000000000003\n",
      " \n",
      "slope at interface =  tensor([[-17.8697]])\n",
      "epoch =  0\n",
      "loss =  0.000423274\n",
      "eq1_loss =  0.0003350751\n",
      "bc1_loss =  5.952286e-05\n",
      "bc2_loss =  2.867603e-05\n",
      "\n",
      "loss limit attained, epoch =  22\n",
      "epoch =  22\n",
      "loss =  0.0003992728\n",
      "eq1_loss =  0.00028644194\n",
      "bc1_loss =  5.4794058e-05\n",
      "bc2_loss =  5.8036803e-05\n",
      "\n",
      "interface_PINN =  0.0153894415\n",
      "interface_Analytical =  0.023314332072783068\n",
      "broke inner loop\n",
      "\n",
      "t =  0.03900000000000003\n",
      " \n",
      "slope at interface =  tensor([[-17.5066]])\n",
      "epoch =  0\n",
      "loss =  0.00041086477\n",
      "eq1_loss =  0.00032965222\n",
      "bc1_loss =  5.5688928e-05\n",
      "bc2_loss =  2.552361e-05\n",
      "\n",
      "loss limit attained, epoch =  7\n",
      "epoch =  7\n",
      "loss =  0.00039984827\n",
      "eq1_loss =  0.00026832588\n",
      "bc1_loss =  8.358346e-05\n",
      "bc2_loss =  4.7938935e-05\n",
      "\n",
      "interface_PINN =  0.015546796\n",
      "interface_Analytical =  0.023619107519125295\n",
      "broke inner loop\n",
      "\n",
      "t =  0.04000000000000003\n",
      " \n",
      "slope at interface =  tensor([[-17.2538]])\n",
      "epoch =  0\n",
      "loss =  0.0004319295\n",
      "eq1_loss =  0.0003273626\n",
      "bc1_loss =  8.4556246e-05\n",
      "bc2_loss =  2.0010668e-05\n",
      "\n",
      "loss limit attained, epoch =  22\n",
      "epoch =  22\n",
      "loss =  0.00039963835\n",
      "eq1_loss =  0.00028112298\n",
      "bc1_loss =  6.343503e-05\n",
      "bc2_loss =  5.5080334e-05\n",
      "\n",
      "interface_PINN =  0.015700238\n",
      "interface_Analytical =  0.023920000000000025\n",
      "broke inner loop\n",
      "\n",
      "t =  0.04100000000000003\n",
      " \n",
      "slope at interface =  tensor([[-16.8248]])\n",
      "epoch =  0\n",
      "loss =  0.00041343112\n",
      "eq1_loss =  0.00032415555\n",
      "bc1_loss =  6.365549e-05\n",
      "bc2_loss =  2.5620062e-05\n",
      "\n",
      "loss limit attained, epoch =  10\n",
      "epoch =  10\n",
      "loss =  0.00039885638\n",
      "eq1_loss =  0.00026654362\n",
      "bc1_loss =  8.096251e-05\n",
      "bc2_loss =  5.135025e-05\n",
      "\n",
      "interface_PINN =  0.015851127\n",
      "interface_Analytical =  0.024217154250654665\n",
      "broke inner loop\n",
      "\n",
      "t =  0.04200000000000003\n",
      " \n",
      "slope at interface =  tensor([[-16.5448]])\n",
      "epoch =  0\n",
      "loss =  0.00042128755\n",
      "eq1_loss =  0.0003203084\n",
      "bc1_loss =  7.794068e-05\n",
      "bc2_loss =  2.303849e-05\n",
      "\n",
      "loss limit attained, epoch =  14\n",
      "epoch =  14\n",
      "loss =  0.00039965913\n",
      "eq1_loss =  0.0002743462\n",
      "bc1_loss =  7.4563985e-05\n",
      "bc2_loss =  5.0748924e-05\n",
      "\n",
      "interface_PINN =  0.01599893\n",
      "interface_Analytical =  0.024510706232175387\n",
      "broke inner loop\n",
      "\n",
      "t =  0.04300000000000003\n",
      " \n",
      "slope at interface =  tensor([[-16.2063]])\n",
      "epoch =  0\n",
      "loss =  0.0004132677\n",
      "eq1_loss =  0.00031827725\n",
      "bc1_loss =  7.1435265e-05\n",
      "bc2_loss =  2.355518e-05\n",
      "\n",
      "loss limit attained, epoch =  8\n",
      "epoch =  8\n",
      "loss =  0.0003993253\n",
      "eq1_loss =  0.0002751391\n",
      "bc1_loss =  8.072028e-05\n",
      "bc2_loss =  4.346593e-05\n",
      "\n",
      "interface_PINN =  0.016144274\n",
      "interface_Analytical =  0.02480078385857998\n",
      "broke inner loop\n",
      "\n",
      "t =  0.04400000000000003\n",
      " \n",
      "slope at interface =  tensor([[-15.9369]])\n",
      "epoch =  0\n",
      "loss =  0.0004169126\n",
      "eq1_loss =  0.00031478453\n",
      "bc1_loss =  8.1392136e-05\n",
      "bc2_loss =  2.0735963e-05\n",
      "\n",
      "loss limit attained, epoch =  10\n",
      "epoch =  10\n",
      "loss =  0.0003985895\n",
      "eq1_loss =  0.00027113245\n",
      "bc1_loss =  8.352462e-05\n",
      "bc2_loss =  4.3932447e-05\n",
      "\n",
      "interface_PINN =  0.016286887\n",
      "interface_Analytical =  0.025087507648230055\n",
      "broke inner loop\n",
      "\n",
      "t =  0.04500000000000003\n",
      " \n",
      "slope at interface =  tensor([[-15.6374]])\n",
      "epoch =  0\n",
      "loss =  0.00041269092\n",
      "eq1_loss =  0.00031196995\n",
      "bc1_loss =  8.005334e-05\n",
      "bc2_loss =  2.066762e-05\n",
      "\n",
      "loss limit attained, epoch =  6\n",
      "epoch =  6\n",
      "loss =  0.00039993404\n",
      "eq1_loss =  0.0002860418\n",
      "bc1_loss =  8.011095e-05\n",
      "bc2_loss =  3.37813e-05\n",
      "\n",
      "interface_PINN =  0.016427118\n",
      "interface_Analytical =  0.025370991308973353\n",
      "broke inner loop\n",
      "\n",
      "t =  0.046000000000000034\n",
      " \n",
      "slope at interface =  tensor([[-15.3761]])\n",
      "epoch =  0\n",
      "loss =  0.0004079945\n",
      "eq1_loss =  0.00031093007\n",
      "bc1_loss =  8.125023e-05\n",
      "bc2_loss =  1.5814165e-05\n",
      "\n",
      "loss limit attained, epoch =  3\n",
      "epoch =  3\n",
      "loss =  0.00039937833\n",
      "eq1_loss =  0.0002862467\n",
      "bc1_loss =  8.89416e-05\n",
      "bc2_loss =  2.4190038e-05\n",
      "\n",
      "interface_PINN =  0.016565308\n",
      "interface_Analytical =  0.025651342265074577\n",
      "broke inner loop\n",
      "\n",
      "t =  0.047000000000000035\n",
      " \n",
      "slope at interface =  tensor([[-15.1523]])\n",
      "epoch =  0\n",
      "loss =  0.00041122083\n",
      "eq1_loss =  0.00030951918\n",
      "bc1_loss =  9.140935e-05\n",
      "bc2_loss =  1.0292293e-05\n",
      "\n",
      "loss limit attained, epoch =  4\n",
      "epoch =  4\n",
      "loss =  0.00039822792\n",
      "eq1_loss =  0.00027260894\n",
      "bc1_loss =  0.00010399428\n",
      "bc2_loss =  2.1624688e-05\n",
      "\n",
      "interface_PINN =  0.016701397\n",
      "interface_Analytical =  0.02592866213285987\n",
      "broke inner loop\n",
      "\n",
      "t =  0.048000000000000036\n",
      " \n",
      "slope at interface =  tensor([[-14.9220]])\n",
      "epoch =  0\n",
      "loss =  0.000420912\n",
      "eq1_loss =  0.00030668313\n",
      "bc1_loss =  0.00010525019\n",
      "bc2_loss =  8.978681e-06\n",
      "\n",
      "loss limit attained, epoch =  7\n",
      "epoch =  7\n",
      "loss =  0.00039740428\n",
      "eq1_loss =  0.0002662888\n",
      "bc1_loss =  0.00010486652\n",
      "bc2_loss =  2.624899e-05\n",
      "\n",
      "interface_PINN =  0.01683489\n",
      "interface_Analytical =  0.026203047151047175\n",
      "broke inner loop\n",
      "\n",
      "t =  0.04900000000000004\n",
      " \n",
      "slope at interface =  tensor([[-14.6375]])\n",
      "epoch =  0\n",
      "loss =  0.0004180041\n",
      "eq1_loss =  0.00030509738\n",
      "bc1_loss =  0.00010118346\n",
      "bc2_loss =  1.1723291e-05\n",
      "\n",
      "loss limit attained, epoch =  6\n",
      "epoch =  6\n",
      "loss =  0.00039796004\n",
      "eq1_loss =  0.00028035446\n",
      "bc1_loss =  9.2149385e-05\n",
      "bc2_loss =  2.5456202e-05\n",
      "\n",
      "interface_PINN =  0.016965896\n",
      "interface_Analytical =  0.026474588570929703\n",
      "broke inner loop\n",
      "\n",
      "t =  0.05000000000000004\n",
      " \n",
      "slope at interface =  tensor([[-14.3646]])\n",
      "epoch =  0\n",
      "loss =  0.00040540693\n",
      "eq1_loss =  0.00030198935\n",
      "bc1_loss =  9.11952e-05\n",
      "bc2_loss =  1.2222372e-05\n",
      "\n",
      "loss limit attained, epoch =  2\n",
      "epoch =  2\n",
      "loss =  0.00039793312\n",
      "eq1_loss =  0.00028925855\n",
      "bc1_loss =  9.156441e-05\n",
      "bc2_loss =  1.7110162e-05\n",
      "\n",
      "interface_PINN =  0.01709501\n",
      "interface_Analytical =  0.026743373010897513\n",
      "broke inner loop\n",
      "\n",
      "t =  0.05100000000000004\n",
      " \n",
      "slope at interface =  tensor([[-14.1574]])\n",
      "epoch =  0\n",
      "loss =  0.000400848\n",
      "eq1_loss =  0.0003013194\n",
      "bc1_loss =  9.2243245e-05\n",
      "bc2_loss =  7.285339e-06\n",
      "\n",
      "loss limit attained, epoch =  1\n",
      "epoch =  1\n",
      "loss =  0.0003961181\n",
      "eq1_loss =  0.00029271244\n",
      "bc1_loss =  9.393165e-05\n",
      "bc2_loss =  9.473984e-06\n",
      "\n",
      "interface_PINN =  0.017222408\n",
      "interface_Analytical =  0.027009482779201854\n",
      "broke inner loop\n",
      "\n",
      "t =  0.05200000000000004\n",
      " \n",
      "slope at interface =  tensor([[-13.9691]])\n",
      "epoch =  0\n",
      "loss =  0.00039944402\n",
      "eq1_loss =  0.00030022746\n",
      "bc1_loss =  9.6153686e-05\n",
      "bc2_loss =  3.0628905e-06\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.00039944402\n",
      "eq1_loss =  0.00030022746\n",
      "bc1_loss =  9.6153686e-05\n",
      "bc2_loss =  3.0628905e-06\n",
      "\n",
      "interface_PINN =  0.01734823\n",
      "interface_Analytical =  0.02727299616837141\n",
      "broke inner loop\n",
      "\n",
      "t =  0.05300000000000004\n",
      " \n",
      "slope at interface =  tensor([[-13.7963]])\n",
      "epoch =  0\n",
      "loss =  0.00039824963\n",
      "eq1_loss =  0.00029847358\n",
      "bc1_loss =  9.951403e-05\n",
      "bc2_loss =  2.6202594e-07\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.00039824963\n",
      "eq1_loss =  0.00029847358\n",
      "bc1_loss =  9.951403e-05\n",
      "bc2_loss =  2.6202594e-07\n",
      "\n",
      "interface_PINN =  0.017472522\n",
      "interface_Analytical =  0.027533987724265473\n",
      "broke inner loop\n",
      "\n",
      "t =  0.05400000000000004\n",
      " \n",
      "slope at interface =  tensor([[-13.6285]])\n",
      "epoch =  0\n",
      "loss =  0.00040252664\n",
      "eq1_loss =  0.00029780835\n",
      "bc1_loss =  0.00010432764\n",
      "bc2_loss =  3.9064287e-07\n",
      "\n",
      "loss limit attained, epoch =  1\n",
      "epoch =  1\n",
      "loss =  0.00039324092\n",
      "eq1_loss =  0.00028240072\n",
      "bc1_loss =  0.00011083812\n",
      "bc2_loss =  2.0520474e-09\n",
      "\n",
      "interface_PINN =  0.017595252\n",
      "interface_Analytical =  0.027792528492384456\n",
      "broke inner loop\n",
      "\n",
      "t =  0.05500000000000004\n",
      " \n",
      "slope at interface =  tensor([[-13.4572]])\n",
      "epoch =  0\n",
      "loss =  0.0004156775\n",
      "eq1_loss =  0.00029653276\n",
      "bc1_loss =  0.00011806836\n",
      "bc2_loss =  1.0763615e-06\n",
      "\n",
      "loss limit attained, epoch =  2\n",
      "epoch =  2\n",
      "loss =  0.0003965095\n",
      "eq1_loss =  0.00026145755\n",
      "bc1_loss =  0.00013495896\n",
      "bc2_loss =  9.2986795e-08\n",
      "\n",
      "interface_PINN =  0.017716343\n",
      "interface_Analytical =  0.028048686243744143\n",
      "broke inner loop\n",
      "\n",
      "t =  0.05600000000000004\n",
      " \n",
      "slope at interface =  tensor([[-13.2775]])\n",
      "epoch =  0\n",
      "loss =  0.000436716\n",
      "eq1_loss =  0.00029412194\n",
      "bc1_loss =  0.00014223078\n",
      "bc2_loss =  3.63274e-07\n",
      "\n",
      "loss limit attained, epoch =  5\n",
      "epoch =  5\n",
      "loss =  0.00039669545\n",
      "eq1_loss =  0.00023407505\n",
      "bc1_loss =  0.0001571316\n",
      "bc2_loss =  5.4888064e-06\n",
      "\n",
      "interface_PINN =  0.017835172\n",
      "interface_Analytical =  0.028302525682348593\n",
      "broke inner loop\n",
      "\n",
      "t =  0.057000000000000044\n",
      " \n",
      "slope at interface =  tensor([[-13.0294]])\n",
      "epoch =  0\n",
      "loss =  0.00044494218\n",
      "eq1_loss =  0.00029179882\n",
      "bc1_loss =  0.00015162486\n",
      "bc2_loss =  1.5184805e-06\n",
      "\n",
      "loss limit attained, epoch =  7\n",
      "epoch =  7\n",
      "loss =  0.00039689118\n",
      "eq1_loss =  0.00027392973\n",
      "bc1_loss =  0.00010853841\n",
      "bc2_loss =  1.44230535e-05\n",
      "\n",
      "interface_PINN =  0.01795109\n",
      "interface_Analytical =  0.028554108636061497\n",
      "broke inner loop\n",
      "\n",
      "t =  0.058000000000000045\n",
      " \n",
      "slope at interface =  tensor([[-12.7103]])\n",
      "epoch =  0\n",
      "loss =  0.0003998032\n",
      "eq1_loss =  0.00028939825\n",
      "bc1_loss =  0.000103162005\n",
      "bc2_loss =  7.2429284e-06\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.0003998032\n",
      "eq1_loss =  0.00028939825\n",
      "bc1_loss =  0.000103162005\n",
      "bc2_loss =  7.2429284e-06\n",
      "\n",
      "interface_PINN =  0.018065618\n",
      "interface_Analytical =  0.0288034942324712\n",
      "broke inner loop\n",
      "\n",
      "t =  0.059000000000000045\n",
      " \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slope at interface =  tensor([[-12.5580]])\n",
      "epoch =  0\n",
      "loss =  0.0003899782\n",
      "eq1_loss =  0.0002884487\n",
      "bc1_loss =  9.8830234e-05\n",
      "bc2_loss =  2.6992598e-06\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.0003899782\n",
      "eq1_loss =  0.0002884487\n",
      "bc1_loss =  9.8830234e-05\n",
      "bc2_loss =  2.6992598e-06\n",
      "\n",
      "interface_PINN =  0.018178806\n",
      "interface_Analytical =  0.02905073906116679\n",
      "broke inner loop\n",
      "\n",
      "t =  0.060000000000000046\n",
      " \n",
      "slope at interface =  tensor([[-12.4108]])\n",
      "epoch =  0\n",
      "loss =  0.00038392952\n",
      "eq1_loss =  0.00028748036\n",
      "bc1_loss =  9.599711e-05\n",
      "bc2_loss =  4.520416e-07\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.00038392952\n",
      "eq1_loss =  0.00028748036\n",
      "bc1_loss =  9.599711e-05\n",
      "bc2_loss =  4.520416e-07\n",
      "\n",
      "interface_PINN =  0.018290695\n",
      "interface_Analytical =  0.029295897323686846\n",
      "broke inner loop\n",
      "\n",
      "t =  0.06100000000000005\n",
      " \n",
      "slope at interface =  tensor([[-12.2686]])\n",
      "epoch =  0\n",
      "loss =  0.00038064455\n",
      "eq1_loss =  0.00028562522\n",
      "bc1_loss =  9.4976676e-05\n",
      "bc2_loss =  4.2630063e-08\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.00038064455\n",
      "eq1_loss =  0.00028562522\n",
      "bc1_loss =  9.4976676e-05\n",
      "bc2_loss =  4.2630063e-08\n",
      "\n",
      "interface_PINN =  0.01840133\n",
      "interface_Analytical =  0.02953902097226653\n",
      "broke inner loop\n",
      "\n",
      "t =  0.06200000000000005\n",
      " \n",
      "slope at interface =  tensor([[-12.1311]])\n",
      "epoch =  0\n",
      "loss =  0.00038160957\n",
      "eq1_loss =  0.000284719\n",
      "bc1_loss =  9.592004e-05\n",
      "bc2_loss =  9.705118e-07\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.00038160957\n",
      "eq1_loss =  0.000284719\n",
      "bc1_loss =  9.592004e-05\n",
      "bc2_loss =  9.705118e-07\n",
      "\n",
      "interface_PINN =  0.018510753\n",
      "interface_Analytical =  0.02978015983838908\n",
      "broke inner loop\n",
      "\n",
      "t =  0.06300000000000004\n",
      " \n",
      "slope at interface =  tensor([[-11.9981]])\n",
      "epoch =  0\n",
      "loss =  0.0003853342\n",
      "eq1_loss =  0.00028367058\n",
      "bc1_loss =  9.890609e-05\n",
      "bc2_loss =  2.7575438e-06\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.0003853342\n",
      "eq1_loss =  0.00028367058\n",
      "bc1_loss =  9.890609e-05\n",
      "bc2_loss =  2.7575438e-06\n",
      "\n",
      "interface_PINN =  0.018619\n",
      "interface_Analytical =  0.030019361752042662\n",
      "broke inner loop\n",
      "\n",
      "t =  0.06400000000000004\n",
      " \n",
      "slope at interface =  tensor([[-11.8693]])\n",
      "epoch =  0\n",
      "loss =  0.00039151905\n",
      "eq1_loss =  0.00028259488\n",
      "bc1_loss =  0.000103933504\n",
      "bc2_loss =  4.9906757e-06\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.00039151905\n",
      "eq1_loss =  0.00028259488\n",
      "bc1_loss =  0.000103933504\n",
      "bc2_loss =  4.9906757e-06\n",
      "\n",
      "interface_PINN =  0.018726107\n",
      "interface_Analytical =  0.030256672652491087\n",
      "broke inner loop\n",
      "\n",
      "t =  0.06500000000000004\n",
      " \n",
      "slope at interface =  tensor([[-11.7441]])\n",
      "epoch =  0\n",
      "loss =  0.0003997636\n",
      "eq1_loss =  0.00028149353\n",
      "bc1_loss =  0.00011092348\n",
      "bc2_loss =  7.346602e-06\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.0003997636\n",
      "eq1_loss =  0.00028149353\n",
      "bc1_loss =  0.00011092348\n",
      "bc2_loss =  7.346602e-06\n",
      "\n",
      "interface_PINN =  0.018832102\n",
      "interface_Analytical =  0.030492136691284883\n",
      "broke inner loop\n",
      "\n",
      "t =  0.06600000000000004\n",
      " \n",
      "slope at interface =  tensor([[-11.6223]])\n",
      "epoch =  0\n",
      "loss =  0.00040966744\n",
      "eq1_loss =  0.00028036925\n",
      "bc1_loss =  0.000119698285\n",
      "bc2_loss =  9.5998885e-06\n",
      "\n",
      "loss limit attained, epoch =  1\n",
      "epoch =  1\n",
      "loss =  0.00039350396\n",
      "eq1_loss =  0.00025854004\n",
      "bc1_loss =  0.0001299977\n",
      "bc2_loss =  4.9662053e-06\n",
      "\n",
      "interface_PINN =  0.018936902\n",
      "interface_Analytical =  0.03072579632816702\n",
      "broke inner loop\n",
      "\n",
      "t =  0.06700000000000005\n",
      " \n",
      "slope at interface =  tensor([[-11.4911]])\n",
      "epoch =  0\n",
      "loss =  0.00042502076\n",
      "eq1_loss =  0.00027822788\n",
      "bc1_loss =  0.00014040836\n",
      "bc2_loss =  6.3845164e-06\n",
      "\n",
      "loss limit attained, epoch =  2\n",
      "epoch =  2\n",
      "loss =  0.00039772302\n",
      "eq1_loss =  0.00023672951\n",
      "bc1_loss =  0.00016041817\n",
      "bc2_loss =  5.753658e-07\n",
      "\n",
      "interface_PINN =  0.019040354\n",
      "interface_Analytical =  0.03095769242046316\n",
      "broke inner loop\n",
      "\n",
      "t =  0.06800000000000005\n",
      " \n",
      "slope at interface =  tensor([[-11.3434]])\n",
      "epoch =  0\n",
      "loss =  0.00044449986\n",
      "eq1_loss =  0.0002761972\n",
      "bc1_loss =  0.00016711184\n",
      "bc2_loss =  1.1908087e-06\n",
      "\n",
      "loss limit attained, epoch =  5\n",
      "epoch =  5\n",
      "loss =  0.00039173756\n",
      "eq1_loss =  0.0002244648\n",
      "bc1_loss =  0.00016241433\n",
      "bc2_loss =  4.8584084e-06\n",
      "\n",
      "interface_PINN =  0.019141717\n",
      "interface_Analytical =  0.031187864306489503\n",
      "broke inner loop\n",
      "\n",
      "t =  0.06900000000000005\n",
      " \n",
      "slope at interface =  tensor([[-11.1143]])\n",
      "epoch =  0\n",
      "loss =  0.00042834657\n",
      "eq1_loss =  0.0002737127\n",
      "bc1_loss =  0.0001522979\n",
      "bc2_loss =  2.3359526e-06\n",
      "\n",
      "loss limit attained, epoch =  4\n",
      "epoch =  4\n",
      "loss =  0.00039454084\n",
      "eq1_loss =  0.00027296713\n",
      "bc1_loss =  0.00011211437\n",
      "bc2_loss =  9.459313e-06\n",
      "\n",
      "interface_PINN =  0.019241106\n",
      "interface_Analytical =  0.0314163498834604\n",
      "broke inner loop\n",
      "\n",
      "t =  0.07000000000000005\n",
      " \n",
      "slope at interface =  tensor([[-10.8980]])\n",
      "epoch =  0\n",
      "loss =  0.00038132956\n",
      "eq1_loss =  0.00027184884\n",
      "bc1_loss =  0.00010380716\n",
      "bc2_loss =  5.6735466e-06\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.00038132956\n",
      "eq1_loss =  0.00027184884\n",
      "bc1_loss =  0.00010380716\n",
      "bc2_loss =  5.6735466e-06\n",
      "\n",
      "interface_PINN =  0.019339459\n",
      "interface_Analytical =  0.03164318568033254\n",
      "broke inner loop\n",
      "\n",
      "t =  0.07100000000000005\n",
      " \n",
      "slope at interface =  tensor([[-10.7843]])\n",
      "epoch =  0\n",
      "loss =  0.0003704706\n",
      "eq1_loss =  0.0002708037\n",
      "bc1_loss =  9.668981e-05\n",
      "bc2_loss =  2.9771268e-06\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.0003704706\n",
      "eq1_loss =  0.0002708037\n",
      "bc1_loss =  9.668981e-05\n",
      "bc2_loss =  2.9771268e-06\n",
      "\n",
      "interface_PINN =  0.01943681\n",
      "interface_Analytical =  0.03186840692598238\n",
      "broke inner loop\n",
      "\n",
      "t =  0.07200000000000005\n",
      " \n",
      "slope at interface =  tensor([[-10.6744]])\n",
      "epoch =  0\n",
      "loss =  0.0003621156\n",
      "eq1_loss =  0.00026975054\n",
      "bc1_loss =  9.110643e-05\n",
      "bc2_loss =  1.2586116e-06\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.0003621156\n",
      "eq1_loss =  0.00026975054\n",
      "bc1_loss =  9.110643e-05\n",
      "bc2_loss =  1.2586116e-06\n",
      "\n",
      "interface_PINN =  0.019533193\n",
      "interface_Analytical =  0.03209204761307702\n",
      "broke inner loop\n",
      "\n",
      "t =  0.07300000000000005\n",
      " \n",
      "slope at interface =  tensor([[-10.5682]])\n",
      "epoch =  0\n",
      "loss =  0.00035627597\n",
      "eq1_loss =  0.0002686867\n",
      "bc1_loss =  8.724986e-05\n",
      "bc2_loss =  3.3939455e-07\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.00035627597\n",
      "eq1_loss =  0.0002686867\n",
      "bc1_loss =  8.724986e-05\n",
      "bc2_loss =  3.3939455e-07\n",
      "\n",
      "interface_PINN =  0.01962864\n",
      "interface_Analytical =  0.03231414055796629\n",
      "broke inner loop\n",
      "\n",
      "t =  0.07400000000000005\n",
      " \n",
      "slope at interface =  tensor([[-10.4657]])\n",
      "epoch =  0\n",
      "loss =  0.0003528302\n",
      "eq1_loss =  0.0002676105\n",
      "bc1_loss =  8.520643e-05\n",
      "bc2_loss =  1.3233503e-08\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.0003528302\n",
      "eq1_loss =  0.0002676105\n",
      "bc1_loss =  8.520643e-05\n",
      "bc2_loss =  1.3233503e-08\n",
      "\n",
      "interface_PINN =  0.019723183\n",
      "interface_Analytical =  0.032534717456895214\n",
      "broke inner loop\n",
      "\n",
      "t =  0.07500000000000005\n",
      " \n",
      "slope at interface =  tensor([[-10.3666]])\n",
      "epoch =  0\n",
      "loss =  0.0003515607\n",
      "eq1_loss =  0.00026652124\n",
      "bc1_loss =  8.496232e-05\n",
      "bc2_loss =  7.714931e-08\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.0003515607\n",
      "eq1_loss =  0.00026652124\n",
      "bc1_loss =  8.496232e-05\n",
      "bc2_loss =  7.714931e-08\n",
      "\n",
      "interface_PINN =  0.019816851\n",
      "interface_Analytical =  0.03275380893880897\n",
      "broke inner loop\n",
      "\n",
      "t =  0.07600000000000005\n",
      " \n",
      "slope at interface =  tensor([[-10.2707]])\n",
      "epoch =  0\n",
      "loss =  0.0003522248\n",
      "eq1_loss =  0.00026541913\n",
      "bc1_loss =  8.6449974e-05\n",
      "bc2_loss =  3.556978e-07\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.0003522248\n",
      "eq1_loss =  0.00026541913\n",
      "bc1_loss =  8.6449974e-05\n",
      "bc2_loss =  3.556978e-07\n",
      "\n",
      "interface_PINN =  0.019909672\n",
      "interface_Analytical =  0.032971444614999845\n",
      "broke inner loop\n",
      "\n",
      "t =  0.07700000000000005\n",
      " \n",
      "slope at interface =  tensor([[-10.1778]])\n",
      "epoch =  0\n",
      "loss =  0.0003545746\n",
      "eq1_loss =  0.00026430513\n",
      "bc1_loss =  8.9556495e-05\n",
      "bc2_loss =  7.129428e-07\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.0003545746\n",
      "eq1_loss =  0.00026430513\n",
      "bc1_loss =  8.9556495e-05\n",
      "bc2_loss =  7.129428e-07\n",
      "\n",
      "interface_PINN =  0.02000167\n",
      "interface_Analytical =  0.03318765312582381\n",
      "broke inner loop\n",
      "\n",
      "t =  0.07800000000000006\n",
      " \n",
      "slope at interface =  tensor([[-10.0875]])\n",
      "epoch =  0\n",
      "loss =  0.00035836388\n",
      "eq1_loss =  0.00026318085\n",
      "bc1_loss =  9.4125855e-05\n",
      "bc2_loss =  1.0571543e-06\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.00035836388\n",
      "eq1_loss =  0.00026318085\n",
      "bc1_loss =  9.4125855e-05\n",
      "bc2_loss =  1.0571543e-06\n",
      "\n",
      "interface_PINN =  0.020092865\n",
      "interface_Analytical =  0.03340246218469534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broke inner loop\n",
      "\n",
      "t =  0.07900000000000006\n",
      " \n",
      "slope at interface =  tensor([[-9.9995]])\n",
      "epoch =  0\n",
      "loss =  0.000363324\n",
      "eq1_loss =  0.00026204833\n",
      "bc1_loss =  9.994021e-05\n",
      "bc2_loss =  1.3354456e-06\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.000363324\n",
      "eq1_loss =  0.00026204833\n",
      "bc1_loss =  9.994021e-05\n",
      "bc2_loss =  1.3354456e-06\n",
      "\n",
      "interface_PINN =  0.020183276\n",
      "interface_Analytical =  0.03361589861955206\n",
      "broke inner loop\n",
      "\n",
      "t =  0.08000000000000006\n",
      " \n",
      "slope at interface =  tensor([[-9.9135]])\n",
      "epoch =  0\n",
      "loss =  0.00036917048\n",
      "eq1_loss =  0.00026090984\n",
      "bc1_loss =  0.00010673274\n",
      "bc2_loss =  1.5278965e-06\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.00036917048\n",
      "eq1_loss =  0.00026090984\n",
      "bc1_loss =  0.00010673274\n",
      "bc2_loss =  1.5278965e-06\n",
      "\n",
      "interface_PINN =  0.020272918\n",
      "interface_Analytical =  0.03382798841196447\n",
      "broke inner loop\n",
      "\n",
      "t =  0.08100000000000006\n",
      " \n",
      "slope at interface =  tensor([[-9.8292]])\n",
      "epoch =  0\n",
      "loss =  0.00037558883\n",
      "eq1_loss =  0.00025976766\n",
      "bc1_loss =  0.00011418381\n",
      "bc2_loss =  1.637357e-06\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.00037558883\n",
      "eq1_loss =  0.00025976766\n",
      "bc1_loss =  0.00011418381\n",
      "bc2_loss =  1.637357e-06\n",
      "\n",
      "interface_PINN =  0.020361803\n",
      "interface_Analytical =  0.034038756734052475\n",
      "broke inner loop\n",
      "\n",
      "t =  0.08200000000000006\n",
      " \n",
      "slope at interface =  tensor([[-9.7463]])\n",
      "epoch =  0\n",
      "loss =  0.0003822455\n",
      "eq1_loss =  0.00025862406\n",
      "bc1_loss =  0.00012194417\n",
      "bc2_loss =  1.6772574e-06\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.0003822455\n",
      "eq1_loss =  0.00025862406\n",
      "bc1_loss =  0.00012194417\n",
      "bc2_loss =  1.6772574e-06\n",
      "\n",
      "interface_PINN =  0.020449944\n",
      "interface_Analytical =  0.034248227983357075\n",
      "broke inner loop\n",
      "\n",
      "t =  0.08300000000000006\n",
      " \n",
      "slope at interface =  tensor([[-9.6646]])\n",
      "epoch =  0\n",
      "loss =  0.00038877878\n",
      "eq1_loss =  0.00025748098\n",
      "bc1_loss =  0.00012962826\n",
      "bc2_loss =  1.6695469e-06\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.00038877878\n",
      "eq1_loss =  0.00025748098\n",
      "bc1_loss =  0.00012962826\n",
      "bc2_loss =  1.6695469e-06\n",
      "\n",
      "interface_PINN =  0.020537348\n",
      "interface_Analytical =  0.03445642581580397\n",
      "broke inner loop\n",
      "\n",
      "t =  0.08400000000000006\n",
      " \n",
      "slope at interface =  tensor([[-9.5838]])\n",
      "epoch =  0\n",
      "loss =  0.0003956298\n",
      "eq1_loss =  0.0002571484\n",
      "bc1_loss =  0.00013684617\n",
      "bc2_loss =  1.6352221e-06\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.0003956298\n",
      "eq1_loss =  0.0002571484\n",
      "bc1_loss =  0.00013684617\n",
      "bc2_loss =  1.6352221e-06\n",
      "\n",
      "interface_PINN =  0.020624023\n",
      "interface_Analytical =  0.03466337317688518\n",
      "broke inner loop\n",
      "\n",
      "t =  0.08500000000000006\n",
      " \n",
      "slope at interface =  tensor([[-9.5039]])\n",
      "epoch =  0\n",
      "loss =  0.00040093236\n",
      "eq1_loss =  0.00025604005\n",
      "bc1_loss =  0.0001433019\n",
      "bc2_loss =  1.5904112e-06\n",
      "\n",
      "loss limit attained, epoch =  1\n",
      "epoch =  1\n",
      "loss =  0.0003885027\n",
      "eq1_loss =  0.00023961876\n",
      "bc1_loss =  0.00014868291\n",
      "bc2_loss =  2.0101426e-07\n",
      "\n",
      "interface_PINN =  0.02070984\n",
      "interface_Analytical =  0.03486909233117493\n",
      "broke inner loop\n",
      "\n",
      "t =  0.08600000000000006\n",
      " \n",
      "slope at interface =  tensor([[-9.4098]])\n",
      "epoch =  0\n",
      "loss =  0.0004069029\n",
      "eq1_loss =  0.00025462435\n",
      "bc1_loss =  0.00015205672\n",
      "bc2_loss =  2.2183714e-07\n",
      "\n",
      "loss limit attained, epoch =  1\n",
      "epoch =  1\n",
      "loss =  0.00039537047\n",
      "eq1_loss =  0.00024153045\n",
      "bc1_loss =  0.00015376668\n",
      "bc2_loss =  7.3356205e-08\n",
      "\n",
      "interface_PINN =  0.020794783\n",
      "interface_Analytical =  0.035073604890287546\n",
      "broke inner loop\n",
      "\n",
      "t =  0.08700000000000006\n",
      " \n",
      "slope at interface =  tensor([[-9.3139]])\n",
      "epoch =  0\n",
      "loss =  0.0004065295\n",
      "eq1_loss =  0.00025324553\n",
      "bc1_loss =  0.00015324974\n",
      "bc2_loss =  3.4229743e-08\n",
      "\n",
      "loss limit attained, epoch =  1\n",
      "epoch =  1\n",
      "loss =  0.00039550348\n",
      "eq1_loss =  0.0002437449\n",
      "bc1_loss =  0.00015104705\n",
      "bc2_loss =  7.115343e-07\n",
      "\n",
      "interface_PINN =  0.02087884\n",
      "interface_Analytical =  0.03527693183937632\n",
      "broke inner loop\n",
      "\n",
      "t =  0.08800000000000006\n",
      " \n",
      "slope at interface =  tensor([[-9.2168]])\n",
      "epoch =  0\n",
      "loss =  0.00039935022\n",
      "eq1_loss =  0.00025190317\n",
      "bc1_loss =  0.00014696685\n",
      "bc2_loss =  4.8019854e-07\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.00039935022\n",
      "eq1_loss =  0.00025190317\n",
      "bc1_loss =  0.00014696685\n",
      "bc2_loss =  4.8019854e-07\n",
      "\n",
      "interface_PINN =  0.02096218\n",
      "interface_Analytical =  0.03547909356226569\n",
      "broke inner loop\n",
      "\n",
      "t =  0.08900000000000007\n",
      " \n",
      "slope at interface =  tensor([[-9.1381]])\n",
      "epoch =  0\n",
      "loss =  0.00039275415\n",
      "eq1_loss =  0.00025081844\n",
      "bc1_loss =  0.00014165984\n",
      "bc2_loss =  2.75873e-07\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.00039275415\n",
      "eq1_loss =  0.00025081844\n",
      "bc1_loss =  0.00014165984\n",
      "bc2_loss =  2.75873e-07\n",
      "\n",
      "interface_PINN =  0.021044815\n",
      "interface_Analytical =  0.03568010986530174\n",
      "broke inner loop\n",
      "\n",
      "t =  0.09000000000000007\n",
      " \n",
      "slope at interface =  tensor([[-9.0607]])\n",
      "epoch =  0\n",
      "loss =  0.00038544164\n",
      "eq1_loss =  0.0002497394\n",
      "bc1_loss =  0.00013558009\n",
      "bc2_loss =  1.2216537e-07\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.00038544164\n",
      "eq1_loss =  0.0002497394\n",
      "bc1_loss =  0.00013558009\n",
      "bc2_loss =  1.2216537e-07\n",
      "\n",
      "interface_PINN =  0.021126755\n",
      "interface_Analytical =  0.03588000000000004\n",
      "broke inner loop\n",
      "\n",
      "t =  0.09100000000000007\n",
      " \n",
      "slope at interface =  tensor([[-8.9846]])\n",
      "epoch =  0\n",
      "loss =  0.0003778622\n",
      "eq1_loss =  0.0002486645\n",
      "bc1_loss =  0.00012916721\n",
      "bc2_loss =  3.049969e-08\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.0003778622\n",
      "eq1_loss =  0.0002486645\n",
      "bc1_loss =  0.00012916721\n",
      "bc2_loss =  3.049969e-08\n",
      "\n",
      "interface_PINN =  0.021208014\n",
      "interface_Analytical =  0.036078782684564104\n",
      "broke inner loop\n",
      "\n",
      "t =  0.09200000000000007\n",
      " \n",
      "slope at interface =  tensor([[-8.9100]])\n",
      "epoch =  0\n",
      "loss =  0.00037039394\n",
      "eq1_loss =  0.0002475925\n",
      "bc1_loss =  0.00012280133\n",
      "bc2_loss =  8.645884e-11\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.00037039394\n",
      "eq1_loss =  0.0002475925\n",
      "bc1_loss =  0.00012280133\n",
      "bc2_loss =  8.645884e-11\n",
      "\n",
      "interface_PINN =  0.021288607\n",
      "interface_Analytical =  0.036276476124342656\n",
      "broke inner loop\n",
      "\n",
      "t =  0.09300000000000007\n",
      " \n",
      "slope at interface =  tensor([[-8.8370]])\n",
      "epoch =  0\n",
      "loss =  0.000364208\n",
      "eq1_loss =  0.0002473991\n",
      "bc1_loss =  0.00011678948\n",
      "bc2_loss =  1.9453239e-08\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.000364208\n",
      "eq1_loss =  0.0002473991\n",
      "bc1_loss =  0.00011678948\n",
      "bc2_loss =  1.9453239e-08\n",
      "\n",
      "interface_PINN =  0.02136855\n",
      "interface_Analytical =  0.0364730980312888\n",
      "broke inner loop\n",
      "\n",
      "t =  0.09400000000000007\n",
      " \n",
      "slope at interface =  tensor([[-8.7656]])\n",
      "epoch =  0\n",
      "loss =  0.00035777275\n",
      "eq1_loss =  0.00024629149\n",
      "bc1_loss =  0.00011141115\n",
      "bc2_loss =  7.009989e-08\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.00035777275\n",
      "eq1_loss =  0.00024629149\n",
      "bc1_loss =  0.00011141115\n",
      "bc2_loss =  7.009989e-08\n",
      "\n",
      "interface_PINN =  0.021447858\n",
      "interface_Analytical =  0.036668665642480135\n",
      "broke inner loop\n",
      "\n",
      "t =  0.09500000000000007\n",
      " \n",
      "slope at interface =  tensor([[-8.6959]])\n",
      "epoch =  0\n",
      "loss =  0.00035216453\n",
      "eq1_loss =  0.0002452195\n",
      "bc1_loss =  0.00010681404\n",
      "bc2_loss =  1.3098565e-07\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.00035216453\n",
      "eq1_loss =  0.0002452195\n",
      "bc1_loss =  0.00010681404\n",
      "bc2_loss =  1.3098565e-07\n",
      "\n",
      "interface_PINN =  0.021526545\n",
      "interface_Analytical =  0.03686319573775452\n",
      "broke inner loop\n",
      "\n",
      "t =  0.09600000000000007\n",
      " \n",
      "slope at interface =  tensor([[-8.6280]])\n",
      "epoch =  0\n",
      "loss =  0.0003474131\n",
      "eq1_loss =  0.00024414744\n",
      "bc1_loss =  0.00010308211\n",
      "bc2_loss =  1.8355928e-07\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.0003474131\n",
      "eq1_loss =  0.00024414744\n",
      "bc1_loss =  0.00010308211\n",
      "bc2_loss =  1.8355928e-07\n",
      "\n",
      "interface_PINN =  0.021604627\n",
      "interface_Analytical =  0.037056704656512604\n",
      "broke inner loop\n",
      "\n",
      "t =  0.09700000000000007\n",
      " \n",
      "slope at interface =  tensor([[-8.5617]])\n",
      "epoch =  0\n",
      "loss =  0.00034354234\n",
      "eq1_loss =  0.0002430754\n",
      "bc1_loss =  0.00010025269\n",
      "bc2_loss =  2.1426685e-07\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.00034354234\n",
      "eq1_loss =  0.0002430754\n",
      "bc1_loss =  0.00010025269\n",
      "bc2_loss =  2.1426685e-07\n",
      "\n",
      "interface_PINN =  0.02168212\n",
      "interface_Analytical =  0.03724920831373471\n",
      "broke inner loop\n",
      "\n",
      "t =  0.09800000000000007\n",
      " \n",
      "slope at interface =  tensor([[-8.4970]])\n",
      "epoch =  0\n",
      "loss =  0.0003405225\n",
      "eq1_loss =  0.00024200353\n",
      "bc1_loss =  9.8302386e-05\n",
      "bc2_loss =  2.165907e-07\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.0003405225\n",
      "eq1_loss =  0.00024200353\n",
      "bc1_loss =  9.8302386e-05\n",
      "bc2_loss =  2.165907e-07\n",
      "\n",
      "interface_PINN =  0.021759039\n",
      "interface_Analytical =  0.037440722215256524\n",
      "broke inner loop\n",
      "\n",
      "t =  0.09900000000000007\n",
      " \n",
      "slope at interface =  tensor([[-8.4339]])\n",
      "epoch =  0\n",
      "loss =  0.00033913917\n",
      "eq1_loss =  0.00024177901\n",
      "bc1_loss =  9.716865e-05\n",
      "bc2_loss =  1.915089e-07\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.00033913917\n",
      "eq1_loss =  0.00024177901\n",
      "bc1_loss =  9.716865e-05\n",
      "bc2_loss =  1.915089e-07\n",
      "\n",
      "interface_PINN =  0.021835394\n",
      "interface_Analytical =  0.037631261472345086\n",
      "broke inner loop\n",
      "\n",
      "t =  0.10000000000000007\n",
      " \n",
      "slope at interface =  tensor([[-8.3723]])\n",
      "epoch =  0\n",
      "loss =  0.00033763127\n",
      "eq1_loss =  0.00024069041\n",
      "bc1_loss =  9.679533e-05\n",
      "bc2_loss =  1.4551915e-07\n",
      "\n",
      "loss limit attained, epoch =  0\n",
      "epoch =  0\n",
      "loss =  0.00033763127\n",
      "eq1_loss =  0.00024069041\n",
      "bc1_loss =  9.679533e-05\n",
      "bc2_loss =  1.4551915e-07\n",
      "\n",
      "interface_PINN =  0.021911202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interface_Analytical =  0.037820840815613856\n",
      "broke inner loop\n",
      "\n",
      "time elapsed =  1026.2828087806702\n"
     ]
    }
   ],
   "source": [
    "N_x = 3001\n",
    "N_bc = 30\n",
    "N_t = 100\n",
    "del_t = 0.001\n",
    "x_l = 0\n",
    "x_r = 0.28\n",
    "T_r = 0\n",
    "T_l = 0.5\n",
    "t_i = 0\n",
    "accuracy_cap = 0.00035\n",
    "N_x_test = 251\n",
    "s_initial = 0.0015\n",
    "\n",
    "# Neural network params\n",
    "layer_size = [1, 3, 3, 1]\n",
    "\n",
    "# material params\n",
    "k1 = 0.01\n",
    "k2 = 0.00912\n",
    "\n",
    "# Training data and initial data\n",
    "model = ANN(layer_size)\n",
    "print(model)\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total trainable parameters in the model:\", total_trainable_params)\n",
    "\n",
    "# # Setup Loss function and Optimiser\n",
    "lr = 1e-4\n",
    "epochs = 90001\n",
    "optimiser1 = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training model\n",
    "start = time.time()\n",
    "loss_store, T_store_pred, T_store_an, x_test_np, s_pred, s_an, t = train_model(model, optimiser1, epochs, T_r, T_l, k1, k2, N_x, x_l, x_r, N_t, N_bc, accuracy_cap, N_x_test, del_t, s_initial)\n",
    "end = time.time()\n",
    "time_elapsed = end - start\n",
    "print(\"time elapsed = \", time_elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb512346",
   "metadata": {},
   "source": [
    "# Results Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6502e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = 30\n",
    "i = 0\n",
    "k = 201\n",
    "\n",
    "plt.plot(x_test_np[i:k].detach().numpy(), T_store_pred[j][i:k])\n",
    "plt.plot(x_test_np[i:k].detach().numpy(), T_store_an[j-1][i:k])\n",
    "Title = \"Stefan 1-phase using Time stepping, \" + \"time = \" + str((j+1)*del_t) + \", delta_t = \" + str(del_t)\n",
    "plt.title(Title)\n",
    "plt.legend([\"PINN\", \"Analytical\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb4b708d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tp = []\n",
    "for i in range(len(s_pred)):\n",
    "    tp.append(s_pred[i][0])\n",
    "\n",
    "i = 0\n",
    "j = 100\n",
    "plt.plot(t[i:j], tp[i:j])\n",
    "plt.plot(t[i:j], s_an[i:j])\n",
    "plt.legend([\"PINN\", \"Analytical\"])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Interface position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f7028ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0.0037820840815613846,\n",
       " 0.005348674602179501,\n",
       " 0.006550761787761791,\n",
       " 0.007564168163122769,\n",
       " 0.008456997102991115,\n",
       " 0.009264176164128148,\n",
       " 0.010006453917347552,\n",
       " 0.010697349204359002,\n",
       " 0.011346252244684153,\n",
       " 0.011960000000000009,\n",
       " 0.012543753824115022,\n",
       " 0.013101523575523586,\n",
       " 0.013636498084185702,\n",
       " 0.014151262841174295,\n",
       " 0.014647948661843418,\n",
       " 0.01512833632624554,\n",
       " 0.015593932153244752,\n",
       " 0.016046023806538506,\n",
       " 0.016485722307499923,\n",
       " 0.016913994205982233,\n",
       " 0.017331686588442585,\n",
       " 0.017739546781132844,\n",
       " 0.018138238062171328,\n",
       " 0.018528352328256302,\n",
       " 0.018910420407806928,\n",
       " 0.019284920533930147,\n",
       " 0.01965228536328538,\n",
       " 0.020012907834695108,\n",
       " 0.020367146093647998,\n",
       " 0.020715327658523797,\n",
       " 0.021057752966544198,\n",
       " 0.021394698408718014,\n",
       " 0.02172641894100362,\n",
       " 0.02205315034184461,\n",
       " 0.022375111172908194,\n",
       " 0.022692504489368315,\n",
       " 0.023005519337758955,\n",
       " 0.023314332072783068,\n",
       " 0.023619107519125295,\n",
       " 0.023920000000000025,\n",
       " 0.024217154250654665,\n",
       " 0.024510706232175387,\n",
       " 0.02480078385857998,\n",
       " 0.025087507648230055,\n",
       " 0.025370991308973353,\n",
       " 0.025651342265074577,\n",
       " 0.02592866213285987,\n",
       " 0.026203047151047175,\n",
       " 0.026474588570929703,\n",
       " 0.026743373010897513,\n",
       " 0.027009482779201854,\n",
       " 0.02727299616837141,\n",
       " 0.027533987724265473,\n",
       " 0.027792528492384456,\n",
       " 0.028048686243744143,\n",
       " 0.028302525682348593,\n",
       " 0.028554108636061497,\n",
       " 0.0288034942324712,\n",
       " 0.02905073906116679,\n",
       " 0.029295897323686846,\n",
       " 0.02953902097226653,\n",
       " 0.02978015983838908,\n",
       " 0.030019361752042662,\n",
       " 0.030256672652491087,\n",
       " 0.030492136691284883,\n",
       " 0.03072579632816702,\n",
       " 0.03095769242046316,\n",
       " 0.031187864306489503,\n",
       " 0.0314163498834604,\n",
       " 0.03164318568033254,\n",
       " 0.03186840692598238,\n",
       " 0.03209204761307702,\n",
       " 0.03231414055796629,\n",
       " 0.032534717456895214,\n",
       " 0.03275380893880897,\n",
       " 0.032971444614999845,\n",
       " 0.03318765312582381,\n",
       " 0.03340246218469534,\n",
       " 0.03361589861955206,\n",
       " 0.03382798841196447,\n",
       " 0.034038756734052475,\n",
       " 0.034248227983357075,\n",
       " 0.03445642581580397,\n",
       " 0.03466337317688518,\n",
       " 0.03486909233117493,\n",
       " 0.035073604890287546,\n",
       " 0.03527693183937632,\n",
       " 0.03547909356226569,\n",
       " 0.03568010986530174,\n",
       " 0.03588000000000004,\n",
       " 0.036078782684564104,\n",
       " 0.036276476124342656,\n",
       " 0.0364730980312888,\n",
       " 0.036668665642480135,\n",
       " 0.03686319573775452,\n",
       " 0.037056704656512604,\n",
       " 0.03724920831373471,\n",
       " 0.037440722215256524,\n",
       " 0.037631261472345086,\n",
       " 0.037820840815613856]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d752a714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.0015], dtype=float32),\n",
       " array([0.00301695], dtype=float32),\n",
       " array([0.00407571], dtype=float32),\n",
       " array([0.00492018], dtype=float32),\n",
       " array([0.00563648], dtype=float32),\n",
       " array([0.00626497], dtype=float32),\n",
       " array([0.00682855], dtype=float32),\n",
       " array([0.0073418], dtype=float32),\n",
       " array([0.00781463], dtype=float32),\n",
       " array([0.00825417], dtype=float32),\n",
       " array([0.00866561], dtype=float32),\n",
       " array([0.00905288], dtype=float32),\n",
       " array([0.00941904], dtype=float32),\n",
       " array([0.00976661], dtype=float32),\n",
       " array([0.01009769], dtype=float32),\n",
       " array([0.01041401], dtype=float32),\n",
       " array([0.01071691], dtype=float32),\n",
       " array([0.01100752], dtype=float32),\n",
       " array([0.01128685], dtype=float32),\n",
       " array([0.01155578], dtype=float32),\n",
       " array([0.01181513], dtype=float32),\n",
       " array([0.01206561], dtype=float32),\n",
       " array([0.01230793], dtype=float32),\n",
       " array([0.01254254], dtype=float32),\n",
       " array([0.01276991], dtype=float32),\n",
       " array([0.01299051], dtype=float32),\n",
       " array([0.01320475], dtype=float32),\n",
       " array([0.01341305], dtype=float32),\n",
       " array([0.01361584], dtype=float32),\n",
       " array([0.01381329], dtype=float32),\n",
       " array([0.01400617], dtype=float32),\n",
       " array([0.01419413], dtype=float32),\n",
       " array([0.01437834], dtype=float32),\n",
       " array([0.01455637], dtype=float32),\n",
       " array([0.01473018], dtype=float32),\n",
       " array([0.01490033], dtype=float32),\n",
       " array([0.01506681], dtype=float32),\n",
       " array([0.01522978], dtype=float32),\n",
       " array([0.01538944], dtype=float32),\n",
       " array([0.0155468], dtype=float32),\n",
       " array([0.01570024], dtype=float32),\n",
       " array([0.01585113], dtype=float32),\n",
       " array([0.01599893], dtype=float32),\n",
       " array([0.01614427], dtype=float32),\n",
       " array([0.01628689], dtype=float32),\n",
       " array([0.01642712], dtype=float32),\n",
       " array([0.01656531], dtype=float32),\n",
       " array([0.0167014], dtype=float32),\n",
       " array([0.01683489], dtype=float32),\n",
       " array([0.0169659], dtype=float32),\n",
       " array([0.01709501], dtype=float32),\n",
       " array([0.01722241], dtype=float32),\n",
       " array([0.01734823], dtype=float32),\n",
       " array([0.01747252], dtype=float32),\n",
       " array([0.01759525], dtype=float32),\n",
       " array([0.01771634], dtype=float32),\n",
       " array([0.01783517], dtype=float32),\n",
       " array([0.01795109], dtype=float32),\n",
       " array([0.01806562], dtype=float32),\n",
       " array([0.01817881], dtype=float32),\n",
       " array([0.01829069], dtype=float32),\n",
       " array([0.01840133], dtype=float32),\n",
       " array([0.01851075], dtype=float32),\n",
       " array([0.018619], dtype=float32),\n",
       " array([0.01872611], dtype=float32),\n",
       " array([0.0188321], dtype=float32),\n",
       " array([0.0189369], dtype=float32),\n",
       " array([0.01904035], dtype=float32),\n",
       " array([0.01914172], dtype=float32),\n",
       " array([0.01924111], dtype=float32),\n",
       " array([0.01933946], dtype=float32),\n",
       " array([0.01943681], dtype=float32),\n",
       " array([0.01953319], dtype=float32),\n",
       " array([0.01962864], dtype=float32),\n",
       " array([0.01972318], dtype=float32),\n",
       " array([0.01981685], dtype=float32),\n",
       " array([0.01990967], dtype=float32),\n",
       " array([0.02000167], dtype=float32),\n",
       " array([0.02009287], dtype=float32),\n",
       " array([0.02018328], dtype=float32),\n",
       " array([0.02027292], dtype=float32),\n",
       " array([0.0203618], dtype=float32),\n",
       " array([0.02044994], dtype=float32),\n",
       " array([0.02053735], dtype=float32),\n",
       " array([0.02062402], dtype=float32),\n",
       " array([0.02070984], dtype=float32),\n",
       " array([0.02079478], dtype=float32),\n",
       " array([0.02087884], dtype=float32),\n",
       " array([0.02096218], dtype=float32),\n",
       " array([0.02104481], dtype=float32),\n",
       " array([0.02112675], dtype=float32),\n",
       " array([0.02120801], dtype=float32),\n",
       " array([0.02128861], dtype=float32),\n",
       " array([0.02136855], dtype=float32),\n",
       " array([0.02144786], dtype=float32),\n",
       " array([0.02152655], dtype=float32),\n",
       " array([0.02160463], dtype=float32),\n",
       " array([0.02168212], dtype=float32),\n",
       " array([0.02175904], dtype=float32),\n",
       " array([0.02183539], dtype=float32),\n",
       " array([0.0219112], dtype=float32)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
