{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4ac27d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.init as init\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8819113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_torch(arr):\n",
    "    \n",
    "    arr = torch.FloatTensor(arr)\n",
    "    arr = arr.unsqueeze(-1)\n",
    "    arr = arr.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def x_train_data(N_x, N_t, x_l, x_r):\n",
    "    \n",
    "    x_train = np.linspace(x_l, x_r, N_x)\n",
    "    x_train = np.tile(x_train, N_t)\n",
    "#     N_xl = mse( torch.where(x_train == x_l,1,0), null ).detach().numpy().item()\n",
    "    x_train = np_to_torch(x_train)\n",
    "    \n",
    "    return x_train\n",
    "\n",
    "def t_train_data(N_x, N_t, t_i, t_f):\n",
    "    \n",
    "    t_train = np.linspace(t_i, t_f, N_t)\n",
    "    t_train = np.repeat(t_train, N_x)\n",
    "    t_train = np_to_torch(t_train)\n",
    "\n",
    "    return t_train\n",
    "            \n",
    "def xavier_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0.3)\n",
    "    \n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, layer_size):\n",
    "        super(ANN, self).__init__()\n",
    "        \n",
    "        # Fully conected model-1\n",
    "        modules_1 = []\n",
    "        for i in range(len(layer_size) - 1):\n",
    "            modules_1.append(nn.Linear(layer_size[i], layer_size[i+1]))  \n",
    "            if i < ( len(layer_size) - 2 ):\n",
    "                modules_1.append(nn.Tanh())\n",
    "#             else:\n",
    "#                 modules_1.append(nn.ReLU())\n",
    "\n",
    "        modules_1.append(nn.Softplus(beta = 2.4))\n",
    "        self.fc_1 = nn.Sequential(*modules_1)\n",
    "        for layer in self.fc_1.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                 layer.weight.data.normal_(mean=0.45, std=0.3)\n",
    "#         self.fc_1.apply(xavier_init)\n",
    "        \n",
    "        # Fully conected model-2\n",
    "        modules_2 = []\n",
    "        for i in range(len(layer_size) - 1):\n",
    "            modules_2.append(nn.Linear(layer_size[i], layer_size[i+1]))  \n",
    "            if i != len(layer_size) - 2 :\n",
    "                modules_2.append(nn.Tanh())\n",
    "#             else:\n",
    "#                 modules_2.append(nn.ReLU())\n",
    "                \n",
    "        self.fc_2 = nn.Sequential(*modules_2)\n",
    "#         self.fc_2.apply(xavier_init)\n",
    "        for layer in self.fc_1.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                 layer.weight.data.normal_(mean=0, std=0.3)\n",
    "        \n",
    "    def forward(self, x_train, t_train):\n",
    "        \n",
    "        T = self.fc_1( torch.cat((x_train, t_train),1) )\n",
    "        dTdx = torch.autograd.grad(T, x_train, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "        d2Tdx2 = torch.autograd.grad(dTdx, x_train, grad_outputs=torch.ones_like(dTdx), create_graph=True)[0]\n",
    "        dTdt = torch.autograd.grad(T, t_train, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "        \n",
    "        f = self.fc_2( torch.cat((x_train, t_train),1) )\n",
    "        dfdt = torch.autograd.grad(f, t_train, grad_outputs=torch.ones_like(f), create_graph=True)[0]\n",
    "        \n",
    "        return T, dTdx, d2Tdx2, dTdt, f, dfdt\n",
    "    \n",
    "def get_loss(x_train, t_train, k1, k2, T_l, T_r, x_l, x_r, t_i, mat_3, mat_4, mat_5, N_3, N_4, N_5, w1, w2, w3, w4, w5, w6):\n",
    "    \n",
    "    mse = nn.MSELoss(reduction='sum')\n",
    "        \n",
    "    T, dTdx, d2Tdx2, dTdt, f, dfdt  = model(x_train, t_train)\n",
    "    \n",
    "    m = nn.Sigmoid()\n",
    "    \n",
    "    f_lim = 1\n",
    "    mat_1 = torch.where(f >= f_lim,1,0)\n",
    "    N_1 = torch.sum( mat_1 )\n",
    "    mat_2 = torch.where(f < f_lim,1,0)\n",
    "    N_2 = torch.sum( mat_2 )\n",
    "    \n",
    "    eq1 = w1*( torch.sum( torch.square( torch.mul(mat_1, dTdt - k1*d2Tdx2 ) ) ) \n",
    "               )/(N_1) \n",
    "    eq2 = w2*torch.sum( torch.square( torch.mul(mat_2, dfdt - k2*d2Tdx2 ) ) )/(N_2)\n",
    "    bc1 = w3*torch.sum( torch.square( torch.mul(torch.where(x_train == x_l,1,0), T - T_l ) ) )/(20) \n",
    "    bc2 = w4*torch.sum( torch.square( torch.mul(mat_3, f - (f_lim + 0.1)  ) ) )/(N_3)\n",
    "    bc3 = w5*torch.sum( torch.square( torch.mul(mat_2, T - T_r  ) ) )/(N_2) \n",
    "    ic1 = w6*torch.sum( torch.square( torch.mul(mat_4, f - 0  ) ) )/(N_4)\n",
    "    ic2 = w6*torch.sum( torch.square( torch.mul(mat_5, T  ) ) )/(N_5)\n",
    "# + torch.sum( torch.square( torch.mul(mat_1, m(-30*dTdt) ) ) )\n",
    "    if (N_1 == 0):\n",
    "        eq1 = 0\n",
    "    if (N_2 == 0):\n",
    "        eq2 = 0\n",
    "        bc3 = 0\n",
    "    if (N_3 == 0):\n",
    "        bc1 = 0\n",
    "        bc2 = 0\n",
    "    if (N_4 == 0):\n",
    "        ic1 = 0\n",
    "\n",
    "    loss = eq1 + eq2 + bc1 + bc2 + bc3 + ic1 + ic2 \n",
    "    \n",
    "    return loss, eq1, eq2, bc1, bc2, bc3, ic1, ic2   \n",
    "\n",
    "def print_loss(epoch, loss, eq1, eq2, bc1, bc2, bc3, ic1, ic2):\n",
    "    print('epoch = ',epoch)\n",
    "    print('loss = ',loss.detach().numpy())\n",
    "    print('eq1_loss = ',eq1)\n",
    "    print('eq2_loss = ',eq2.detach().numpy())\n",
    "    print('bc1_loss = ',bc1.detach().numpy())\n",
    "    print('bc2_loss = ',bc2.detach().numpy())\n",
    "    print('bc3_loss = ',bc3.detach().numpy())\n",
    "    print('ic1_loss = ',ic1.detach().numpy())\n",
    "    print('ic2_loss = ',ic2.detach().numpy())\n",
    "    \n",
    "def lambda_analytical(k1, k2):\n",
    "    x = []\n",
    "    er = []\n",
    "    cnt = 0\n",
    "    for i in np.arange(0.1, 5, 0.001):\n",
    "        x.append(i)\n",
    "        er.append(math.erf(x[-1]))\n",
    "        cnt = cnt+1\n",
    "\n",
    "    x = np.array(x)\n",
    "    er = np.array(er)\n",
    "    y =[]\n",
    "    y = np.exp(-x*x)/(er*math.sqrt(math.pi))-x*k1/(k2*2)\n",
    "\n",
    "    for i in range(1,cnt):\n",
    "        if(y[i]*y[i-1]<0):\n",
    "            lam = x[i]\n",
    "            break\n",
    "    \n",
    "    return lam\n",
    "\n",
    "def analytical(N_x, N_t, x_test, t_test, T_r, k1, k2, T_l):\n",
    "\n",
    "    x_test = x_test.detach().numpy()\n",
    "    t_test = t_test.detach().numpy()\n",
    "    y_an = np.zeros((N_x*N_t, 1))\n",
    "    lam = lambda_analytical(k1, k2)\n",
    "    s = np.sqrt(k1*t_test)*2*lam\n",
    "    \n",
    "    for j in range(N_x*N_t):\n",
    "        if(x_test[j]<s[j]):\n",
    "            y_an[j] = T_l - T_l*math.erf( x_test[j]/( 2*np.sqrt(k1*t_test[j]) ) )/ math.erf(lam) \n",
    "        else:\n",
    "            y_an[j] = T_r\n",
    "            \n",
    "    y_an = np.reshape(y_an, (N_x*N_t, 1))\n",
    "    \n",
    "    return y_an, s\n",
    "    \n",
    "def train_model(model, optimiser1, epochs, T_r, T_l, k1, k2, N_x, x_l, x_r, N_t, t_i, t_f, accuracy_cap):\n",
    "    \n",
    "    loss_store = []\n",
    "    T_store_an = []\n",
    "    mse = nn.MSELoss(reduction='sum')\n",
    "    model.train()  \n",
    "    \n",
    "    # Initial conditions\n",
    "    x_train = x_train_data(N_x, N_t, x_l, x_r)\n",
    "    t_train = t_train_data(N_x, N_t, t_i, t_f)\n",
    "    mat_3 = torch.mul( torch.where(t_train == t_i,1,0), torch.where(x_train <= 0.01,1,0) )\n",
    "    N_3 = torch.sum( mat_3 )\n",
    "    print(\"N_3 = \", N_3)\n",
    "    mat_4 = torch.mul( torch.where(t_train == t_i,1,0), torch.where(x_train > 0.01,1,0) )\n",
    "    N_4 = torch.sum(mat_4)\n",
    "    print(\"N_4 = \", N_4)\n",
    "    mat_5 = torch.mul( torch.where(t_train == t_i,1,0), torch.where(x_train != 0,1,0) )\n",
    "    N_5 = torch.sum(mat_5)\n",
    "    print(\"N_5 = \", N_5)\n",
    "    # Loss function weights\n",
    "    w1 = 1\n",
    "    w2 = 1\n",
    "    w3 = 1\n",
    "    w4 = 1\n",
    "    w5 = 1\n",
    "    w6 = 1\n",
    "    \n",
    "    for epoch in range(epochs):        \n",
    "    #Backpropogation and optimisation\n",
    "        loss, eq1, eq2, bc1, bc2, bc3, ic1, ic2 = get_loss(x_train, t_train, k1, k2, T_l, T_r, x_l, x_r, \n",
    "                                                      t_i, mat_3, mat_4, mat_5, N_3, N_4, N_5, w1, w2, w3, w4, w5, w6)\n",
    "        optimiser1.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser1.step()  \n",
    "        loss_store.append(loss.detach().numpy())\n",
    "        \n",
    "        if epoch%2000==0:\n",
    "            print_loss(epoch, loss, eq1, eq2, bc1, bc2, bc3, ic1, ic2)\n",
    "            print(\"\")\n",
    "                    \n",
    "    # Store the results after each time step\n",
    "    T_pred, dTdx, d2Tdx2, dTdt, f, dfdt = model(x_train, t_train)\n",
    "    T_an, _ = analytical(N_x, N_t, x_train, t_train, T_r, k1, k2, T_l)\n",
    "\n",
    "    return loss_store, T_pred.detach().numpy(), T_an, x_train.detach().numpy(), f.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa5eeff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN(\n",
      "  (fc_1): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=6, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=6, out_features=1, bias=True)\n",
      "    (5): Softplus(beta=2.4, threshold=20)\n",
      "  )\n",
      "  (fc_2): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=6, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=6, out_features=6, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=6, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Total trainable parameters in the model: 134\n",
      "N_3 =  tensor(5)\n",
      "N_4 =  tensor(95)\n",
      "N_5 =  tensor(99)\n",
      "epoch =  0\n",
      "loss =  2.4336739\n",
      "eq1_loss =  0\n",
      "eq2_loss =  0.009418008\n",
      "bc1_loss =  1.3229355\n",
      "bc2_loss =  1.0248349\n",
      "bc3_loss =  0.034159273\n",
      "ic1_loss =  0.00792927\n",
      "ic2_loss =  0.034397107\n",
      "\n",
      "epoch =  2000\n",
      "loss =  1.5189974\n",
      "eq1_loss =  0\n",
      "eq2_loss =  0.0005248081\n",
      "bc1_loss =  0.4512147\n",
      "bc2_loss =  0.2987672\n",
      "bc3_loss =  0.2418501\n",
      "ic1_loss =  0.29167902\n",
      "ic2_loss =  0.23496154\n",
      "\n",
      "epoch =  4000\n",
      "loss =  1.3424693\n",
      "eq1_loss =  0\n",
      "eq2_loss =  0.024785096\n",
      "bc1_loss =  0.35536772\n",
      "bc2_loss =  0.2622116\n",
      "bc3_loss =  0.23645833\n",
      "ic1_loss =  0.26008037\n",
      "ic2_loss =  0.20356633\n",
      "\n",
      "epoch =  6000\n",
      "loss =  1.01923\n",
      "eq1_loss =  0\n",
      "eq2_loss =  0.018789548\n",
      "bc1_loss =  0.3179432\n",
      "bc2_loss =  0.13785207\n",
      "bc3_loss =  0.21365185\n",
      "ic1_loss =  0.17934783\n",
      "ic2_loss =  0.15164551\n",
      "\n",
      "epoch =  8000\n",
      "loss =  1.3936605\n",
      "eq1_loss =  tensor(0.3152, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  0.004843399\n",
      "bc1_loss =  0.62264335\n",
      "bc2_loss =  0.05645616\n",
      "bc3_loss =  0.12111134\n",
      "ic1_loss =  0.16995332\n",
      "ic2_loss =  0.103490174\n",
      "\n",
      "epoch =  10000\n",
      "loss =  1.0146334\n",
      "eq1_loss =  tensor(0.0272, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  0.00019561546\n",
      "bc1_loss =  0.3676214\n",
      "bc2_loss =  0.041037906\n",
      "bc3_loss =  0.20639539\n",
      "ic1_loss =  0.17539482\n",
      "ic2_loss =  0.19683701\n",
      "\n",
      "epoch =  12000\n",
      "loss =  0.8147642\n",
      "eq1_loss =  tensor(0.0011, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  0.010484933\n",
      "bc1_loss =  0.23744449\n",
      "bc2_loss =  0.0316589\n",
      "bc3_loss =  0.17044078\n",
      "ic1_loss =  0.18442224\n",
      "ic2_loss =  0.17925881\n",
      "\n",
      "epoch =  14000\n",
      "loss =  0.709558\n",
      "eq1_loss =  tensor(0.0037, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  0.015864799\n",
      "bc1_loss =  0.16193196\n",
      "bc2_loss =  0.030426431\n",
      "bc3_loss =  0.14825052\n",
      "ic1_loss =  0.17739041\n",
      "ic2_loss =  0.17203964\n",
      "\n",
      "epoch =  16000\n",
      "loss =  0.56775445\n",
      "eq1_loss =  tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  0.0062466203\n",
      "bc1_loss =  0.09404063\n",
      "bc2_loss =  0.028255086\n",
      "bc3_loss =  0.11738708\n",
      "ic1_loss =  0.1535638\n",
      "ic2_loss =  0.16006558\n",
      "\n",
      "epoch =  18000\n",
      "loss =  0.47315806\n",
      "eq1_loss =  tensor(0.0126, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  0.012568014\n",
      "bc1_loss =  0.06957676\n",
      "bc2_loss =  0.02195538\n",
      "bc3_loss =  0.09415597\n",
      "ic1_loss =  0.11942872\n",
      "ic2_loss =  0.142855\n",
      "\n",
      "epoch =  20000\n",
      "loss =  0.3891338\n",
      "eq1_loss =  tensor(0.0074, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  0.007675903\n",
      "bc1_loss =  0.059825193\n",
      "bc2_loss =  0.015281275\n",
      "bc3_loss =  0.08268533\n",
      "ic1_loss =  0.08770368\n",
      "ic2_loss =  0.12860028\n",
      "\n",
      "epoch =  22000\n",
      "loss =  0.32015437\n",
      "eq1_loss =  tensor(0.0054, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  0.0052914717\n",
      "bc1_loss =  0.055471648\n",
      "bc2_loss =  0.011932623\n",
      "bc3_loss =  0.06767059\n",
      "ic1_loss =  0.06518732\n",
      "ic2_loss =  0.10920742\n",
      "\n",
      "epoch =  24000\n",
      "loss =  0.2855602\n",
      "eq1_loss =  tensor(0.0051, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  0.00681302\n",
      "bc1_loss =  0.058464825\n",
      "bc2_loss =  0.011952353\n",
      "bc3_loss =  0.0546352\n",
      "ic1_loss =  0.056000795\n",
      "ic2_loss =  0.09262417\n",
      "\n",
      "epoch =  26000\n",
      "loss =  0.27026278\n",
      "eq1_loss =  tensor(0.0048, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  0.008525543\n",
      "bc1_loss =  0.064409256\n",
      "bc2_loss =  0.012631668\n",
      "bc3_loss =  0.045709446\n",
      "ic1_loss =  0.052247554\n",
      "ic2_loss =  0.08194251\n",
      "\n",
      "epoch =  28000\n",
      "loss =  0.26255593\n",
      "eq1_loss =  tensor(0.0044, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  0.008762983\n",
      "bc1_loss =  0.068288185\n",
      "bc2_loss =  0.013023193\n",
      "bc3_loss =  0.041055188\n",
      "ic1_loss =  0.050530568\n",
      "ic2_loss =  0.07645368\n",
      "\n",
      "epoch =  30000\n",
      "loss =  0.2539077\n",
      "eq1_loss =  tensor(0.0042, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  0.007877811\n",
      "bc1_loss =  0.06856631\n",
      "bc2_loss =  0.013507863\n",
      "bc3_loss =  0.038149193\n",
      "ic1_loss =  0.047963027\n",
      "ic2_loss =  0.073677175\n",
      "\n",
      "epoch =  32000\n",
      "loss =  0.24345113\n",
      "eq1_loss =  tensor(0.0038, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  0.006500562\n",
      "bc1_loss =  0.06684212\n",
      "bc2_loss =  0.014640817\n",
      "bc3_loss =  0.036715582\n",
      "ic1_loss =  0.042785026\n",
      "ic2_loss =  0.072138384\n",
      "\n",
      "epoch =  34000\n",
      "loss =  0.23312244\n",
      "eq1_loss =  tensor(0.0034, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  0.0049247886\n",
      "bc1_loss =  0.064494625\n",
      "bc2_loss =  0.016676433\n",
      "bc3_loss =  0.036208775\n",
      "ic1_loss =  0.036053963\n",
      "ic2_loss =  0.07137662\n",
      "\n",
      "epoch =  36000\n",
      "loss =  0.22408567\n",
      "eq1_loss =  tensor(0.0034, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  0.003250117\n",
      "bc1_loss =  0.06251757\n",
      "bc2_loss =  0.019066587\n",
      "bc3_loss =  0.034973152\n",
      "ic1_loss =  0.030783085\n",
      "ic2_loss =  0.0701309\n",
      "\n",
      "epoch =  38000\n",
      "loss =  0.2182563\n",
      "eq1_loss =  tensor(0.0039, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  0.0029644207\n",
      "bc1_loss =  0.061428595\n",
      "bc2_loss =  0.020567209\n",
      "bc3_loss =  0.033055954\n",
      "ic1_loss =  0.028146636\n",
      "ic2_loss =  0.06814675\n",
      "\n",
      "epoch =  40000\n",
      "loss =  0.21420068\n",
      "eq1_loss =  tensor(0.0047, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  0.0032077546\n",
      "bc1_loss =  0.060365956\n",
      "bc2_loss =  0.0209827\n",
      "bc3_loss =  0.031233387\n",
      "ic1_loss =  0.02721729\n",
      "ic2_loss =  0.06649216\n",
      "\n",
      "epoch =  42000\n",
      "loss =  0.21115285\n",
      "eq1_loss =  tensor(0.0054, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  0.0034651428\n",
      "bc1_loss =  0.059387743\n",
      "bc2_loss =  0.021116922\n",
      "bc3_loss =  0.029955963\n",
      "ic1_loss =  0.026744502\n",
      "ic2_loss =  0.065122314\n",
      "\n",
      "epoch =  44000\n",
      "loss =  0.20844245\n",
      "eq1_loss =  tensor(0.0057, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  0.0037650035\n",
      "bc1_loss =  0.058324628\n",
      "bc2_loss =  0.021164741\n",
      "bc3_loss =  0.02894613\n",
      "ic1_loss =  0.026428042\n",
      "ic2_loss =  0.064090036\n",
      "\n",
      "epoch =  46000\n",
      "loss =  0.20570439\n",
      "eq1_loss =  tensor(0.0058, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  0.0041144895\n",
      "bc1_loss =  0.05705306\n",
      "bc2_loss =  0.021153862\n",
      "bc3_loss =  0.02799376\n",
      "ic1_loss =  0.026189571\n",
      "ic2_loss =  0.063433595\n",
      "\n",
      "epoch =  48000\n",
      "loss =  0.20328051\n",
      "eq1_loss =  tensor(0.0054, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  0.0045696334\n",
      "bc1_loss =  0.05573721\n",
      "bc2_loss =  0.02111598\n",
      "bc3_loss =  0.027535582\n",
      "ic1_loss =  0.02599948\n",
      "ic2_loss =  0.06296329\n",
      "\n",
      "epoch =  50000\n",
      "loss =  0.20030221\n",
      "eq1_loss =  tensor(0.0048, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  0.005027921\n",
      "bc1_loss =  0.05407722\n",
      "bc2_loss =  0.020825975\n",
      "bc3_loss =  0.02677865\n",
      "ic1_loss =  0.025976969\n",
      "ic2_loss =  0.06279366\n",
      "\n",
      "time elapsed =  537.8203163146973\n"
     ]
    }
   ],
   "source": [
    "N_x = 100\n",
    "N_t = 40\n",
    "\n",
    "x_l = 0\n",
    "x_r = 0.2\n",
    "\n",
    "T_r = 0\n",
    "T_l = 1\n",
    "\n",
    "t_i = 0\n",
    "t_f = 0.1\n",
    "accuracy_cap = 0.0004\n",
    "\n",
    "# Neural network params\n",
    "layer_size = [2, 6, 6, 1]\n",
    "\n",
    "# material params\n",
    "k1 = 0.05\n",
    "k2 = 0.5\n",
    "\n",
    "# Training data and initial data\n",
    "model = ANN(layer_size)\n",
    "print(model)\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total trainable parameters in the model:\", total_trainable_params)\n",
    "\n",
    "# # Setup Loss function and Optimiser\n",
    "lr = 8e-5\n",
    "epochs = 50001\n",
    "optimiser1 = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training model\n",
    "start = time.time()\n",
    "loss_store, T_store_pred, T_store_an, x_test_np, f = train_model(model, optimiser1, epochs, T_r, T_l, k1, k2, N_x, x_l, x_r, N_t, t_i, t_f, accuracy_cap)\n",
    "end = time.time()\n",
    "time_elapsed = end - start\n",
    "print(\"time elapsed = \", time_elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb512346",
   "metadata": {},
   "source": [
    "# Results Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6502e17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 0\n",
    "plt.plot(x_test_np[i*N_x:(i+1)*N_x-1], T_store_pred[i*N_x:(i+1)*N_x-1])\n",
    "i = 5\n",
    "# plt.plot(x_test_np[i*N_x:(i+1)*N_x-1], T_store_pred[i*N_x:(i+1)*N_x-1])\n",
    "# plt.plot(x_test_np[i*N_x:(i+1)*N_x-1], T_store_an[i*N_x:(i+1)*N_x-1])\n",
    "t = t_i + (t_f - t_i)/(N_t-1)*(i)\n",
    "Title = \"Time = \" + str( \"{:.3f}\".format (t))\n",
    "plt.title(Title)\n",
    "# plt.xlim([0, 0.02])\n",
    "# plt.legend([\"PINN\", \"Analytical\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b52a0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_data(N_x, N_t, x_l, x_r)\n",
    "t_train = t_train_data(N_x, N_t, t_i, t_f)\n",
    "T, dTdx, d2Tdx2, dTdt, f, dfdt  = model(x_train, t_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd02cd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([110.4355], grad_fn=<SelectBackward0>)\n",
      "tensor([109.9076], grad_fn=<SelectBackward0>)\n",
      "tensor([109.3230], grad_fn=<SelectBackward0>)\n",
      "tensor([108.6806], grad_fn=<SelectBackward0>)\n",
      "tensor([107.9794], grad_fn=<SelectBackward0>)\n",
      "tensor([107.2187], grad_fn=<SelectBackward0>)\n",
      "tensor([106.3981], grad_fn=<SelectBackward0>)\n",
      "tensor([105.5174], grad_fn=<SelectBackward0>)\n",
      "tensor([104.5766], grad_fn=<SelectBackward0>)\n",
      "tensor([103.5761], grad_fn=<SelectBackward0>)\n",
      "tensor([102.5166], grad_fn=<SelectBackward0>)\n",
      "tensor([101.3988], grad_fn=<SelectBackward0>)\n",
      "tensor([100.2241], grad_fn=<SelectBackward0>)\n",
      "tensor([98.9940], grad_fn=<SelectBackward0>)\n",
      "tensor([97.7101], grad_fn=<SelectBackward0>)\n",
      "tensor([96.3747], grad_fn=<SelectBackward0>)\n",
      "tensor([94.9899], grad_fn=<SelectBackward0>)\n",
      "tensor([93.5583], grad_fn=<SelectBackward0>)\n",
      "tensor([92.0825], grad_fn=<SelectBackward0>)\n",
      "tensor([90.5657], grad_fn=<SelectBackward0>)\n",
      "tensor([89.0107], grad_fn=<SelectBackward0>)\n",
      "tensor([87.4210], grad_fn=<SelectBackward0>)\n",
      "tensor([85.7998], grad_fn=<SelectBackward0>)\n",
      "tensor([84.1506], grad_fn=<SelectBackward0>)\n",
      "tensor([82.4769], grad_fn=<SelectBackward0>)\n",
      "tensor([80.7823], grad_fn=<SelectBackward0>)\n",
      "tensor([79.0704], grad_fn=<SelectBackward0>)\n",
      "tensor([77.3447], grad_fn=<SelectBackward0>)\n",
      "tensor([75.6088], grad_fn=<SelectBackward0>)\n",
      "tensor([73.8661], grad_fn=<SelectBackward0>)\n",
      "tensor([72.1201], grad_fn=<SelectBackward0>)\n",
      "tensor([70.3741], grad_fn=<SelectBackward0>)\n",
      "tensor([68.6313], grad_fn=<SelectBackward0>)\n",
      "tensor([66.8947], grad_fn=<SelectBackward0>)\n",
      "tensor([65.1674], grad_fn=<SelectBackward0>)\n",
      "tensor([63.4522], grad_fn=<SelectBackward0>)\n",
      "tensor([61.7516], grad_fn=<SelectBackward0>)\n",
      "tensor([60.0682], grad_fn=<SelectBackward0>)\n",
      "tensor([58.4043], grad_fn=<SelectBackward0>)\n",
      "tensor([56.7621], grad_fn=<SelectBackward0>)\n",
      "tensor([55.1434], grad_fn=<SelectBackward0>)\n",
      "tensor([53.5502], grad_fn=<SelectBackward0>)\n",
      "tensor([51.9839], grad_fn=<SelectBackward0>)\n",
      "tensor([50.4461], grad_fn=<SelectBackward0>)\n",
      "tensor([48.9381], grad_fn=<SelectBackward0>)\n",
      "tensor([47.4609], grad_fn=<SelectBackward0>)\n",
      "tensor([46.0157], grad_fn=<SelectBackward0>)\n",
      "tensor([44.6030], grad_fn=<SelectBackward0>)\n",
      "tensor([43.2235], grad_fn=<SelectBackward0>)\n",
      "tensor([41.8779], grad_fn=<SelectBackward0>)\n",
      "tensor([40.5665], grad_fn=<SelectBackward0>)\n",
      "tensor([39.2894], grad_fn=<SelectBackward0>)\n",
      "tensor([38.0470], grad_fn=<SelectBackward0>)\n",
      "tensor([36.8390], grad_fn=<SelectBackward0>)\n",
      "tensor([35.6656], grad_fn=<SelectBackward0>)\n",
      "tensor([34.5264], grad_fn=<SelectBackward0>)\n",
      "tensor([33.4212], grad_fn=<SelectBackward0>)\n",
      "tensor([32.3496], grad_fn=<SelectBackward0>)\n",
      "tensor([31.3111], grad_fn=<SelectBackward0>)\n",
      "tensor([30.3053], grad_fn=<SelectBackward0>)\n",
      "tensor([29.3316], grad_fn=<SelectBackward0>)\n",
      "tensor([28.3892], grad_fn=<SelectBackward0>)\n",
      "tensor([27.4775], grad_fn=<SelectBackward0>)\n",
      "tensor([26.5957], grad_fn=<SelectBackward0>)\n",
      "tensor([25.7431], grad_fn=<SelectBackward0>)\n",
      "tensor([24.9188], grad_fn=<SelectBackward0>)\n",
      "tensor([24.1219], grad_fn=<SelectBackward0>)\n",
      "tensor([23.3515], grad_fn=<SelectBackward0>)\n",
      "tensor([22.6067], grad_fn=<SelectBackward0>)\n",
      "tensor([21.8866], grad_fn=<SelectBackward0>)\n",
      "tensor([21.1902], grad_fn=<SelectBackward0>)\n",
      "tensor([20.5165], grad_fn=<SelectBackward0>)\n",
      "tensor([19.8646], grad_fn=<SelectBackward0>)\n",
      "tensor([19.2335], grad_fn=<SelectBackward0>)\n",
      "tensor([18.6222], grad_fn=<SelectBackward0>)\n",
      "tensor([18.0297], grad_fn=<SelectBackward0>)\n",
      "tensor([17.4552], grad_fn=<SelectBackward0>)\n",
      "tensor([16.8975], grad_fn=<SelectBackward0>)\n",
      "tensor([16.3560], grad_fn=<SelectBackward0>)\n",
      "tensor([15.8295], grad_fn=<SelectBackward0>)\n",
      "tensor([15.3174], grad_fn=<SelectBackward0>)\n",
      "tensor([14.8186], grad_fn=<SelectBackward0>)\n",
      "tensor([14.3325], grad_fn=<SelectBackward0>)\n",
      "tensor([13.8582], grad_fn=<SelectBackward0>)\n",
      "tensor([13.3950], grad_fn=<SelectBackward0>)\n",
      "tensor([12.9423], grad_fn=<SelectBackward0>)\n",
      "tensor([12.4994], grad_fn=<SelectBackward0>)\n",
      "tensor([12.0657], grad_fn=<SelectBackward0>)\n",
      "tensor([11.6408], grad_fn=<SelectBackward0>)\n",
      "tensor([11.2241], grad_fn=<SelectBackward0>)\n",
      "tensor([10.8152], grad_fn=<SelectBackward0>)\n",
      "tensor([10.4138], grad_fn=<SelectBackward0>)\n",
      "tensor([10.0196], grad_fn=<SelectBackward0>)\n",
      "tensor([9.6323], grad_fn=<SelectBackward0>)\n",
      "tensor([9.2519], grad_fn=<SelectBackward0>)\n",
      "tensor([8.8780], grad_fn=<SelectBackward0>)\n",
      "tensor([8.5108], grad_fn=<SelectBackward0>)\n",
      "tensor([8.1501], grad_fn=<SelectBackward0>)\n",
      "tensor([7.7961], grad_fn=<SelectBackward0>)\n",
      "tensor([7.4487], grad_fn=<SelectBackward0>)\n",
      "tensor([109.4365], grad_fn=<SelectBackward0>)\n",
      "tensor([108.9264], grad_fn=<SelectBackward0>)\n",
      "tensor([108.3632], grad_fn=<SelectBackward0>)\n",
      "tensor([107.7456], grad_fn=<SelectBackward0>)\n",
      "tensor([107.0727], grad_fn=<SelectBackward0>)\n",
      "tensor([106.3436], grad_fn=<SelectBackward0>)\n",
      "tensor([105.5579], grad_fn=<SelectBackward0>)\n",
      "tensor([104.7152], grad_fn=<SelectBackward0>)\n",
      "tensor([103.8153], grad_fn=<SelectBackward0>)\n",
      "tensor([102.8584], grad_fn=<SelectBackward0>)\n",
      "tensor([101.8451], grad_fn=<SelectBackward0>)\n",
      "tensor([100.7758], grad_fn=<SelectBackward0>)\n",
      "tensor([99.6517], grad_fn=<SelectBackward0>)\n",
      "tensor([98.4739], grad_fn=<SelectBackward0>)\n",
      "tensor([97.2440], grad_fn=<SelectBackward0>)\n",
      "tensor([95.9637], grad_fn=<SelectBackward0>)\n",
      "tensor([94.6350], grad_fn=<SelectBackward0>)\n",
      "tensor([93.2602], grad_fn=<SelectBackward0>)\n",
      "tensor([91.8417], grad_fn=<SelectBackward0>)\n",
      "tensor([90.3821], grad_fn=<SelectBackward0>)\n",
      "tensor([88.8844], grad_fn=<SelectBackward0>)\n",
      "tensor([87.3513], grad_fn=<SelectBackward0>)\n",
      "tensor([85.7862], grad_fn=<SelectBackward0>)\n",
      "tensor([84.1921], grad_fn=<SelectBackward0>)\n",
      "tensor([82.5724], grad_fn=<SelectBackward0>)\n",
      "tensor([80.9304], grad_fn=<SelectBackward0>)\n",
      "tensor([79.2696], grad_fn=<SelectBackward0>)\n",
      "tensor([77.5932], grad_fn=<SelectBackward0>)\n",
      "tensor([75.9047], grad_fn=<SelectBackward0>)\n",
      "tensor([74.2074], grad_fn=<SelectBackward0>)\n",
      "tensor([72.5047], grad_fn=<SelectBackward0>)\n",
      "tensor([70.7998], grad_fn=<SelectBackward0>)\n",
      "tensor([69.0957], grad_fn=<SelectBackward0>)\n",
      "tensor([67.3956], grad_fn=<SelectBackward0>)\n",
      "tensor([65.7024], grad_fn=<SelectBackward0>)\n",
      "tensor([64.0187], grad_fn=<SelectBackward0>)\n",
      "tensor([62.3474], grad_fn=<SelectBackward0>)\n",
      "tensor([60.6908], grad_fn=<SelectBackward0>)\n",
      "tensor([59.0514], grad_fn=<SelectBackward0>)\n",
      "tensor([57.4313], grad_fn=<SelectBackward0>)\n",
      "tensor([55.8325], grad_fn=<SelectBackward0>)\n",
      "tensor([54.2568], grad_fn=<SelectBackward0>)\n",
      "tensor([52.7061], grad_fn=<SelectBackward0>)\n",
      "tensor([51.1817], grad_fn=<SelectBackward0>)\n",
      "tensor([49.6852], grad_fn=<SelectBackward0>)\n",
      "tensor([48.2176], grad_fn=<SelectBackward0>)\n",
      "tensor([46.7800], grad_fn=<SelectBackward0>)\n",
      "tensor([45.3734], grad_fn=<SelectBackward0>)\n",
      "tensor([43.9984], grad_fn=<SelectBackward0>)\n",
      "tensor([42.6557], grad_fn=<SelectBackward0>)\n",
      "tensor([41.3458], grad_fn=<SelectBackward0>)\n",
      "tensor([40.0690], grad_fn=<SelectBackward0>)\n",
      "tensor([38.8256], grad_fn=<SelectBackward0>)\n",
      "tensor([37.6155], grad_fn=<SelectBackward0>)\n",
      "tensor([36.4390], grad_fn=<SelectBackward0>)\n",
      "tensor([35.2958], grad_fn=<SelectBackward0>)\n",
      "tensor([34.1857], grad_fn=<SelectBackward0>)\n",
      "tensor([33.1085], grad_fn=<SelectBackward0>)\n",
      "tensor([32.0638], grad_fn=<SelectBackward0>)\n",
      "tensor([31.0512], grad_fn=<SelectBackward0>)\n",
      "tensor([30.0700], grad_fn=<SelectBackward0>)\n",
      "tensor([29.1199], grad_fn=<SelectBackward0>)\n",
      "tensor([28.2000], grad_fn=<SelectBackward0>)\n",
      "tensor([27.3098], grad_fn=<SelectBackward0>)\n",
      "tensor([26.4484], grad_fn=<SelectBackward0>)\n",
      "tensor([25.6151], grad_fn=<SelectBackward0>)\n",
      "tensor([24.8091], grad_fn=<SelectBackward0>)\n",
      "tensor([24.0295], grad_fn=<SelectBackward0>)\n",
      "tensor([23.2754], grad_fn=<SelectBackward0>)\n",
      "tensor([22.5460], grad_fn=<SelectBackward0>)\n",
      "tensor([21.8403], grad_fn=<SelectBackward0>)\n",
      "tensor([21.1572], grad_fn=<SelectBackward0>)\n",
      "tensor([20.4960], grad_fn=<SelectBackward0>)\n",
      "tensor([19.8557], grad_fn=<SelectBackward0>)\n",
      "tensor([19.2352], grad_fn=<SelectBackward0>)\n",
      "tensor([18.6336], grad_fn=<SelectBackward0>)\n",
      "tensor([18.0501], grad_fn=<SelectBackward0>)\n",
      "tensor([17.4835], grad_fn=<SelectBackward0>)\n",
      "tensor([16.9331], grad_fn=<SelectBackward0>)\n",
      "tensor([16.3979], grad_fn=<SelectBackward0>)\n",
      "tensor([15.8771], grad_fn=<SelectBackward0>)\n",
      "tensor([15.3697], grad_fn=<SelectBackward0>)\n",
      "tensor([14.8750], grad_fn=<SelectBackward0>)\n",
      "tensor([14.3921], grad_fn=<SelectBackward0>)\n",
      "tensor([13.9204], grad_fn=<SelectBackward0>)\n",
      "tensor([13.4592], grad_fn=<SelectBackward0>)\n",
      "tensor([13.0078], grad_fn=<SelectBackward0>)\n",
      "tensor([12.5655], grad_fn=<SelectBackward0>)\n",
      "tensor([12.1320], grad_fn=<SelectBackward0>)\n",
      "tensor([11.7066], grad_fn=<SelectBackward0>)\n",
      "tensor([11.2889], grad_fn=<SelectBackward0>)\n",
      "tensor([10.8786], grad_fn=<SelectBackward0>)\n",
      "tensor([10.4754], grad_fn=<SelectBackward0>)\n",
      "tensor([10.0789], grad_fn=<SelectBackward0>)\n",
      "tensor([9.6891], grad_fn=<SelectBackward0>)\n",
      "tensor([9.3057], grad_fn=<SelectBackward0>)\n",
      "tensor([8.9288], grad_fn=<SelectBackward0>)\n",
      "tensor([8.5582], grad_fn=<SelectBackward0>)\n",
      "tensor([8.1940], grad_fn=<SelectBackward0>)\n",
      "tensor([7.8363], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(200):\n",
    "    print(d2Tdx2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7d4ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
