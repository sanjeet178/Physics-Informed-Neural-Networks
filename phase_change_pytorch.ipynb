{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06690022",
   "metadata": {},
   "source": [
    "# LBFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60a4264d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8512/689676941.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Iterations v/s Loss Storage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcycler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcycler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_api\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\matplotlib\\colorbar.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_api\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontour\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mticker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0martist\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmartist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpatches\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpatches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\matplotlib\\contour.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mClabelText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mText\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \"\"\"\n\u001b[0;32m     36\u001b[0m     \u001b[0mUnlike\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mordinary\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mget_rotation\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0man\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36m__init_subclass__\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"set\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__qualname__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"{cls.__qualname__}.set\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update_set_signature_and_docstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     _PROPERTIES_EXCLUDED_FROM_SET = [\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36m_update_set_signature_and_docstring\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    137\u001b[0m         cls.set.__signature__ = Signature(\n\u001b[0;32m    138\u001b[0m             [Parameter(\"self\", Parameter.POSITIONAL_OR_KEYWORD),\n\u001b[1;32m--> 139\u001b[1;33m              *[Parameter(prop, Parameter.KEYWORD_ONLY, default=_UNSET)\n\u001b[0m\u001b[0;32m    140\u001b[0m                \u001b[1;32mfor\u001b[0m \u001b[0mprop\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mArtistInspector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_setters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m                if prop not in Artist._PROPERTIES_EXCLUDED_FROM_SET]])\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\matplotlib\\artist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    137\u001b[0m         cls.set.__signature__ = Signature(\n\u001b[0;32m    138\u001b[0m             [Parameter(\"self\", Parameter.POSITIONAL_OR_KEYWORD),\n\u001b[1;32m--> 139\u001b[1;33m              *[Parameter(prop, Parameter.KEYWORD_ONLY, default=_UNSET)\n\u001b[0m\u001b[0;32m    140\u001b[0m                \u001b[1;32mfor\u001b[0m \u001b[0mprop\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mArtistInspector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_setters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m                if prop not in Artist._PROPERTIES_EXCLUDED_FROM_SET]])\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\inspect.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, kind, default, annotation)\u001b[0m\n\u001b[0;32m   2524\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2525\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2526\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_annotation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mannotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2528\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0m_empty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Iterations v/s Loss Storage\n",
    "iters = [0]\n",
    "loss_store = []\n",
    "\n",
    "# Boundary Conditions \n",
    "left_temp = 1\n",
    "right_temp = 0\n",
    "x_l = 0\n",
    "x_r = 0.4\n",
    "t_i = 0\n",
    "t_f = 0.1\n",
    "s_i = 0\n",
    "T_i = 0\n",
    "\n",
    "# Parameters of the equation\n",
    "c1 = 1.0\n",
    "c2 = 0.5\n",
    "\n",
    "# Setup training and test dataset\n",
    "N_div_train = 70\n",
    "N_bc = 800\n",
    "N_ic = 800\n",
    "\n",
    "x_train1 =[]\n",
    "t_train1 =[]\n",
    "for i in range(N_div_train):\n",
    "    for j in range(N_div_train):\n",
    "        x_train1.append(x_l + (i-1)*(x_r-x_l)/N_div_train)\n",
    "        t_train1.append(t_i + (j-1)*t_f/N_div_train)\n",
    "\n",
    "x_train1 = torch.FloatTensor(x_train1)\n",
    "t_train1 = torch.FloatTensor(t_train1)   \n",
    "x_bc = torch.ones(N_bc)*x_l\n",
    "x_ic = torch.rand(N_ic)\n",
    "t_bc = torch.rand(N_bc)\n",
    "t_ic = torch.ones(N_ic)*t_i\n",
    "x_train2 = torch.cat((x_train1,x_bc,x_ic),0)\n",
    "t_train2 = torch.cat((t_train1,t_bc,t_ic),0)\n",
    "null = torch.zeros(N_div_train*N_div_train + N_bc + N_ic)\n",
    "\n",
    "x_train2 = x_train2.unsqueeze(-1)\n",
    "t_train2 = t_train2.unsqueeze(-1)\n",
    "x_train = x_train2.clone().detach().requires_grad_(True)\n",
    "t_train = t_train2.clone().detach().requires_grad_(True)\n",
    "null = null.unsqueeze(-1)\n",
    "\n",
    "# Setup NN\n",
    "n_input = 2\n",
    "n_output = 1\n",
    "n_nodes = 30\n",
    "NN1 = nn.Sequential( nn.Linear(n_input, n_nodes), nn.Tanh(),\n",
    "                     nn.Linear(n_nodes, n_nodes), nn.Tanh(),\n",
    "                     nn.Linear(n_nodes, n_nodes), nn.Tanh(),\n",
    "                     nn.Linear(n_nodes, n_nodes), nn.Tanh(),\n",
    "                     nn.Linear(n_nodes, n_nodes), nn.Tanh(),\n",
    "                     nn.Linear(n_nodes, n_output) )\n",
    "\n",
    "n_input = 1\n",
    "NN2 = nn.Sequential( nn.Linear(1, n_nodes), nn.Tanh(),\n",
    "                     nn.Linear(n_nodes, n_nodes), nn.Tanh(),\n",
    "                     nn.Linear(n_nodes, n_nodes), nn.Tanh(),\n",
    "                     nn.Linear(n_nodes, n_nodes), nn.Tanh(),\n",
    "                     nn.Linear(n_nodes, n_nodes), nn.Tanh(),\n",
    "                     nn.Linear(n_nodes, n_output) )\n",
    "\n",
    "for layer in NN1.modules():\n",
    "    if isinstance(layer, nn.Linear):\n",
    "         layer.weight.data.normal_(mean=0, std=0.3)\n",
    "            \n",
    "for layer in NN2.modules():\n",
    "    if isinstance(layer, nn.Linear):\n",
    "         layer.weight.data.normal_(mean=0, std=0.3)\n",
    "\n",
    "# Hyper-parameters\n",
    "learning_rate = 2e-4\n",
    "n_iters = 32000\n",
    "            \n",
    "# Setup Loss function and LBFGS Optimiser\n",
    "mse = nn.MSELoss()\n",
    "optimiser = torch.optim.LBFGS([*NN1.parameters(), *NN2.parameters()],\n",
    "                              lr = learning_rate, \n",
    "                              tolerance_change=1e-40,\n",
    "                              max_iter = n_iters)\n",
    "\n",
    "# For training NN\n",
    "def closure(): \n",
    "    \n",
    "    T = NN1( torch.cat((x_train, t_train),1) )\n",
    "    dTdt = torch.autograd.grad(T, t_train, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "    dTdx = torch.autograd.grad(T, x_train, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "    dT2dx2 = torch.autograd.grad(dTdx, x_train, grad_outputs=torch.ones_like(dTdx), create_graph=True)[0]\n",
    "    s = NN2(t_train)\n",
    "    Ts = NN1( torch.cat((s, t_train),1) )\n",
    "    dTsdx = torch.autograd.grad(Ts, s, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "    dsdt = torch.autograd.grad(s, t_train, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "    \n",
    "    optimiser.zero_grad()\n",
    "    eq1 = mse( dTdt-c1*dT2dx2, null )\n",
    "    ic1 = 6*mse( torch.mul(torch.where(t_train == t_i,1,0),(T - T_i)), null )\n",
    "    bc1 = 6*mse( torch.mul(torch.where(x_train == x_l,1,0),(T - left_temp)), null )\n",
    "    bc2 = 6*mse( Ts - right_temp, null )\n",
    "    eq2 = mse( dsdt+c2*dTsdx, null ) \n",
    "    ic2 = 6*mse( torch.mul(torch.where(t_train == t_i,1,0),(s - s_i)), null )\n",
    "    loss = eq1 + bc1 + bc2 + ic1 + eq2 + ic2\n",
    "    loss.backward()   \n",
    "    \n",
    "    iters.append(iters[-1]+1)\n",
    "    loss_store.append(loss.detach().numpy())\n",
    "    if iters[-1]%100 == 0:\n",
    "        print('epoch = ',iters[-1],', loss = ',loss.detach().numpy())\n",
    "        \n",
    "    if i%1000 == 0:\n",
    "        print('eq1_loss = ',eq1.detach().numpy())\n",
    "        print('ic1_loss = ',ic1.detach().numpy())\n",
    "        print('bc1_loss = ',bc1.detach().numpy())\n",
    "        print('bc2_loss = ',bc2.detach().numpy())\n",
    "        print('eq2_loss = ',eq2.detach().numpy())\n",
    "        print('ic2_loss = ',ic2.detach().numpy())\n",
    "        \n",
    "    return loss\n",
    "\n",
    "optimiser.step(closure)\n",
    "\n",
    "# Extract Weights and Biases\n",
    "w1 = list(NN1.parameters())\n",
    "w2 = list(NN2.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d50b27c",
   "metadata": {},
   "source": [
    "# Adams Optimiser, c1 = c2 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea236bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Iterations v/s Loss Storage\n",
    "iters = [0]\n",
    "loss_store = []\n",
    "\n",
    "# Boundary Conditions \n",
    "left_temp = 1\n",
    "right_temp = 0\n",
    "x_l = 0\n",
    "x_r = 1\n",
    "t_i = 0\n",
    "t_f = 1\n",
    "s_i = 0.000000001\n",
    "T_i = 0\n",
    "\n",
    "# Parameters of the equation\n",
    "c1 = 1.0\n",
    "c2 = 0.5\n",
    "\n",
    "# Setup training and test dataset\n",
    "N_div_train = 35\n",
    "N_bc = 200\n",
    "N_ic = 200\n",
    "ax = 0.01\n",
    "rx = (x_r/ax)**(1/(N_div_train-1))\n",
    "at = 0.01\n",
    "rt = (t_f/at)**(1/(N_div_train-1))\n",
    "\n",
    "x_train1 =[]\n",
    "t_train1 =[]\n",
    "for i in range(N_div_train):\n",
    "    for j in range(N_div_train):\n",
    "#         x_train1.append(x_l + (i-1)*(x_r-x_l)/N_div_train)\n",
    "        x_train1.append(ax*rx**i)\n",
    "        t_train1.append(t_i + (j-1)*t_f/N_div_train)\n",
    "#         t_train1.append(at*rt**i)\n",
    "\n",
    "x_train1 = torch.FloatTensor(x_train1)\n",
    "t_train1 = torch.FloatTensor(t_train1)   \n",
    "x_bc = torch.ones(N_bc)*x_l\n",
    "x_ic = torch.rand(N_ic)\n",
    "t_bc = torch.rand(N_bc)\n",
    "t_ic = torch.ones(N_ic)*t_i\n",
    "x_train2 = torch.cat((x_train1,x_bc,x_ic),0)\n",
    "t_train2 = torch.cat((t_train1,t_bc,t_ic),0)\n",
    "null = torch.zeros(N_div_train*N_div_train + N_bc + N_ic)\n",
    "\n",
    "x_train2 = x_train2.unsqueeze(-1)\n",
    "t_train2 = t_train2.unsqueeze(-1)\n",
    "x_train = x_train2.clone().detach().requires_grad_(True)\n",
    "t_train = t_train2.clone().detach().requires_grad_(True)\n",
    "null = null.unsqueeze(-1)\n",
    "\n",
    "# Setup NN\n",
    "n_input = 2\n",
    "n_output = 1\n",
    "n_nodes = 20\n",
    "NN1 = nn.Sequential( nn.Linear(n_input, n_nodes), nn.Tanh(),\n",
    "                     nn.Linear(n_nodes, n_nodes), nn.Tanh(),\n",
    "                     nn.Linear(n_nodes, n_nodes), nn.Tanh(),\n",
    "                     nn.Linear(n_nodes, n_nodes), nn.Tanh(),\n",
    "                     nn.Linear(n_nodes, n_output) )\n",
    "\n",
    "n_input = 1\n",
    "NN2 = nn.Sequential( nn.Linear(1, n_nodes), nn.Tanh(),\n",
    "                     nn.Linear(n_nodes, n_nodes), nn.Tanh(),\n",
    "                     nn.Linear(n_nodes, n_nodes), nn.Tanh(),\n",
    "                     nn.Linear(n_nodes, n_nodes), nn.Tanh(),\n",
    "                     nn.Linear(n_nodes, n_output) )\n",
    "\n",
    "for layer in NN1.modules():\n",
    "    if isinstance(layer, nn.Linear):\n",
    "         layer.weight.data.normal_(mean=0, std=0.2)\n",
    "            \n",
    "for layer in NN2.modules():\n",
    "    if isinstance(layer, nn.Linear):\n",
    "         layer.weight.data.normal_(mean=0, std=0.2)\n",
    "\n",
    "# Hyper-parameters\n",
    "learning_rate = 2e-4\n",
    "learning_rate_1 = 9e-5\n",
    "n_iters = 62000\n",
    "            \n",
    "#loss function weights\n",
    "\n",
    "w1 = 1\n",
    "w2 = 1\n",
    "w3 = 6\n",
    "w4 = 1\n",
    "w5 = 1\n",
    "w6 = 12\n",
    "\n",
    "# Setup Loss function and Optimiser\n",
    "mse = nn.MSELoss()\n",
    "# optimiser = torch.optim.SGD([*NN1.parameters(), *NN2.parameters()], lr=learning_rate)\n",
    "optimiser = torch.optim.Adam([*NN1.parameters(), *NN2.parameters()], lr=learning_rate)\n",
    "optimiser1 = torch.optim.Adam([*NN1.parameters(), *NN2.parameters()], lr=learning_rate_1)\n",
    "\n",
    "\n",
    "# For training NN\n",
    "for i in range(n_iters):\n",
    "    \n",
    "    T = NN1( torch.cat((x_train, t_train),1) )\n",
    "    dTdt = torch.autograd.grad(T, t_train, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "    dTdx = torch.autograd.grad(T, x_train, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "    dT2dx2 = torch.autograd.grad(dTdx, x_train, grad_outputs=torch.ones_like(dTdx), create_graph=True)[0]\n",
    "    s = NN2(t_train)\n",
    "    Ts = NN1( torch.cat((s, t_train),1) )\n",
    "    dTsdx = torch.autograd.grad(Ts, s, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "    dsdt = torch.autograd.grad(s, t_train, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "    \n",
    "    optimiser.zero_grad()\n",
    "    eq1 = w1*mse( dTdt-c1*dT2dx2, null ) \n",
    "    ic1 = w2*mse( torch.mul(torch.where(t_train == t_i,1,0),(T - T_i)), null ) \n",
    "    bc1 = w3*mse( torch.mul(torch.where(x_train == x_l,1,0),(T - left_temp)), null ) \n",
    "    bc2 = w4*mse( Ts - right_temp, null ) \n",
    "    eq2 = w5*mse( dsdt+c2*dTsdx, null ) \n",
    "    ic2 = w6*mse( torch.mul(torch.where(t_train == t_i,1,0),(s - s_i)), null ) \n",
    "    loss = eq1 + bc1 + bc2 + ic1 + eq2 + ic2\n",
    "    loss.backward()   \n",
    "    optimiser.step()\n",
    "    \n",
    "    iters.append(iters[-1]+1)\n",
    "    loss_store.append(loss.detach().numpy())\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        print('epoch = ',i,', loss = ',loss.detach().numpy())\n",
    "        \n",
    "    if i%1000 == 0:\n",
    "        print('eq1_loss = ',eq1.detach().numpy())\n",
    "        print('ic1_loss = ',ic1.detach().numpy())\n",
    "        print('bc1_loss = ',bc1.detach().numpy())\n",
    "        print('bc2_loss = ',bc2.detach().numpy())\n",
    "        print('eq2_loss = ',eq2.detach().numpy())\n",
    "        print('ic2_loss = ',ic2.detach().numpy())\n",
    "        \n",
    "    if(torch.where(loss < 0.035,1,0)):\n",
    "        print('break')\n",
    "        break\n",
    "\n",
    "        \n",
    "for i in range(n_iters):\n",
    "    \n",
    "    T = NN1( torch.cat((x_train, t_train),1) )\n",
    "    dTdt = torch.autograd.grad(T, t_train, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "    dTdx = torch.autograd.grad(T, x_train, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "    dT2dx2 = torch.autograd.grad(dTdx, x_train, grad_outputs=torch.ones_like(dTdx), create_graph=True)[0]\n",
    "    s = NN2(t_train)\n",
    "    Ts = NN1( torch.cat((s, t_train),1) )\n",
    "    dTsdx = torch.autograd.grad(Ts, s, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "    dsdt = torch.autograd.grad(s, t_train, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "    \n",
    "    optimiser1.zero_grad()\n",
    "    eq1 = w1*mse( dTdt-c1*dT2dx2, null ) \n",
    "    ic1 = w2*mse( torch.mul(torch.where(t_train == t_i,1,0),(T - T_i)), null ) \n",
    "    bc1 = w3*mse( torch.mul(torch.where(x_train == x_l,1,0),(T - left_temp)), null ) \n",
    "    bc2 = w4*mse( Ts - right_temp, null ) \n",
    "    eq2 = w5*mse( dsdt+c2*dTsdx, null ) \n",
    "    ic2 = w6*mse( torch.mul(torch.where(t_train == t_i,1,0),(s - s_i)), null ) \n",
    "    loss = eq1 + bc1 + bc2 + ic1 + eq2 + ic2\n",
    "    loss.backward()   \n",
    "    optimiser1.step()\n",
    "    \n",
    "    iters.append(iters[-1]+1)\n",
    "    loss_store.append(loss.detach().numpy())\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        print('epoch = ',i,', loss = ',loss.detach().numpy())\n",
    "        \n",
    "    if i%1000 == 0:\n",
    "        print('eq1_loss = ',eq1.detach().numpy())\n",
    "        print('ic1_loss = ',ic1.detach().numpy())\n",
    "        print('bc1_loss = ',bc1.detach().numpy())\n",
    "        print('bc2_loss = ',bc2.detach().numpy())\n",
    "        print('eq2_loss = ',eq2.detach().numpy())\n",
    "        print('ic2_loss = ',ic2.detach().numpy())\n",
    "        \n",
    "\n",
    "# Extract Weights and Biases\n",
    "w1 = list(NN1.parameters())\n",
    "w2 = list(NN2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c0e6ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Loss v/s iteration\n",
    "plt.figure(0)\n",
    "plt.plot(iters[1:len(iters)], loss_store[0:len(iters)])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530aa3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lam Calculation\n",
    "c1 = 0.1\n",
    "c2 = 0.5\n",
    "x = []\n",
    "er = []\n",
    "cnt = 0\n",
    "for i in np.arange(0.1, 5, 0.001):\n",
    "    x.append(i)\n",
    "    er.append(math.erf(x[-1]))\n",
    "    cnt = cnt+1\n",
    "    \n",
    "x = np.array(x)\n",
    "er = np.array(er)\n",
    "y =[]\n",
    "y = np.exp(-x*x)/(er*math.sqrt(math.pi))-x*c1/c2\n",
    "\n",
    "for i in range(1,cnt):\n",
    "    if(y[i]*y[i-1]<0):\n",
    "        lam = x[i]\n",
    "        print(lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6444e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature Distribution\n",
    "\n",
    "N_test = 2000\n",
    "\n",
    "t_pred = 0.3\n",
    "x_test = torch.linspace(x_l,x_r,N_test)\n",
    "t_test = torch.ones(N_test)*t_pred\n",
    "x_test = x_test.unsqueeze(-1)\n",
    "t_test = t_test.unsqueeze(-1)\n",
    "\n",
    "y_pred = NN1( torch.cat((x_test, t_test),1) )\n",
    "s_pred = NN2(t_test)\n",
    "\n",
    "y_pred = y_pred.detach().numpy()\n",
    "s_pred = s_pred.detach().numpy()\n",
    "x_test = x_test.detach().numpy()\n",
    "t_test = t_test.detach().numpy()\n",
    "\n",
    "# PINN\n",
    "for i in range(N_test):\n",
    "    if(x_test[i]>s_pred[i]):\n",
    "        y_pred[i] = 0 \n",
    "plt.plot(x_test, y_pred)\n",
    "        \n",
    "# Analytical\n",
    "s_an = np.sqrt(c1*t_pred)*2*lam\n",
    "y_an = []\n",
    "for i in range(N_test):\n",
    "    if(x_test[i]>s_an):\n",
    "        y_an.append(0)\n",
    "    else:\n",
    "        y_an.append(1 - math.erf( x_test[i]/( 2*np.sqrt(c1*t_test[i]) ) )/ math.erf(lam) )     \n",
    "plt.plot(x_test, y_an)\n",
    "\n",
    "plt.legend(['PINN', 'Analytical'])\n",
    "plt.title('Time = '+ str(t_pred) )\n",
    "plt.ylabel('Temperature')\n",
    "plt.xlabel('Domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6a284d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Interface Position\n",
    "t_test = torch.linspace(t_i, t_f, N_test)\n",
    "t_test = t_test.unsqueeze(-1)\n",
    "s_pred = NN2(t_test)\n",
    "T_interface = NN1( torch.cat((s_pred, t_test),1) )\n",
    "\n",
    "t_test = t_test.detach().numpy()\n",
    "s_pred = s_pred.detach().numpy()\n",
    "T_interface = T_interface.detach().numpy()\n",
    "\n",
    "plt.plot(t_test,s_pred)\n",
    "plt.plot(t_test, np.sqrt(t_test)*2*lam)\n",
    "plt.legend(['PINN', 'Analytical'])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Interface Position')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a43fe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_test, T_interface)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Interface Temperature')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926da39b",
   "metadata": {},
   "source": [
    "# Adams optimiser, c1 = 0.05, c2 = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e7e46d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0 , loss =  0.3267597\n",
      "eq1_loss =  2.0882705e-06\n",
      "ic1_loss =  0.014165337\n",
      "bc1_loss =  0.08661413\n",
      "bc2_loss =  0.14756735\n",
      "eq2_loss =  0.00021106032\n",
      "ic2_loss =  0.07819972\n",
      "epoch =  4000 , loss =  0.20661043\n",
      "eq1_loss =  0.00173178\n",
      "ic1_loss =  0.003920551\n",
      "bc1_loss =  0.14927007\n",
      "bc2_loss =  0.05163366\n",
      "eq2_loss =  4.4335557e-05\n",
      "ic2_loss =  1.0024007e-05\n",
      "epoch =  8000 , loss =  0.033105694\n",
      "eq1_loss =  0.0059045115\n",
      "ic1_loss =  0.0012874184\n",
      "bc1_loss =  0.013309335\n",
      "bc2_loss =  0.0042394185\n",
      "eq2_loss =  0.0043219845\n",
      "ic2_loss =  0.0040430236\n",
      "epoch =  12000 , loss =  0.012804104\n",
      "eq1_loss =  0.0007220776\n",
      "ic1_loss =  8.793018e-05\n",
      "bc1_loss =  0.009491166\n",
      "bc2_loss =  0.000515132\n",
      "eq2_loss =  0.0002533825\n",
      "ic2_loss =  0.001734416\n",
      "epoch =  16000 , loss =  0.007362347\n",
      "eq1_loss =  0.00015770481\n",
      "ic1_loss =  0.00019629775\n",
      "bc1_loss =  0.0053244466\n",
      "bc2_loss =  0.00022268287\n",
      "eq2_loss =  0.00028101166\n",
      "ic2_loss =  0.0011802037\n",
      "epoch =  20000 , loss =  0.005789935\n",
      "eq1_loss =  0.00012247349\n",
      "ic1_loss =  0.0001801237\n",
      "bc1_loss =  0.003815818\n",
      "bc2_loss =  0.0002211492\n",
      "eq2_loss =  0.0004621564\n",
      "ic2_loss =  0.0009882142\n",
      "epoch =  24000 , loss =  0.004918124\n",
      "eq1_loss =  7.446776e-05\n",
      "ic1_loss =  0.00016975378\n",
      "bc1_loss =  0.002988263\n",
      "bc2_loss =  0.00020979028\n",
      "eq2_loss =  0.0004750056\n",
      "ic2_loss =  0.0010008435\n",
      "epoch =  28000 , loss =  0.0045037935\n",
      "eq1_loss =  6.892674e-05\n",
      "ic1_loss =  0.00017014556\n",
      "bc1_loss =  0.0026656492\n",
      "bc2_loss =  0.00019181363\n",
      "eq2_loss =  0.0004484043\n",
      "ic2_loss =  0.0009588539\n",
      "epoch =  32000 , loss =  0.004252183\n",
      "eq1_loss =  6.917311e-05\n",
      "ic1_loss =  0.00017200792\n",
      "bc1_loss =  0.002487842\n",
      "bc2_loss =  0.00017818598\n",
      "eq2_loss =  0.00041884\n",
      "ic2_loss =  0.0009261338\n",
      "epoch =  36000 , loss =  0.0040414757\n",
      "eq1_loss =  6.659833e-05\n",
      "ic1_loss =  0.0001745907\n",
      "bc1_loss =  0.0023423065\n",
      "bc2_loss =  0.00016457551\n",
      "eq2_loss =  0.00039275005\n",
      "ic2_loss =  0.00090065453\n",
      "epoch =  40000 , loss =  0.003865708\n",
      "eq1_loss =  6.8751775e-05\n",
      "ic1_loss =  0.00017566631\n",
      "bc1_loss =  0.0022342808\n",
      "bc2_loss =  0.00015625347\n",
      "eq2_loss =  0.00035901618\n",
      "ic2_loss =  0.00087173906\n",
      "epoch =  44000 , loss =  0.0037140562\n",
      "eq1_loss =  6.783353e-05\n",
      "ic1_loss =  0.00017723063\n",
      "bc1_loss =  0.00213845\n",
      "bc2_loss =  0.00014828319\n",
      "eq2_loss =  0.00033524106\n",
      "ic2_loss =  0.000847018\n",
      "epoch =  48000 , loss =  0.0035854224\n",
      "eq1_loss =  6.997826e-05\n",
      "ic1_loss =  0.00017839862\n",
      "bc1_loss =  0.0020611654\n",
      "bc2_loss =  0.00014181001\n",
      "eq2_loss =  0.00031312744\n",
      "ic2_loss =  0.0008209428\n",
      "time elapsed =  646.6702842712402\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Iterations v/s Loss Storage\n",
    "iters = [0]\n",
    "loss_store = []\n",
    "\n",
    "# Boundary Conditions \n",
    "left_temp = 1\n",
    "right_temp = 0\n",
    "x_l = 0\n",
    "x_r = 0.7\n",
    "t_i = 0\n",
    "t_f = 1\n",
    "s_i = 0.000000001\n",
    "T_i = 0\n",
    "\n",
    "# Parameters of the equation\n",
    "k1 = 0.05\n",
    "k2 = 0.8\n",
    "\n",
    "# Setup training and test dataset\n",
    "N_div_train = 35\n",
    "N_bc = 100\n",
    "N_ic = 100\n",
    "a = 0.01\n",
    "r = (x_r/a)**(1/(N_div_train-1))\n",
    "\n",
    "x_train1 =[]\n",
    "t_train1 =[]\n",
    "for i in range(N_div_train):\n",
    "    for j in range(N_div_train):\n",
    "#         x_train1.append(x_l + (i-1)*(x_r-x_l)/N_div_train)\n",
    "        x_train1.append(a*r**i)\n",
    "        t_train1.append(t_i + (j-1)*t_f/N_div_train)\n",
    "\n",
    "x_train1 = torch.FloatTensor(x_train1)\n",
    "t_train1 = torch.FloatTensor(t_train1)   \n",
    "x_bc = torch.ones(N_bc)*x_l\n",
    "x_ic = torch.rand(N_ic)\n",
    "t_bc = torch.rand(N_bc)\n",
    "t_ic = torch.ones(N_ic)*t_i\n",
    "x_train2 = torch.cat((x_train1,x_bc,x_ic),0)\n",
    "t_train2 = torch.cat((t_train1,t_bc,t_ic),0)\n",
    "null = torch.zeros(N_div_train*N_div_train + N_bc + N_ic)\n",
    "\n",
    "x_train2 = x_train2.unsqueeze(-1)\n",
    "t_train2 = t_train2.unsqueeze(-1)\n",
    "x_train = x_train2.clone().detach().requires_grad_(True)\n",
    "t_train = t_train2.clone().detach().requires_grad_(True)\n",
    "null = null.unsqueeze(-1)\n",
    "\n",
    "# Setup NN\n",
    "n_input = 2\n",
    "n_output = 1\n",
    "n_nodes = 5\n",
    "NN1 = nn.Sequential( nn.Linear(n_input, n_nodes), nn.Tanh(),\n",
    "                     nn.Linear(n_nodes, n_nodes), nn.Tanh(),\n",
    "                     nn.Linear(n_nodes, n_nodes), nn.Tanh(),\n",
    "                     nn.Linear(n_nodes, n_output) )\n",
    "\n",
    "n_input = 1\n",
    "NN2 = nn.Sequential( nn.Linear(1, n_nodes), nn.Tanh(),\n",
    "                     nn.Linear(n_nodes, n_nodes), nn.Tanh(),\n",
    "                     nn.Linear(n_nodes, n_nodes), nn.Tanh(),\n",
    "                     nn.Linear(n_nodes, n_output) )\n",
    "\n",
    "for layer in NN1.modules():\n",
    "    if isinstance(layer, nn.Linear):\n",
    "         layer.weight.data.normal_(mean=0, std=0.2)\n",
    "            \n",
    "for layer in NN2.modules():\n",
    "    if isinstance(layer, nn.Linear):\n",
    "         layer.weight.data.normal_(mean=0, std=0.2)\n",
    "\n",
    "# Hyper-parameters\n",
    "learning_rate = 1e-4\n",
    "n_iters = 52000\n",
    "n_iters1 = 31000\n",
    "n_iters2 = 31000\n",
    "            \n",
    "#loss function weights\n",
    "\n",
    "w1 = 2\n",
    "w2 = 1\n",
    "w3 = 6\n",
    "w4 = 1\n",
    "w5 = 1\n",
    "w6 = 12\n",
    "\n",
    "# Setup Loss function and Optimiser\n",
    "mse = nn.MSELoss()\n",
    "# optimiser = torch.optim.SGD([*NN1.parameters(), *NN2.parameters()], lr=learning_rate)\n",
    "optimiser = torch.optim.Adam([*NN1.parameters(), *NN2.parameters()], lr=learning_rate)\n",
    "# optimiser1 = torch.optim.Adam([*NN1.parameters(), *NN2.parameters()], lr=learning_rate_1)\n",
    "# optimiser2 = torch.optim.Adam([*NN1.parameters(), *NN2.parameters()], lr=learning_rate_2)\n",
    "\n",
    "start = time.time()\n",
    "# For training NN\n",
    "for i in range(n_iters):\n",
    "    \n",
    "    T = NN1( torch.cat((x_train, t_train),1) )\n",
    "    dTdt = torch.autograd.grad(T, t_train, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "    d2Tdtdx = torch.autograd.grad(dTdt, x_train, grad_outputs=torch.ones_like(dTdt), create_graph=True)[0]\n",
    "    \n",
    "    dTdx = torch.autograd.grad(T, x_train, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "    dT2dx2 = torch.autograd.grad(dTdx, x_train, grad_outputs=torch.ones_like(dTdx), create_graph=True)[0]\n",
    "    \n",
    "    s = NN2(t_train)\n",
    "    Ts = NN1( torch.cat((s, t_train),1) )\n",
    "    dTsdx = torch.autograd.grad(Ts, s, grad_outputs=torch.ones_like(Ts), create_graph=True)[0]\n",
    "    dsdt = torch.autograd.grad(s, t_train, grad_outputs=torch.ones_like(s), create_graph=True)[0]\n",
    "    \n",
    "    optimiser.zero_grad()\n",
    "    cnt = torch.sum(torch.where(s > x_train,1,0))\n",
    "    eq1 = mse(torch.mul ( s < x_train, (dTdt-k1*dT2dx2)), null) \n",
    "#     eq1 = w1*mse(torch.mul(torch.sigmoid(s - x_train), (dTdt-k1*dT2dx2)), null)\n",
    "    ic1 = w2*mse( torch.mul(torch.where(t_train == t_i,1,0),(T - T_i)), null ) \n",
    "    bc1 = w3*mse( torch.mul(torch.where(x_train == x_l,1,0),(T - torch.sqrt(0.3 + t_train*0.7))), null ) \n",
    "    bc2 = w4*mse( Ts - right_temp, null ) \n",
    "    eq2 = w5*mse( dsdt+k2*dTsdx, null ) \n",
    "    ic2 = w6*mse( torch.mul(torch.where(t_train == t_i,1,0),(s - s_i)), null ) \n",
    "    loss = eq1 + bc1 + bc2 + ic1 + eq2 + ic2\n",
    "    loss.backward()   \n",
    "    optimiser.step()\n",
    "    \n",
    "    iters.append(iters[-1]+1)\n",
    "    loss_store.append(loss.detach().numpy())\n",
    "    \n",
    "        \n",
    "    if i%4000 == 0:\n",
    "        print('epoch = ',i,', loss = ',loss.detach().numpy())\n",
    "        print('eq1_loss = ',eq1.detach().numpy())\n",
    "        print('ic1_loss = ',ic1.detach().numpy())\n",
    "        print('bc1_loss = ',bc1.detach().numpy())\n",
    "        print('bc2_loss = ',bc2.detach().numpy())\n",
    "        print('eq2_loss = ',eq2.detach().numpy())\n",
    "        print('ic2_loss = ',ic2.detach().numpy())\n",
    "        \n",
    "end = time.time()\n",
    "print(\"time elapsed = \",end-start)\n",
    "# Extract Weights and Biases\n",
    "w1 = list(NN1.parameters())\n",
    "w2 = list(NN2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61539f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss v/s iteration\n",
    "plt.figure(0)\n",
    "plt.plot(iters[1:len(iters)], loss_store[0:len(iters)])\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Iterations')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d22bad6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# lam Calculation\n",
    "x = []\n",
    "er = []\n",
    "cnt = 0\n",
    "for i in np.arange(0.1, 5, 0.001):\n",
    "    x.append(i)\n",
    "    er.append(math.erf(x[-1]))\n",
    "    cnt = cnt+1\n",
    "    \n",
    "x = np.array(x)\n",
    "er = np.array(er)\n",
    "y =[]\n",
    "y = np.exp(-x*x)/(er*math.sqrt(math.pi))-x*k1/k2\n",
    "\n",
    "for i in range(1,cnt):\n",
    "    if(y[i]*y[i-1]<0):\n",
    "        lam = x[i]\n",
    "        print(lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cef6c8d",
   "metadata": {},
   "source": [
    "# L2-norm error calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac323007",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train =[]\n",
    "t_train =[]\n",
    "\n",
    "x_l = 0.001\n",
    "t_i = 0\n",
    "x_r = 0.3\n",
    "t_f = 0.1\n",
    "\n",
    "n_time = 100\n",
    "n_x = 100\n",
    "\n",
    "x_train = torch.linspace(x_l,x_r,n_x)\n",
    "x_train = x_train.unsqueeze(-1)\n",
    "x_train_np = np.linspace(x_l,x_r,n_x)\n",
    "\n",
    "L2_err = 0\n",
    "\n",
    "for i in np.arange(t_i, t_f, (t_f - t_i)/n_time):\n",
    "    t_train = torch.ones(n_time)*i\n",
    "    t_train = t_train.unsqueeze(-1)\n",
    "\n",
    "    # PINN\n",
    "    y_nn = NN1( torch.cat((x_train, t_train),1) )\n",
    "    s_nn = NN2(t_train)\n",
    "    y_nn = y_nn.detach().numpy()\n",
    "    s_nn = s_nn.detach().numpy()\n",
    "\n",
    "    y_nn_np = np.zeros(n_x)\n",
    "    for j in range(n_x):\n",
    "        if(x_train_np[j]<s_nn[j][0]):\n",
    "            y_nn_np[j] = y_nn[j][0] \n",
    "\n",
    "    # Analytical\n",
    "    s_an = math.sqrt(k1*i)*2*lam\n",
    "    y_an = np.zeros(n_x)\n",
    "    for j in range(n_x):\n",
    "        if(x_train_np[j]<s_an):\n",
    "            y_an[j] = 1 - math.erf( x_train[j]/( 2*np.sqrt(k1*i) ) )/ math.erf(lam) \n",
    "\n",
    "    L2_err = L2_err + np.sum((y_an - y_nn_np)**2) \n",
    "    \n",
    "L2_err = np.sqrt(L2_err)\n",
    "print(L2_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01e4eac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Domain')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Temperature Distribution\n",
    "\n",
    "N_test = 2000\n",
    "\n",
    "t_pred = 0.4\n",
    "x_test = torch.linspace(0.7,x_l,N_test)\n",
    "t_test = torch.ones(N_test)*t_pred\n",
    "x_test = x_test.unsqueeze(-1)\n",
    "t_test = t_test.unsqueeze(-1)\n",
    "\n",
    "y_pred = NN1( torch.cat((x_test, t_test),1) )\n",
    "s_pred = NN2(t_test)\n",
    "\n",
    "y_pred = y_pred.detach().numpy()\n",
    "s_pred = s_pred.detach().numpy()\n",
    "x_test = x_test.detach().numpy()\n",
    "t_test = t_test.detach().numpy()\n",
    "\n",
    "# PINN\n",
    "for i in range(N_test):\n",
    "    if(x_test[i]>s_pred[i]):\n",
    "        y_pred[i] = 0 \n",
    "plt.plot(x_test, y_pred)\n",
    "        \n",
    "# Analytical\n",
    "s_an = np.sqrt(k1*t_pred)*2*lam\n",
    "y_an = []\n",
    "for i in range(N_test):\n",
    "    if(x_test[i]>s_an):\n",
    "        y_an.append(0)\n",
    "    else:\n",
    "        y_an.append(1 - math.erf( x_test[i]/( 2*np.sqrt(k1*t_test[i]) ) )/ math.erf(lam) )     \n",
    "# plt.plot(x_test, y_an)\n",
    "\n",
    "plt.legend(['PINN', 'Analytical'])\n",
    "plt.title('Time = '+ str(t_pred) )\n",
    "plt.ylabel('Temperature')\n",
    "plt.xlabel('Domain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02f0651b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Interface Position')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interface Position\n",
    "\n",
    "t_test = torch.linspace(t_i, 1, N_test)\n",
    "t_test = t_test.unsqueeze(-1)\n",
    "s_pred = NN2(t_test)\n",
    "T_interface = NN1( torch.cat((s_pred, t_test),1) )\n",
    "\n",
    "t_test = t_test.detach().numpy()\n",
    "s_pred = s_pred.detach().numpy()\n",
    "T_interface = T_interface.detach().numpy()\n",
    "\n",
    "plt.plot(t_test,s_pred)\n",
    "# plt.plot(t_test, np.sqrt(k1*t_test)*2*lam)\n",
    "plt.legend(['PINN', 'Analytical'])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Interface Position')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
