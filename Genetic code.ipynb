{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af46a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d602f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_torch(arr):\n",
    "    \n",
    "    arr = torch.FloatTensor(arr)\n",
    "    arr = arr.unsqueeze(-1)\n",
    "    arr = arr.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def x_train_data(N_x, N_t, x_l, x_r):\n",
    "    \n",
    "    x_train = np.linspace(x_l, x_r, N_x)\n",
    "    x_train = np.tile(x_train, N_t)\n",
    "    x_train = np_to_torch(x_train)\n",
    "    \n",
    "    return x_train\n",
    "\n",
    "def t_train_data(N_x, N_t, t_i, t_f):\n",
    "    \n",
    "    t_train = np.linspace(t_i, t_f, N_t)\n",
    "    t_train = np.repeat(t_train, N_x)\n",
    "    t_train = np_to_torch(t_train)\n",
    "\n",
    "    return t_train\n",
    "\n",
    "def generate_genome(mean, std, num_weights, num_biases):\n",
    "    return np.random.normal(mean, std, num_weights), np.random.normal(mean, std, num_biases)\n",
    "\n",
    "def generate_population(num_pop, mean, std, num_weights, num_biases):\n",
    "    pop_wts = []\n",
    "    pop_bia = []\n",
    "    \n",
    "    for i in range(num_pop):\n",
    "        a, b = generate_genome(mean, std, num_weights, num_biases)\n",
    "        pop_wts.append(a)\n",
    "        pop_bia.append(b)\n",
    "    \n",
    "    return pop_wts, pop_bia\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, layer_size):\n",
    "        super(ANN, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(2, 5)\n",
    "        self.l2 = nn.Linear(5, 5)\n",
    "        self.l3 = nn.Linear(5, 1)\n",
    "        \n",
    "    def forward(self, x_train, t_train):\n",
    "        \n",
    "        tanh = nn.Tanh()\n",
    "        a = tanh( self.l1(torch.cat((x_train, t_train),1)) )\n",
    "        b = tanh( self.l2(a) )\n",
    "        T = self.l2(b)\n",
    "        \n",
    "#         print(T[0:10])\n",
    "        dTdx = torch.autograd.grad(T, x_train, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "        d2Tdx2 = torch.autograd.grad(dTdx, x_train, grad_outputs=torch.ones_like(dTdx), create_graph=True)[0]\n",
    "\n",
    "        dTdt = torch.autograd.grad(T, t_train, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "        \n",
    "        return T, dTdx, d2Tdx2, dTdt\n",
    "\n",
    "def num_weights_biases(model):\n",
    "    \n",
    "    num_weights = 0\n",
    "    num_biases = 0\n",
    "\n",
    "    for param_tensor in model.state_dict():\n",
    "        if 'weight' in param_tensor:\n",
    "            num_weights += model.state_dict()[param_tensor].numel()\n",
    "        elif 'bias' in param_tensor:\n",
    "            num_biases += model.state_dict()[param_tensor].numel()\n",
    "\n",
    "    return num_weights, num_biases\n",
    "\n",
    "def eval_fitness_loss(x_train, t_train, k1, mat_1, mat_2, mat_3, N, N_1, N_2, N_3):\n",
    "    \n",
    "    T, dTdx, d2Tdx2, dTdt  = model(x_train, t_train)\n",
    "    eq1 = torch.sum( torch.square( dTdt - k1*d2Tdx2 )  )/(N) \n",
    "    bc1 = torch.sum(torch.square(torch.mul( mat_1,(T - 1) ) ) )/N_1\n",
    "    bc2 = torch.sum(torch.square(torch.mul( mat_2,(T) ) ) )/N_2\n",
    "    ic1 = torch.sum(torch.square(torch.mul( mat_3,(T) ) ) )/N_3\n",
    "    \n",
    "    fitness = 1/(eq1 + bc1 + bc2 + ic1)\n",
    "    loss = eq1 + bc1 + bc2 + ic1\n",
    "    \n",
    "    return fitness.detach().numpy(), loss\n",
    "\n",
    "def tour_selection(scores, num_pop, selection_k):\n",
    "    \n",
    "    a = np.random.randint( 0, num_pop, size=selection_k, dtype=int)\n",
    "    \n",
    "    # Initialise first and second index\n",
    "    first_idx = 1\n",
    "    second_idx = 0\n",
    "    if a[0]>a[1]:\n",
    "        first_idx = 0\n",
    "        second_idx = 1\n",
    "       \n",
    "    # Tournament selection loop\n",
    "    for i in range(2, selection_k):\n",
    "        if a[i]>a[first_idx]:\n",
    "            first_idx = i\n",
    "            continue\n",
    "        if a[i]<a[first_idx] and a[i]>a[second_idx]:\n",
    "            second_idx = i\n",
    "            continue\n",
    "            \n",
    "    return first_idx, second_idx\n",
    "\n",
    "def crossover(pop_wts, pop_bia, first_idx, second_idx):\n",
    "    \n",
    "    # choose crossover index for weights and biases\n",
    "    weight_cut = np.random.randint( 0, num_weights, dtype=int)\n",
    "    bias_cut = np.random.randint( 0, num_biases, dtype=int)\n",
    "    \n",
    "    # Crossover of weights\n",
    "    a = pop_wts[first_idx].copy()\n",
    "    b = pop_wts[second_idx].copy()\n",
    "    pop_wts[first_idx] = np.concatenate( (a[0:weight_cut], b[weight_cut:]) )\n",
    "    pop_wts[second_idx] = np.concatenate( (b[0:weight_cut], a[weight_cut:]) )\n",
    "    \n",
    "    # Crossover of biases\n",
    "    a = pop_bia[first_idx].copy()\n",
    "    b = pop_bia[second_idx].copy()\n",
    "    pop_bia[first_idx] = np.concatenate( (a[0:bias_cut], b[bias_cut:]) )\n",
    "    pop_bia[second_idx] = np.concatenate( (b[0:bias_cut], a[bias_cut:]) )\n",
    "    \n",
    "    return pop_wts, pop_bia\n",
    "\n",
    "def scramble_mutation(pop_wts, pop_bia, first_idx, second_idx, mutation_wts, mutation_bia, num_weights, num_biases):\n",
    "    \n",
    "    # mutate first child\n",
    "    a = np.random.randint( 0, num_weights - mutation_wts, dtype=int)\n",
    "    np.random.shuffle( pop_wts[first_idx][a:a + mutation_wts] )\n",
    "    a = np.random.randint( 0, num_biases - mutation_bia, dtype=int)\n",
    "    np.random.shuffle( pop_bia[first_idx][a:a + mutation_bia] )\n",
    "    \n",
    "    # mutate second child\n",
    "    a = np.random.randint( 0, num_weights - mutation_wts, dtype=int)\n",
    "    np.random.shuffle( pop_wts[second_idx][a:a + mutation_wts] )\n",
    "    a = np.random.randint( 0, num_biases - mutation_bia, dtype=int)\n",
    "    np.random.shuffle( pop_bia[second_idx][a:a + mutation_bia] )\n",
    "    \n",
    "    return pop_wts, pop_bia\n",
    "\n",
    "def load_wts_bia(model, wts, bia):\n",
    "    \n",
    "    model.l1.weight.data = torch.FloatTensor( wts[0:10].reshape((5, 2)) )\n",
    "    model.l2.weight.data = torch.FloatTensor( wts[10:35].reshape((5, 5)) )\n",
    "    model.l3.weight.data = torch.FloatTensor( wts[35:].reshape((1, 5)) )\n",
    "    \n",
    "    model.l1.bias.data = torch.FloatTensor( bia[0:5].reshape((5)) )\n",
    "    model.l2.bias.data = torch.FloatTensor( bia[5:10].reshape((5)) )\n",
    "    model.l3.bias.data = torch.FloatTensor( bia[10].reshape((1)) )\n",
    "    \n",
    "def get_wts_bia(model, wts, bia):\n",
    "    \n",
    "    w1 = model.l1.weight.data.flatten().detach().numpy() \n",
    "    w2 = model.l2.weight.data.flatten().detach().numpy() \n",
    "    w3 = model.l3.weight.data.flatten().detach().numpy() \n",
    "    wts = np.concatenate((w1, w2, w3))\n",
    "    \n",
    "    b1 = model.l1.bias.data.flatten().detach().numpy() \n",
    "    b2 = model.l2.bias.data.flatten().detach().numpy() \n",
    "    b3 = model.l3.bias.data.flatten().detach().numpy() \n",
    "    bia = np.concatenate((b1, b2, b3))\n",
    "    \n",
    "    return wts, bia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81c095f6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN(\n",
      "  (l1): Linear(in_features=2, out_features=5, bias=True)\n",
      "  (l2): Linear(in_features=5, out_features=5, bias=True)\n",
      "  (l3): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n",
      "Total trainable parameters in the model: 51\n",
      "Number of weights: 40\n",
      "Number of biases: 11\n"
     ]
    }
   ],
   "source": [
    "N_x = 40\n",
    "N_t = 40\n",
    "\n",
    "x_l = 0\n",
    "x_r = 0.5\n",
    "\n",
    "t_i = 0\n",
    "t_f = 0.6\n",
    "\n",
    "# Genetic algorithm\n",
    "num_pop = 50\n",
    "num_gen = 200\n",
    "selection_k = 10\n",
    "mutation_wts = 7\n",
    "mutation_bia = 3\n",
    "\n",
    "# material params\n",
    "k1 = 0.05\n",
    "\n",
    "# Neural network params\n",
    "layer_size = [2, 2, 1]\n",
    "mean = 0\n",
    "std = 0.3\n",
    "\n",
    "# Neural Network architecture\n",
    "model = ANN(layer_size)\n",
    "print(model)\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total trainable parameters in the model:\", total_trainable_params)\n",
    "num_weights, num_biases = num_weights_biases(model)\n",
    "print(f\"Number of weights: {num_weights}\")\n",
    "print(f\"Number of biases: {num_biases}\")\n",
    "\n",
    "# Setup Loss function and Optimiser\n",
    "lr = 5e-3\n",
    "epochs = 150\n",
    "optimiser1 = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1762763c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N =  1600\n",
      "N_3 =  tensor(39)\n",
      "N_2 =  tensor(40)\n",
      "N_1 =  tensor(40)\n",
      "Generation =  0 , Best Fitness =  0.94889164\n",
      "Solution Index =  1\n",
      "Generation =  10 , Best Fitness =  4.321183\n",
      "Solution Index =  48\n",
      "Generation =  20 , Best Fitness =  5.467894\n",
      "Solution Index =  48\n",
      "Generation =  30 , Best Fitness =  6.2849503\n",
      "Solution Index =  48\n",
      "Generation =  40 , Best Fitness =  6.6559963\n",
      "Solution Index =  48\n",
      "Generation =  50 , Best Fitness =  6.9747305\n",
      "Solution Index =  46\n",
      "Generation =  60 , Best Fitness =  7.2436576\n",
      "Solution Index =  46\n",
      "Generation =  70 , Best Fitness =  7.5133176\n",
      "Solution Index =  26\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17232/2180573667.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;31m# Evaluate loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_fitness_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmat_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmat_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmat_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0moptimiser1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17232/684103226.py\u001b[0m in \u001b[0;36meval_fitness_loss\u001b[1;34m(x_train, t_train, k1, mat_1, mat_2, mat_3, N, N_1, N_2, N_3)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0meq1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdTdt\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mk1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0md2Tdx2\u001b[0m \u001b[1;33m)\u001b[0m  \u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0mbc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mmat_1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mN_1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[0mbc2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mmat_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mN_2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m     \u001b[0mic1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mmat_3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mN_3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_fitness = []\n",
    "\n",
    "# initialise population\n",
    "pop_wts, pop_bia = generate_population(num_pop, mean, std, num_weights, num_biases)\n",
    "\n",
    "# initialise training data and matrices \n",
    "x_train = x_train_data(N_x, N_t, x_l, x_r)\n",
    "t_train = t_train_data(N_x, N_t, t_i, t_f)\n",
    "N = x_train.shape[0]\n",
    "print(\"N = \", N)\n",
    "mat_3 = torch.mul( torch.where(t_train == t_i,1,0), torch.where(x_train != 0,1,0) )\n",
    "N_3 = torch.sum( mat_3 )\n",
    "print(\"N_3 = \", N_3)\n",
    "mat_2 = torch.where(x_train == x_r,1,0)\n",
    "N_2 = torch.sum(mat_2)\n",
    "print(\"N_2 = \", N_2)\n",
    "mat_1 = torch.where(x_train == x_l,1,0)\n",
    "N_1 = torch.sum(mat_1)\n",
    "print(\"N_1 = \", N_1)\n",
    "\n",
    "for gen in range(num_gen):\n",
    "    \n",
    "    ################### Gradient Based Algorithm #####################\n",
    "    for i in range(num_pop):\n",
    "        \n",
    "        # Load wts and biases\n",
    "        load_wts_bia(model, pop_wts[i], pop_bia[i])\n",
    "        \n",
    "        for j in range(epochs):\n",
    "        \n",
    "            # Evaluate loss\n",
    "            _, loss = eval_fitness_loss(x_train, t_train, k1, mat_1, mat_2, mat_3, N, N_1, N_2, N_3) \n",
    "            optimiser1.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser1.step()\n",
    "            \n",
    "        # get weights and store them\n",
    "#         print('i = ',i,', loss = ', loss.detach().numpy())\n",
    "        wts, bia = get_wts_bia(model, pop_wts[i], pop_bia[i])\n",
    "        pop_wts[i] = wts\n",
    "        pop_bia[i] = bia\n",
    "        \n",
    "    ################### Genetic Algorithm #####################\n",
    "    scores = []\n",
    "    for i in range(num_pop):\n",
    "\n",
    "        # Load wts and biases\n",
    "        load_wts_bia(model, pop_wts[i], pop_bia[i])\n",
    "\n",
    "        # Evaluate fitness\n",
    "        fitness, _ = eval_fitness_loss(x_train, t_train, k1, mat_1, mat_2, mat_3, N, N_1, N_2, N_3) \n",
    "        scores.append(fitness)\n",
    "        \n",
    "    # Store results\n",
    "    best_fitness.append(max(scores))\n",
    "    sol_idx = scores.index(best_fitness[-1])\n",
    "    \n",
    "    if gen%10==0:\n",
    "        print('Generation = ',gen,', Best Fitness = ', best_fitness[-1])\n",
    "        print('Solution Index = ', sol_idx)\n",
    "    \n",
    "    # Tournament selection\n",
    "    first_idx, second_idx = tour_selection(scores, num_pop, selection_k)\n",
    "\n",
    "    # Crossover\n",
    "    pop_wts, pop_bia = crossover(pop_wts, pop_bia, first_idx, second_idx)\n",
    "    \n",
    "    # Mutation\n",
    "    pop_wts, pop_bia = scramble_mutation(pop_wts, pop_bia, first_idx, second_idx, mutation_wts, mutation_bia, num_weights, num_biases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
