{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4ac27d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.init as init\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8819113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_torch(arr):\n",
    "    \n",
    "    arr = torch.FloatTensor(arr)\n",
    "    arr = arr.unsqueeze(-1)\n",
    "    arr = arr.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def x_train_data(N_x, x_l, x_r, N_bc):\n",
    "\n",
    "    x_train = np.geomspace(x_l+0.001, x_r, N_x)\n",
    "    x_bc1 = np.ones(N_bc)*x_l\n",
    "    x_bc2 = np.ones(N_bc)*x_r\n",
    "    x_train = np.concatenate((x_train,x_bc1,x_bc2),0)\n",
    "    x_train = np_to_torch(x_train)\n",
    "    N_xl = torch.sum( torch.where(x_train == x_l,1,0) ).detach().numpy().item()\n",
    "    N_xr = torch.sum( torch.where(x_train == x_r,1,0) ).detach().numpy().item()\n",
    "    \n",
    "    return x_train, N_xl, N_xr\n",
    "\n",
    "def initial_temp(N_x, N_bc, T_l, T_r):\n",
    "\n",
    "    T_prev_1 = np.zeros(N_x)\n",
    "    T_prev_1[0] = T_l\n",
    "    T_prev_2 = np.ones(N_bc)*T_l\n",
    "    T_prev_3 = np.ones(N_bc)*T_r\n",
    "    T_prev = np.concatenate((T_prev_1, T_prev_2, T_prev_3),0)\n",
    "    T_prev = np_to_torch(T_prev)\n",
    "    \n",
    "    return T_prev\n",
    "\n",
    "def x_test_data(N_x_test, x_l, x_r):\n",
    "\n",
    "    x_test = np.linspace(x_l, x_r, N_x_test)\n",
    "    x_test_np = x_test.reshape(N_x_test,1)\n",
    "    x_test_torch = np_to_torch(x_test)\n",
    "    \n",
    "    return x_test_np, x_test_torch\n",
    "            \n",
    "def xavier_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0.05)\n",
    "    \n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, layer_size):\n",
    "        super(ANN, self).__init__()\n",
    "        \n",
    "        # Fully conected model\n",
    "        modules = []\n",
    "        for i in range(len(layer_size) - 1):\n",
    "            modules.append(nn.Linear(layer_size[i], layer_size[i+1]))  \n",
    "            modules.append(nn.Tanh())\n",
    "\n",
    "        self.fc = nn.Sequential(*modules)\n",
    "        self.fc.apply(xavier_init)\n",
    "        \n",
    "    def forward(self, x_train):\n",
    "        op = self.fc( x_train )\n",
    "        op_x = torch.autograd.grad(op, x_train, grad_outputs=torch.ones_like(op), create_graph=True)[0]\n",
    "        op_x2 = torch.autograd.grad(op_x, x_train, grad_outputs=torch.ones_like(op_x), create_graph=True)[0]\n",
    "\n",
    "        return op, op_x2\n",
    "    \n",
    "def get_loss(x_train, k1, N_tot, T_l, T_r, N_xl, N_xr, x_l, x_r, T_prev, del_t):\n",
    "    \n",
    "    mse = nn.MSELoss(reduction='sum')\n",
    "    w1 = 1\n",
    "    w2 = 1\n",
    "    w3 = 1\n",
    "    T, d2Tdx2 = model(x_train)\n",
    "    eq1 = w1*torch.sum( torch.square(T - T_prev - del_t*k1*d2Tdx2 ) )/N_tot\n",
    "    bc1 = w2*torch.sum( torch.square( torch.mul(torch.where(x_train == x_l,1,0),(T - T_l)) ) )/(N_xl)\n",
    "    bc2 = w3*torch.sum( torch.square( torch.mul(torch.where(x_train == x_r,1,0),(T - T_r)) ) )/(N_xr)\n",
    "    loss = eq1 + bc1 + bc2\n",
    "    \n",
    "    return loss, eq1, bc1, bc2\n",
    "\n",
    "def print_loss(epoch, loss, eq1, bc1, bc2):\n",
    "    print('epoch = ',epoch)\n",
    "    print('loss = ',loss.detach().numpy())\n",
    "    print('eq1_loss = ',eq1.detach().numpy())\n",
    "    print('bc1_loss = ',bc1.detach().numpy())\n",
    "    print('bc2_loss = ',bc2.detach().numpy())\n",
    "\n",
    "def L2_err(N_x_test, x_test, y_an, model):\n",
    "    \n",
    "    y_pred,_  = model(x_test)\n",
    "    y_pred = y_pred.detach().numpy()\n",
    "\n",
    "    L2_err =  np.sum((y_an - y_pred)**2)/(N_x_test)\n",
    "    return L2_err\n",
    "\n",
    "def analytical(x_test, t_test, T_l):\n",
    "\n",
    "    i = 1\n",
    "    y_an = 0\n",
    "    for i in range(1, 60, 1): \n",
    "        y_an += -2*T_l*(-1)**(i+1)/(i*np.pi)*np.sin(i*np.pi*(1 - x_test))*np.exp(-(i*np.pi)**2*k1*t_test)   \n",
    "    y_an += T_l*(1 - x_test)\n",
    "    \n",
    "    return y_an\n",
    "    \n",
    "    \n",
    "def train_model(model, optimiser1, epochs, T_r, T_l, k1, N_x, x_l, x_r, N_t, N_bc, accuracy_cap, N_x_test):\n",
    "    \n",
    "    loss_store = []\n",
    "    T_store_pred = []\n",
    "    T_store_an = []\n",
    "    mse = nn.MSELoss(reduction='sum')\n",
    "    model.train()  \n",
    "    \n",
    "    N_tot = N_x + 2*N_bc\n",
    "    print(\"N_tot = \", N_tot)\n",
    "    x_train, N_xl, N_xr = x_train_data(N_x, x_l, x_r, N_bc)\n",
    "    x_test_np, x_test_torch = x_test_data(N_x_test, x_l, x_r)\n",
    "    t_test = 0\n",
    "    \n",
    "    for i in range(N_t):\n",
    "        \n",
    "        t_test = t_test + del_t\n",
    "        print(\"t = \", t_test)\n",
    "        print(\" \")\n",
    "        y_an = analytical(x_test_np, t_test, T_l)\n",
    "        \n",
    "        if(i==0):\n",
    "            T_prev = initial_temp(N_x, N_bc, T_l, T_r)\n",
    "        else:\n",
    "            T_prev,_ = model(x_train)  \n",
    "            T_prev = T_prev.clone().detach().requires_grad_(False)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            #Backpropogation and optimisation\n",
    "            loss, eq1, bc1, bc2 = get_loss(x_train, k1, N_tot, T_l, T_r, N_xl, N_xr, x_l, x_r, T_prev, del_t)\n",
    "            optimiser1.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser1.step()  \n",
    "            loss_store.append(loss.detach().numpy())\n",
    "\n",
    "            L2_norm_err = L2_err(N_x_test, x_test_torch, y_an, model)\n",
    "            \n",
    "            if epoch%1000==0:\n",
    "                print_loss(epoch, loss, eq1, bc1, bc2)\n",
    "                print(\"L2_err= \", L2_norm_err )\n",
    "                print(\"\")\n",
    "            \n",
    "            if L2_norm_err<accuracy_cap :\n",
    "                print(\"loss limit attained, epoch = \", epoch,\" L2_err= \", L2_norm_err)\n",
    "                print(\"\")\n",
    "                break\n",
    "            \n",
    "        # Store the results after each time step\n",
    "        T_st,_ = model(x_test_torch)   \n",
    "        T_st = T_st.detach().numpy()\n",
    "        T_store_pred.append(T_st)\n",
    "        T_an = analytical(x_test_np, t_test, T_l)\n",
    "        T_store_an.append(T_an)\n",
    "        \n",
    "        print(\"broke inner loop\")\n",
    "        print(\"\")\n",
    "\n",
    "    return loss_store, T_store_pred, T_store_an, x_test_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa5eeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=25, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=25, out_features=25, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=25, out_features=25, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=25, out_features=1, bias=True)\n",
      "    (7): Tanh()\n",
      "  )\n",
      ")\n",
      "Total trainable parameters in the model: 1376\n",
      "N_tot =  121\n",
      "t =  0.005\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.3151473\n",
      "eq1_loss =  0.30192807\n",
      "bc1_loss =  0.99950254\n",
      "bc2_loss =  0.0137166465\n",
      "L2_err=  0.021165524675879555\n",
      "\n",
      "epoch =  1000\n",
      "loss =  0.2673001\n",
      "eq1_loss =  0.21504194\n",
      "bc1_loss =  0.052060287\n",
      "bc2_loss =  0.00019787782\n",
      "L2_err=  0.2103212548386834\n",
      "\n",
      "epoch =  2000\n",
      "loss =  0.26301298\n",
      "eq1_loss =  0.21398477\n",
      "bc1_loss =  0.048825648\n",
      "bc2_loss =  0.00020255886\n",
      "L2_err=  0.19319733286243526\n",
      "\n",
      "epoch =  3000\n",
      "loss =  0.2459944\n",
      "eq1_loss =  0.1980615\n",
      "bc1_loss =  0.04779861\n",
      "bc2_loss =  0.00013430932\n",
      "L2_err=  0.12096354520681049\n",
      "\n",
      "epoch =  4000\n",
      "loss =  0.21405278\n",
      "eq1_loss =  0.1722159\n",
      "bc1_loss =  0.041833572\n",
      "bc2_loss =  3.3197725e-06\n",
      "L2_err=  0.049686124187828834\n",
      "\n",
      "epoch =  5000\n",
      "loss =  0.1931377\n",
      "eq1_loss =  0.15688153\n",
      "bc1_loss =  0.03623972\n",
      "bc2_loss =  1.6451566e-05\n",
      "L2_err=  0.03476851220397508\n",
      "\n",
      "epoch =  6000\n",
      "loss =  0.17457727\n",
      "eq1_loss =  0.13959613\n",
      "bc1_loss =  0.034980875\n",
      "bc2_loss =  2.537193e-07\n",
      "L2_err=  0.00990472875034336\n",
      "\n",
      "epoch =  7000\n",
      "loss =  0.16201775\n",
      "eq1_loss =  0.124437064\n",
      "bc1_loss =  0.03757844\n",
      "bc2_loss =  2.2478926e-06\n",
      "L2_err=  0.004670474675939814\n",
      "\n",
      "epoch =  8000\n",
      "loss =  0.14902748\n",
      "eq1_loss =  0.10568307\n",
      "bc1_loss =  0.0433442\n",
      "bc2_loss =  2.1465173e-07\n",
      "L2_err=  0.001513730743925236\n",
      "\n",
      "epoch =  9000\n",
      "loss =  0.12272951\n",
      "eq1_loss =  0.07034587\n",
      "bc1_loss =  0.05238354\n",
      "bc2_loss =  1.0275207e-07\n",
      "L2_err=  0.0034776829695447645\n",
      "\n",
      "epoch =  10000\n",
      "loss =  0.078488596\n",
      "eq1_loss =  0.032668356\n",
      "bc1_loss =  0.045819595\n",
      "bc2_loss =  6.4470964e-07\n",
      "L2_err=  0.0038018192452185253\n",
      "\n",
      "epoch =  11000\n",
      "loss =  0.053274862\n",
      "eq1_loss =  0.019357143\n",
      "bc1_loss =  0.033917557\n",
      "bc2_loss =  1.6477512e-07\n",
      "L2_err=  0.001128282510999302\n",
      "\n",
      "epoch =  12000\n",
      "loss =  0.041736975\n",
      "eq1_loss =  0.013411494\n",
      "bc1_loss =  0.028325483\n",
      "bc2_loss =  1.6804338e-09\n",
      "L2_err=  0.0009374651606640295\n",
      "\n",
      "epoch =  13000\n",
      "loss =  0.03468115\n",
      "eq1_loss =  0.010352649\n",
      "bc1_loss =  0.024328494\n",
      "bc2_loss =  6.8419936e-09\n",
      "L2_err=  0.0007618362461935095\n",
      "\n",
      "epoch =  14000\n",
      "loss =  0.03030385\n",
      "eq1_loss =  0.008996073\n",
      "bc1_loss =  0.021307778\n",
      "bc2_loss =  4.972716e-10\n",
      "L2_err=  0.0006205016729935578\n",
      "\n",
      "epoch =  15000\n",
      "loss =  0.027291\n",
      "eq1_loss =  0.008253095\n",
      "bc1_loss =  0.0190379\n",
      "bc2_loss =  3.6107808e-09\n",
      "L2_err=  0.0005712524667813919\n",
      "\n",
      "epoch =  16000\n",
      "loss =  0.025014706\n",
      "eq1_loss =  0.007892091\n",
      "bc1_loss =  0.017122615\n",
      "bc2_loss =  3.1022385e-10\n",
      "L2_err=  0.0005442770983826559\n",
      "\n",
      "epoch =  17000\n",
      "loss =  0.023173178\n",
      "eq1_loss =  0.0076629277\n",
      "bc1_loss =  0.015510249\n",
      "bc2_loss =  1.1007987e-09\n",
      "L2_err=  0.0005142983403402928\n",
      "\n",
      "epoch =  18000\n",
      "loss =  0.021591961\n",
      "eq1_loss =  0.0075478475\n",
      "bc1_loss =  0.0140441125\n",
      "bc2_loss =  1.8289591e-09\n",
      "L2_err=  0.0004824132705530531\n",
      "\n",
      "epoch =  19000\n",
      "loss =  0.0201703\n",
      "eq1_loss =  0.007394108\n",
      "bc1_loss =  0.0127761895\n",
      "bc2_loss =  1.969935e-09\n",
      "L2_err=  0.0004532221221825193\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.01\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.07975121\n",
      "eq1_loss =  0.068125814\n",
      "bc1_loss =  0.011625392\n",
      "bc2_loss =  4.2805423e-10\n",
      "L2_err=  0.0015061347965634872\n",
      "\n",
      "epoch =  1000\n",
      "loss =  0.0045115487\n",
      "eq1_loss =  0.00065357576\n",
      "bc1_loss =  0.0038579642\n",
      "bc2_loss =  8.626497e-09\n",
      "L2_err=  0.00017551027397389222\n",
      "\n",
      "epoch =  2000\n",
      "loss =  0.0042419047\n",
      "eq1_loss =  0.0006117901\n",
      "bc1_loss =  0.003630111\n",
      "bc2_loss =  3.772734e-09\n",
      "L2_err=  0.00017454084687655577\n",
      "\n",
      "epoch =  3000\n",
      "loss =  0.0039009536\n",
      "eq1_loss =  0.0005500606\n",
      "bc1_loss =  0.0033508928\n",
      "bc2_loss =  2.7478897e-10\n",
      "L2_err=  0.00017268470602596712\n",
      "\n",
      "epoch =  4000\n",
      "loss =  0.003497739\n",
      "eq1_loss =  0.00047714263\n",
      "bc1_loss =  0.0030205962\n",
      "bc2_loss =  2.8302716e-10\n",
      "L2_err=  0.00017142240105607473\n",
      "\n",
      "epoch =  5000\n",
      "loss =  0.0030675207\n",
      "eq1_loss =  0.0003948018\n",
      "bc1_loss =  0.002672717\n",
      "bc2_loss =  1.9168023e-09\n",
      "L2_err=  0.00016596423361037039\n",
      "\n",
      "epoch =  6000\n",
      "loss =  0.0026967677\n",
      "eq1_loss =  0.00032667432\n",
      "bc1_loss =  0.0023700914\n",
      "bc2_loss =  2.143496e-09\n",
      "L2_err=  0.00015739117518273762\n",
      "\n",
      "epoch =  7000\n",
      "loss =  0.0023928082\n",
      "eq1_loss =  0.00027217687\n",
      "bc1_loss =  0.0021206252\n",
      "bc2_loss =  5.96029e-09\n",
      "L2_err=  0.00015036477227740755\n",
      "\n",
      "epoch =  8000\n",
      "loss =  0.0021446554\n",
      "eq1_loss =  0.00023754948\n",
      "bc1_loss =  0.0019071051\n",
      "bc2_loss =  1.0007837e-09\n",
      "L2_err=  0.0001454078375518165\n",
      "\n",
      "epoch =  9000\n",
      "loss =  0.0019431817\n",
      "eq1_loss =  0.00020820978\n",
      "bc1_loss =  0.0017349704\n",
      "bc2_loss =  1.587022e-09\n",
      "L2_err=  0.000142232485750322\n",
      "\n",
      "epoch =  10000\n",
      "loss =  0.0017775615\n",
      "eq1_loss =  0.00019217434\n",
      "bc1_loss =  0.0015853868\n",
      "bc2_loss =  3.0372752e-10\n",
      "L2_err=  0.00014034516879724552\n",
      "\n",
      "epoch =  11000\n",
      "loss =  0.0016400805\n",
      "eq1_loss =  0.00017864764\n",
      "bc1_loss =  0.0014614329\n",
      "bc2_loss =  4.048517e-11\n",
      "L2_err=  0.00013996264089151798\n",
      "\n",
      "epoch =  12000\n",
      "loss =  0.0015247051\n",
      "eq1_loss =  0.0001671833\n",
      "bc1_loss =  0.001357521\n",
      "bc2_loss =  6.443135e-10\n",
      "L2_err=  0.00014033766840443093\n",
      "\n",
      "epoch =  13000\n",
      "loss =  0.0014259567\n",
      "eq1_loss =  0.00016131802\n",
      "bc1_loss =  0.0012646375\n",
      "bc2_loss =  1.2514157e-09\n",
      "L2_err=  0.0001407492696078701\n",
      "\n",
      "epoch =  14000\n",
      "loss =  0.0013404042\n",
      "eq1_loss =  0.00015199483\n",
      "bc1_loss =  0.0011884085\n",
      "bc2_loss =  7.956855e-10\n",
      "L2_err=  0.00014167949322753195\n",
      "\n",
      "epoch =  15000\n",
      "loss =  0.001266195\n",
      "eq1_loss =  0.00014558464\n",
      "bc1_loss =  0.0011206102\n",
      "bc2_loss =  2.6139646e-10\n",
      "L2_err=  0.000142259375697103\n",
      "\n",
      "epoch =  16000\n",
      "loss =  0.0012027606\n",
      "eq1_loss =  0.00014004402\n",
      "bc1_loss =  0.0010627165\n",
      "bc2_loss =  1.0392479e-10\n",
      "L2_err=  0.00014313092466974924\n",
      "\n",
      "epoch =  17000\n",
      "loss =  0.0011526821\n",
      "eq1_loss =  0.00014263553\n",
      "bc1_loss =  0.0010100295\n",
      "bc2_loss =  1.710147e-08\n",
      "L2_err=  0.0001441345881343962\n",
      "\n",
      "epoch =  18000\n",
      "loss =  0.001104057\n",
      "eq1_loss =  0.0001370514\n",
      "bc1_loss =  0.0009670007\n",
      "bc2_loss =  4.7394084e-09\n",
      "L2_err=  0.00014416298205365867\n",
      "\n",
      "epoch =  19000\n",
      "loss =  0.0010546941\n",
      "eq1_loss =  0.00012744452\n",
      "bc1_loss =  0.0009272489\n",
      "bc2_loss =  5.57826e-10\n",
      "L2_err=  0.00014404143052084635\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.015\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.0069126044\n",
      "eq1_loss =  0.00602079\n",
      "bc1_loss =  0.00089180947\n",
      "bc2_loss =  5.1595817e-09\n",
      "L2_err=  0.000584935870002788\n",
      "\n",
      "loss limit attained, epoch =  883  L2_err=  9.998059899413819e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.02\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.0014782641\n",
      "eq1_loss =  0.0010011859\n",
      "bc1_loss =  0.00047707828\n",
      "bc2_loss =  2.4016344e-12\n",
      "L2_err=  0.00032615169951143104\n",
      "\n",
      "loss limit attained, epoch =  325  L2_err=  9.990794668662422e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.025\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.00080346595\n",
      "eq1_loss =  0.00047560086\n",
      "bc1_loss =  0.00032784606\n",
      "bc2_loss =  1.9043902e-08\n",
      "L2_err=  0.0002235544336116056\n",
      "\n",
      "loss limit attained, epoch =  577  L2_err=  9.999247127756225e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.030000000000000002\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.00053464365\n",
      "eq1_loss =  0.00028632977\n",
      "bc1_loss =  0.00024831382\n",
      "bc2_loss =  3.105871e-11\n",
      "L2_err=  0.0001661765768612005\n",
      "\n",
      "epoch =  1000\n",
      "loss =  0.00024839788\n",
      "eq1_loss =  4.88337e-05\n",
      "bc1_loss =  0.00019956079\n",
      "bc2_loss =  3.3851946e-09\n",
      "L2_err=  0.00011148142330501406\n",
      "\n",
      "loss limit attained, epoch =  1725  L2_err=  9.998511761320594e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.035\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.0003908085\n",
      "eq1_loss =  0.00019204748\n",
      "bc1_loss =  0.00019876004\n",
      "bc2_loss =  9.885648e-10\n",
      "L2_err=  0.00012541593693951396\n",
      "\n",
      "epoch =  1000\n",
      "loss =  0.00020159203\n",
      "eq1_loss =  3.6731988e-05\n",
      "bc1_loss =  0.00016485876\n",
      "bc2_loss =  1.2795706e-09\n",
      "L2_err=  0.00010345749851945557\n",
      "\n",
      "loss limit attained, epoch =  1178  L2_err=  9.99871792219823e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.04\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.00030247957\n",
      "eq1_loss =  0.00013799938\n",
      "bc1_loss =  0.0001644794\n",
      "bc2_loss =  7.948451e-10\n",
      "L2_err=  0.00010061340643712463\n",
      "\n",
      "loss limit attained, epoch =  3  L2_err=  9.898425253059313e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.045\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.00029524128\n",
      "eq1_loss =  0.00013299656\n",
      "bc1_loss =  0.00015971382\n",
      "bc2_loss =  2.5308927e-06\n",
      "L2_err=  0.00017938879231661406\n",
      "\n",
      "loss limit attained, epoch =  418  L2_err=  9.999608217592795e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.049999999999999996\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.0002396467\n",
      "eq1_loss =  0.000102367514\n",
      "bc1_loss =  0.00013710008\n",
      "bc2_loss =  1.7910487e-07\n",
      "L2_err=  0.00015506172113496363\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss limit attained, epoch =  631  L2_err=  9.999267686312475e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.05499999999999999\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.0002004419\n",
      "eq1_loss =  8.065289e-05\n",
      "bc1_loss =  0.00011975177\n",
      "bc2_loss =  3.7228492e-08\n",
      "L2_err=  0.00013135262644533265\n",
      "\n",
      "loss limit attained, epoch =  727  L2_err=  9.998733916560198e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.05999999999999999\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.00017059018\n",
      "eq1_loss =  6.445862e-05\n",
      "bc1_loss =  0.00010612525\n",
      "bc2_loss =  6.312778e-09\n",
      "L2_err=  0.00011577330937637894\n",
      "\n",
      "loss limit attained, epoch =  767  L2_err=  9.988052862417339e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.06499999999999999\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.00014871193\n",
      "eq1_loss =  5.375139e-05\n",
      "bc1_loss =  9.493602e-05\n",
      "bc2_loss =  2.4515664e-08\n",
      "L2_err=  0.00010416545386592095\n",
      "\n",
      "loss limit attained, epoch =  755  L2_err=  9.993859846108007e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.06999999999999999\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.00013563373\n",
      "eq1_loss =  4.957139e-05\n",
      "bc1_loss =  8.589004e-05\n",
      "bc2_loss =  1.7230246e-07\n",
      "L2_err=  9.693967612340588e-05\n",
      "\n",
      "loss limit attained, epoch =  0  L2_err=  9.693967612340588e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.075\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.00014132839\n",
      "eq1_loss =  5.371747e-05\n",
      "bc1_loss =  8.444225e-05\n",
      "bc2_loss =  3.1686704e-06\n",
      "L2_err=  0.00014438923018242176\n",
      "\n",
      "loss limit attained, epoch =  281  L2_err=  9.999768167830709e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.08\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.00011593127\n",
      "eq1_loss =  3.8031e-05\n",
      "bc1_loss =  7.770511e-05\n",
      "bc2_loss =  1.9515902e-07\n",
      "L2_err=  0.0001339554994549145\n",
      "\n",
      "loss limit attained, epoch =  379  L2_err=  9.957349600666986e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.085\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.00011378781\n",
      "eq1_loss =  4.1337582e-05\n",
      "bc1_loss =  7.1997565e-05\n",
      "bc2_loss =  4.5266285e-07\n",
      "L2_err=  0.00012449582948131753\n",
      "\n",
      "loss limit attained, epoch =  404  L2_err=  9.98770103594245e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.09000000000000001\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.000106593645\n",
      "eq1_loss =  3.9353356e-05\n",
      "bc1_loss =  6.665071e-05\n",
      "bc2_loss =  5.8958096e-07\n",
      "L2_err=  0.00011823091985363586\n",
      "\n",
      "loss limit attained, epoch =  557  L2_err=  9.93473664087771e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.09500000000000001\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.00010014368\n",
      "eq1_loss =  3.751738e-05\n",
      "bc1_loss =  6.187434e-05\n",
      "bc2_loss =  7.519629e-07\n",
      "L2_err=  0.00011220975008824928\n",
      "\n",
      "loss limit attained, epoch =  596  L2_err=  9.974995824206146e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.10000000000000002\n",
      " \n",
      "epoch =  0\n",
      "loss =  8.973297e-05\n",
      "eq1_loss =  3.1573923e-05\n",
      "bc1_loss =  5.7516685e-05\n",
      "bc2_loss =  6.423574e-07\n",
      "L2_err=  0.00010692687142341037\n",
      "\n",
      "loss limit attained, epoch =  3  L2_err=  9.799796550446882e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.10500000000000002\n",
      " \n",
      "epoch =  0\n",
      "loss =  9.1970745e-05\n",
      "eq1_loss =  3.0604628e-05\n",
      "bc1_loss =  5.6608096e-05\n",
      "bc2_loss =  4.758018e-06\n",
      "L2_err=  0.00012812185850660077\n",
      "\n",
      "loss limit attained, epoch =  277  L2_err=  9.991582510862358e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.11000000000000003\n",
      " \n",
      "epoch =  0\n",
      "loss =  7.7539415e-05\n",
      "eq1_loss =  2.3661461e-05\n",
      "bc1_loss =  5.2934956e-05\n",
      "bc2_loss =  9.429936e-07\n",
      "L2_err=  0.000122340807354539\n",
      "\n",
      "loss limit attained, epoch =  276  L2_err=  9.961006255707403e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.11500000000000003\n",
      " \n",
      "epoch =  0\n",
      "loss =  8.583679e-05\n",
      "eq1_loss =  3.4188208e-05\n",
      "bc1_loss =  5.01351e-05\n",
      "bc2_loss =  1.5134887e-06\n",
      "L2_err=  0.00011489749800971026\n",
      "\n",
      "loss limit attained, epoch =  934  L2_err=  9.978136870955221e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.12000000000000004\n",
      " \n",
      "epoch =  0\n",
      "loss =  7.295526e-05\n",
      "eq1_loss =  2.521126e-05\n",
      "bc1_loss =  4.6852358e-05\n",
      "bc2_loss =  8.916485e-07\n",
      "L2_err=  0.00010951073388565053\n",
      "\n",
      "epoch =  1000\n",
      "loss =  5.117039e-05\n",
      "eq1_loss =  7.575502e-06\n",
      "bc1_loss =  4.354692e-05\n",
      "bc2_loss =  4.7972193e-08\n",
      "L2_err=  0.00010194360279780617\n",
      "\n",
      "loss limit attained, epoch =  1535  L2_err=  9.990268950946789e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.12500000000000003\n",
      " \n",
      "epoch =  0\n",
      "loss =  7.2761795e-05\n",
      "eq1_loss =  2.8139146e-05\n",
      "bc1_loss =  4.3576027e-05\n",
      "bc2_loss =  1.0466222e-06\n",
      "L2_err=  0.00010487475380562995\n",
      "\n",
      "epoch =  1000\n",
      "loss =  4.7616304e-05\n",
      "eq1_loss =  7.134796e-06\n",
      "bc1_loss =  4.0454077e-05\n",
      "bc2_loss =  2.7432106e-08\n",
      "L2_err=  0.00010224576101825366\n",
      "\n",
      "epoch =  2000\n",
      "loss =  4.646689e-05\n",
      "eq1_loss =  6.6392763e-06\n",
      "bc1_loss =  3.9771552e-05\n",
      "bc2_loss =  5.6061374e-08\n",
      "L2_err=  0.00010127119730359456\n",
      "\n",
      "loss limit attained, epoch =  2554  L2_err=  9.989334881162329e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.13000000000000003\n",
      " \n",
      "epoch =  0\n",
      "loss =  6.3467734e-05\n",
      "eq1_loss =  2.300734e-05\n",
      "bc1_loss =  3.965437e-05\n",
      "bc2_loss =  8.060206e-07\n",
      "L2_err=  0.00010045659816537681\n",
      "\n",
      "loss limit attained, epoch =  3  L2_err=  9.777284693627269e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.13500000000000004\n",
      " \n",
      "epoch =  0\n",
      "loss =  5.7661302e-05\n",
      "eq1_loss =  1.619019e-05\n",
      "bc1_loss =  3.8927315e-05\n",
      "bc2_loss =  2.5438e-06\n",
      "L2_err=  0.00011139348874563317\n",
      "\n",
      "loss limit attained, epoch =  27  L2_err=  9.978800380400767e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.14000000000000004\n",
      " \n",
      "epoch =  0\n",
      "loss =  5.019707e-05\n",
      "eq1_loss =  1.2766508e-05\n",
      "bc1_loss =  3.7151105e-05\n",
      "bc2_loss =  2.794604e-07\n",
      "L2_err=  0.00011308397863403453\n",
      "\n",
      "loss limit attained, epoch =  61  L2_err=  9.994397101566868e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.14500000000000005\n",
      " \n",
      "epoch =  0\n",
      "loss =  4.7508456e-05\n",
      "eq1_loss =  1.2002462e-05\n",
      "bc1_loss =  3.549517e-05\n",
      "bc2_loss =  1.0821203e-08\n",
      "L2_err=  0.00010791426364041338\n",
      "\n",
      "loss limit attained, epoch =  39  L2_err=  9.993409829105362e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.15000000000000005\n",
      " \n",
      "epoch =  0\n",
      "loss =  4.5566223e-05\n",
      "eq1_loss =  1.1460866e-05\n",
      "bc1_loss =  3.4092413e-05\n",
      "bc2_loss =  1.294219e-08\n",
      "L2_err=  0.0001082057283062959\n",
      "\n",
      "loss limit attained, epoch =  61  L2_err=  9.998703366875323e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.15500000000000005\n",
      " \n",
      "epoch =  0\n",
      "loss =  4.358318e-05\n",
      "eq1_loss =  1.086522e-05\n",
      "bc1_loss =  3.271794e-05\n",
      "bc2_loss =  1.9266786e-11\n",
      "L2_err=  0.00010647243291516932\n",
      "\n",
      "loss limit attained, epoch =  66  L2_err=  9.999261026937438e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.16000000000000006\n",
      " \n",
      "epoch =  0\n",
      "loss =  4.179855e-05\n",
      "eq1_loss =  1.0352651e-05\n",
      "bc1_loss =  3.14439e-05\n",
      "bc2_loss =  2.0024002e-09\n",
      "L2_err=  0.00010527457202873778\n",
      "\n",
      "loss limit attained, epoch =  70  L2_err=  9.998774460368744e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.16500000000000006\n",
      " \n",
      "epoch =  0\n",
      "loss =  4.0175342e-05\n",
      "eq1_loss =  9.891601e-06\n",
      "bc1_loss =  3.0283652e-05\n",
      "bc2_loss =  8.784995e-11\n",
      "L2_err=  0.00010447716510084156\n",
      "\n",
      "epoch =  1000\n",
      "loss =  3.4059263e-05\n",
      "eq1_loss =  5.038762e-06\n",
      "bc1_loss =  2.90205e-05\n",
      "bc2_loss =  8.8129504e-13\n",
      "L2_err=  0.00010175689290618034\n",
      "\n",
      "epoch =  2000\n",
      "loss =  3.3605753e-05\n",
      "eq1_loss =  4.8996935e-06\n",
      "bc1_loss =  2.8701568e-05\n",
      "bc2_loss =  4.4933026e-09\n",
      "L2_err=  0.0001006663088487226\n",
      "\n",
      "loss limit attained, epoch =  2472  L2_err=  9.999850659136995e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.17000000000000007\n",
      " \n",
      "epoch =  0\n",
      "loss =  3.7461723e-05\n",
      "eq1_loss =  8.984513e-06\n",
      "bc1_loss =  2.8477207e-05\n",
      "bc2_loss =  3.987033e-12\n",
      "L2_err=  0.00010217651634888365\n",
      "\n",
      "loss limit attained, epoch =  27  L2_err=  9.994784799183682e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.17500000000000007\n",
      " \n",
      "epoch =  0\n",
      "loss =  3.619304e-05\n",
      "eq1_loss =  8.656146e-06\n",
      "bc1_loss =  2.753035e-05\n",
      "bc2_loss =  6.5456254e-09\n",
      "L2_err=  0.00010185914028040336\n",
      "\n",
      "loss limit attained, epoch =  6  L2_err=  9.99429184629735e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.18000000000000008\n",
      " \n",
      "epoch =  0\n",
      "loss =  3.5753004e-05\n",
      "eq1_loss =  8.516685e-06\n",
      "bc1_loss =  2.7157583e-05\n",
      "bc2_loss =  7.8738466e-08\n",
      "L2_err=  0.00010701049327506211\n",
      "\n",
      "loss limit attained, epoch =  34  L2_err=  9.997225173406576e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.18500000000000008\n",
      " \n",
      "epoch =  0\n",
      "loss =  3.463159e-05\n",
      "eq1_loss =  8.260212e-06\n",
      "bc1_loss =  2.6359043e-05\n",
      "bc2_loss =  1.2337268e-08\n",
      "L2_err=  0.00010900958228377423\n",
      "\n",
      "loss limit attained, epoch =  46  L2_err=  9.996429521870705e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.19000000000000009\n",
      " \n",
      "epoch =  0\n",
      "loss =  3.351721e-05\n",
      "eq1_loss =  7.961422e-06\n",
      "bc1_loss =  2.5555539e-05\n",
      "bc2_loss =  2.5064917e-10\n",
      "L2_err=  0.00010829021859412398\n",
      "\n",
      "loss limit attained, epoch =  56  L2_err=  9.998947967020207e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.1950000000000001\n",
      " \n",
      "epoch =  0\n",
      "loss =  3.2424687e-05\n",
      "eq1_loss =  7.672904e-06\n",
      "bc1_loss =  2.474846e-05\n",
      "bc2_loss =  3.321151e-09\n",
      "L2_err=  0.00010737771184310249\n",
      "\n",
      "loss limit attained, epoch =  108  L2_err=  9.999500810789145e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.2000000000000001\n",
      " \n",
      "epoch =  0\n",
      "loss =  3.1408483e-05\n",
      "eq1_loss =  7.415688e-06\n",
      "bc1_loss =  2.3991686e-05\n",
      "bc2_loss =  1.1096054e-09\n",
      "L2_err=  0.00010671324000938793\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss limit attained, epoch =  124  L2_err=  9.999624687147813e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.2050000000000001\n",
      " \n",
      "epoch =  0\n",
      "loss =  3.067824e-05\n",
      "eq1_loss =  7.404364e-06\n",
      "bc1_loss =  2.3246663e-05\n",
      "bc2_loss =  2.7212618e-08\n",
      "L2_err=  0.0001063021291271737\n",
      "\n",
      "loss limit attained, epoch =  169  L2_err=  9.995217726485866e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.2100000000000001\n",
      " \n",
      "epoch =  0\n",
      "loss =  2.9647601e-05\n",
      "eq1_loss =  7.0170454e-06\n",
      "bc1_loss =  2.2611917e-05\n",
      "bc2_loss =  1.8638922e-08\n",
      "L2_err=  0.00010559343295291948\n",
      "\n",
      "loss limit attained, epoch =  158  L2_err=  9.993722580914618e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.2150000000000001\n",
      " \n",
      "epoch =  0\n",
      "loss =  2.8935012e-05\n",
      "eq1_loss =  6.919139e-06\n",
      "bc1_loss =  2.19854e-05\n",
      "bc2_loss =  3.0473675e-08\n",
      "L2_err=  0.00010508150842345218\n",
      "\n",
      "loss limit attained, epoch =  191  L2_err=  9.988133022518608e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.2200000000000001\n",
      " \n",
      "epoch =  0\n",
      "loss =  2.8295763e-05\n",
      "eq1_loss =  6.8599234e-06\n",
      "bc1_loss =  2.139028e-05\n",
      "bc2_loss =  4.5558437e-08\n",
      "L2_err=  0.00010456992418379423\n",
      "\n",
      "loss limit attained, epoch =  194  L2_err=  9.996829530991872e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.22500000000000012\n",
      " \n",
      "epoch =  0\n",
      "loss =  2.7277e-05\n",
      "eq1_loss =  6.3942325e-06\n",
      "bc1_loss =  2.0862637e-05\n",
      "bc2_loss =  2.013057e-08\n",
      "L2_err=  0.00010395705848496797\n",
      "\n",
      "loss limit attained, epoch =  9  L2_err=  9.97372428361823e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.23000000000000012\n",
      " \n",
      "epoch =  0\n",
      "loss =  3.056237e-05\n",
      "eq1_loss =  9.077251e-06\n",
      "bc1_loss =  2.0370626e-05\n",
      "bc2_loss =  1.1144898e-06\n",
      "L2_err=  0.00010514905565268967\n",
      "\n",
      "loss limit attained, epoch =  29  L2_err=  9.976015945496602e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.23500000000000013\n",
      " \n",
      "epoch =  0\n",
      "loss =  2.6223426e-05\n",
      "eq1_loss =  6.028577e-06\n",
      "bc1_loss =  2.004855e-05\n",
      "bc2_loss =  1.4629956e-07\n",
      "L2_err=  0.00010682582474865511\n",
      "\n",
      "loss limit attained, epoch =  171  L2_err=  9.999497246490437e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.24000000000000013\n",
      " \n",
      "epoch =  0\n",
      "loss =  2.5440371e-05\n",
      "eq1_loss =  5.890608e-06\n",
      "bc1_loss =  1.9528345e-05\n",
      "bc2_loss =  2.1419076e-08\n",
      "L2_err=  0.00010601940735534355\n",
      "\n",
      "loss limit attained, epoch =  9  L2_err=  9.982400530909012e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.24500000000000013\n",
      " \n",
      "epoch =  0\n",
      "loss =  2.815357e-05\n",
      "eq1_loss =  8.142303e-06\n",
      "bc1_loss =  1.909511e-05\n",
      "bc2_loss =  9.16157e-07\n",
      "L2_err=  0.00010703955419647056\n",
      "\n",
      "loss limit attained, epoch =  56  L2_err=  9.991016685037219e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.2500000000000001\n",
      " \n",
      "epoch =  0\n",
      "loss =  2.4438496e-05\n",
      "eq1_loss =  5.6083236e-06\n",
      "bc1_loss =  1.8732726e-05\n",
      "bc2_loss =  9.7445614e-08\n",
      "L2_err=  0.00010783190395744772\n",
      "\n",
      "loss limit attained, epoch =  142  L2_err=  9.988139271237378e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.2550000000000001\n",
      " \n",
      "epoch =  0\n",
      "loss =  2.625859e-05\n",
      "eq1_loss =  7.851464e-06\n",
      "bc1_loss =  1.8165409e-05\n",
      "bc2_loss =  2.4171857e-07\n",
      "L2_err=  0.00010772607056676235\n",
      "\n",
      "loss limit attained, epoch =  181  L2_err=  9.988964706466425e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.2600000000000001\n",
      " \n",
      "epoch =  0\n",
      "loss =  2.3775057e-05\n",
      "eq1_loss =  5.833045e-06\n",
      "bc1_loss =  1.782206e-05\n",
      "bc2_loss =  1.1995101e-07\n",
      "L2_err=  0.00010736854033521199\n",
      "\n",
      "loss limit attained, epoch =  12  L2_err=  9.92077255505259e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.2650000000000001\n",
      " \n",
      "epoch =  0\n",
      "loss =  2.5529967e-05\n",
      "eq1_loss =  6.9860557e-06\n",
      "bc1_loss =  1.7434666e-05\n",
      "bc2_loss =  1.1092453e-06\n",
      "L2_err=  0.00010797878898770068\n",
      "\n",
      "loss limit attained, epoch =  71  L2_err=  9.99792280416907e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.27000000000000013\n",
      " \n",
      "epoch =  0\n",
      "loss =  2.2322612e-05\n",
      "eq1_loss =  5.0881945e-06\n",
      "bc1_loss =  1.7155555e-05\n",
      "bc2_loss =  7.886396e-08\n",
      "L2_err=  0.00010762729239415635\n",
      "\n",
      "loss limit attained, epoch =  201  L2_err=  9.99758117890941e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.27500000000000013\n",
      " \n",
      "epoch =  0\n",
      "loss =  2.3992301e-05\n",
      "eq1_loss =  7.099923e-06\n",
      "bc1_loss =  1.6654163e-05\n",
      "bc2_loss =  2.3821488e-07\n",
      "L2_err=  0.00010770739736554878\n",
      "\n",
      "loss limit attained, epoch =  12  L2_err=  9.993550794385557e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.28000000000000014\n",
      " \n",
      "epoch =  0\n",
      "loss =  2.2442826e-05\n",
      "eq1_loss =  5.424415e-06\n",
      "bc1_loss =  1.6420987e-05\n",
      "bc2_loss =  5.974228e-07\n",
      "L2_err=  0.00010702144717935009\n",
      "\n",
      "loss limit attained, epoch =  225  L2_err=  9.98975617026701e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.28500000000000014\n",
      " \n",
      "epoch =  0\n",
      "loss =  2.1353384e-05\n",
      "eq1_loss =  5.1407733e-06\n",
      "bc1_loss =  1.6048261e-05\n",
      "bc2_loss =  1.6434996e-07\n",
      "L2_err=  0.00010768681112092059\n",
      "\n",
      "loss limit attained, epoch =  9  L2_err=  9.952102826797604e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.29000000000000015\n",
      " \n",
      "epoch =  0\n",
      "loss =  2.1269176e-05\n",
      "eq1_loss =  4.778596e-06\n",
      "bc1_loss =  1.5869678e-05\n",
      "bc2_loss =  6.209018e-07\n",
      "L2_err=  0.00010653256989917828\n",
      "\n",
      "loss limit attained, epoch =  7  L2_err=  9.935162949526312e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.29500000000000015\n",
      " \n",
      "epoch =  0\n",
      "loss =  2.1786762e-05\n",
      "eq1_loss =  5.019728e-06\n",
      "bc1_loss =  1.5621337e-05\n",
      "bc2_loss =  1.1456951e-06\n",
      "L2_err=  0.00010901841634798733\n",
      "\n",
      "loss limit attained, epoch =  209  L2_err=  9.999609059012443e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.30000000000000016\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.985936e-05\n",
      "eq1_loss =  4.506761e-06\n",
      "bc1_loss =  1.5279751e-05\n",
      "bc2_loss =  7.2848565e-08\n",
      "L2_err=  0.00010874525354615391\n",
      "\n",
      "loss limit attained, epoch =  9  L2_err=  9.92585948751e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.30500000000000016\n",
      " \n",
      "epoch =  0\n",
      "loss =  2.0507789e-05\n",
      "eq1_loss =  4.726442e-06\n",
      "bc1_loss =  1.5049037e-05\n",
      "bc2_loss =  7.323113e-07\n",
      "L2_err=  0.00010810651568141793\n",
      "\n",
      "loss limit attained, epoch =  15  L2_err=  9.970421673160371e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.31000000000000016\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.9604828e-05\n",
      "eq1_loss =  4.4099047e-06\n",
      "bc1_loss =  1.4758187e-05\n",
      "bc2_loss =  4.3673566e-07\n",
      "L2_err=  0.00010940480323860166\n",
      "\n",
      "loss limit attained, epoch =  152  L2_err=  9.999214893588615e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.31500000000000017\n",
      " \n",
      "epoch =  0\n",
      "loss =  2.2077173e-05\n",
      "eq1_loss =  7.261584e-06\n",
      "bc1_loss =  1.4320016e-05\n",
      "bc2_loss =  4.9557207e-07\n",
      "L2_err=  0.00010976425807889489\n",
      "\n",
      "loss limit attained, epoch =  172  L2_err=  9.95106566790052e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.3200000000000002\n",
      " \n",
      "epoch =  0\n",
      "loss =  2.2554903e-05\n",
      "eq1_loss =  7.809478e-06\n",
      "bc1_loss =  1.4044374e-05\n",
      "bc2_loss =  7.0105096e-07\n",
      "L2_err=  0.00011045193838135009\n",
      "\n",
      "loss limit attained, epoch =  15  L2_err=  9.95652662058638e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.3250000000000002\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.9503253e-05\n",
      "eq1_loss =  4.991566e-06\n",
      "bc1_loss =  1.3901334e-05\n",
      "bc2_loss =  6.103534e-07\n",
      "L2_err=  0.00010960941259676922\n",
      "\n",
      "loss limit attained, epoch =  255  L2_err=  9.974587926569521e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.3300000000000002\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.8408224e-05\n",
      "eq1_loss =  4.590896e-06\n",
      "bc1_loss =  1.3614814e-05\n",
      "bc2_loss =  2.0251353e-07\n",
      "L2_err=  0.00010910401591850105\n",
      "\n",
      "loss limit attained, epoch =  10  L2_err=  9.967730194181915e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.3350000000000002\n",
      " \n",
      "epoch =  0\n",
      "loss =  2.2233975e-05\n",
      "eq1_loss =  8.5815545e-06\n",
      "bc1_loss =  1.3326486e-05\n",
      "bc2_loss =  3.2593613e-07\n",
      "L2_err=  0.00011093800643524417\n",
      "\n",
      "loss limit attained, epoch =  4  L2_err=  9.930231650956832e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.3400000000000002\n",
      " \n",
      "epoch =  0\n",
      "loss =  2.03221e-05\n",
      "eq1_loss =  5.193869e-06\n",
      "bc1_loss =  1.3342157e-05\n",
      "bc2_loss =  1.7860754e-06\n",
      "L2_err=  0.00011147440017894441\n",
      "\n",
      "loss limit attained, epoch =  236  L2_err=  9.99810655099053e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.3450000000000002\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.7105926e-05\n",
      "eq1_loss =  3.8536436e-06\n",
      "bc1_loss =  1.31404495e-05\n",
      "bc2_loss =  1.1183129e-07\n",
      "L2_err=  0.00011105683095309812\n",
      "\n",
      "loss limit attained, epoch =  12  L2_err=  9.889409109894019e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.3500000000000002\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.7262335e-05\n",
      "eq1_loss =  3.8528096e-06\n",
      "bc1_loss =  1.2951e-05\n",
      "bc2_loss =  4.585255e-07\n",
      "L2_err=  0.00010929961430961168\n",
      "\n",
      "loss limit attained, epoch =  10  L2_err=  9.958615213484235e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.3550000000000002\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.6705691e-05\n",
      "eq1_loss =  3.728957e-06\n",
      "bc1_loss =  1.2775707e-05\n",
      "bc2_loss =  2.010276e-07\n",
      "L2_err=  0.00011022854838545832\n",
      "\n",
      "loss limit attained, epoch =  10  L2_err=  9.980915354298457e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.3600000000000002\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.6450098e-05\n",
      "eq1_loss =  3.6743086e-06\n",
      "bc1_loss =  1.2589334e-05\n",
      "bc2_loss =  1.8645625e-07\n",
      "L2_err=  0.00011034314038659269\n",
      "\n",
      "loss limit attained, epoch =  10  L2_err=  9.958024459624265e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.3650000000000002\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.636938e-05\n",
      "eq1_loss =  3.6350489e-06\n",
      "bc1_loss =  1.2403914e-05\n",
      "bc2_loss =  3.304167e-07\n",
      "L2_err=  0.00010983471074563217\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss limit attained, epoch =  13  L2_err=  9.990244925108961e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.3700000000000002\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.6073498e-05\n",
      "eq1_loss =  3.5806556e-06\n",
      "bc1_loss =  1.2190718e-05\n",
      "bc2_loss =  3.0212507e-07\n",
      "L2_err=  0.00010979266190843917\n",
      "\n",
      "loss limit attained, epoch =  334  L2_err=  9.968991612180525e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.3750000000000002\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.6737562e-05\n",
      "eq1_loss =  4.6843375e-06\n",
      "bc1_loss =  1.1834575e-05\n",
      "bc2_loss =  2.1864983e-07\n",
      "L2_err=  0.0001098937399607417\n",
      "\n",
      "loss limit attained, epoch =  12  L2_err=  9.849900285112864e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.3800000000000002\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.6546237e-05\n",
      "eq1_loss =  4.216377e-06\n",
      "bc1_loss =  1.1702483e-05\n",
      "bc2_loss =  6.273765e-07\n",
      "L2_err=  0.00010902990267238228\n",
      "\n",
      "loss limit attained, epoch =  9  L2_err=  9.992570894235835e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.38500000000000023\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.5518362e-05\n",
      "eq1_loss =  3.5806654e-06\n",
      "bc1_loss =  1.1589796e-05\n",
      "bc2_loss =  3.479025e-07\n",
      "L2_err=  0.00010986684964398635\n",
      "\n",
      "loss limit attained, epoch =  224  L2_err=  9.967459297398484e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.39000000000000024\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.755332e-05\n",
      "eq1_loss =  5.642e-06\n",
      "bc1_loss =  1.1285417e-05\n",
      "bc2_loss =  6.259034e-07\n",
      "L2_err=  0.00011188390241323648\n",
      "\n",
      "loss limit attained, epoch =  17  L2_err=  9.997697912375307e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.39500000000000024\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.5360105e-05\n",
      "eq1_loss =  3.7175282e-06\n",
      "bc1_loss =  1.1195889e-05\n",
      "bc2_loss =  4.466874e-07\n",
      "L2_err=  0.00011103094970654156\n",
      "\n",
      "loss limit attained, epoch =  198  L2_err=  9.950563582146955e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.40000000000000024\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.7976949e-05\n",
      "eq1_loss =  6.198853e-06\n",
      "bc1_loss =  1.0929054e-05\n",
      "bc2_loss =  8.490417e-07\n",
      "L2_err=  0.0001120791242927948\n",
      "\n",
      "loss limit attained, epoch =  179  L2_err=  9.963935182143143e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.40500000000000025\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.690376e-05\n",
      "eq1_loss =  5.334677e-06\n",
      "bc1_loss =  1.0795082e-05\n",
      "bc2_loss =  7.739998e-07\n",
      "L2_err=  0.00011302721173422748\n",
      "\n",
      "loss limit attained, epoch =  15  L2_err=  9.978135259314107e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.41000000000000025\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.544497e-05\n",
      "eq1_loss =  4.180017e-06\n",
      "bc1_loss =  1.0712987e-05\n",
      "bc2_loss =  5.519652e-07\n",
      "L2_err=  0.0001119795701126283\n",
      "\n",
      "loss limit attained, epoch =  245  L2_err=  9.992164706110426e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.41500000000000026\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.5230124e-05\n",
      "eq1_loss =  4.3570753e-06\n",
      "bc1_loss =  1.0528065e-05\n",
      "bc2_loss =  3.4498285e-07\n",
      "L2_err=  0.00011163185283673452\n",
      "\n",
      "loss limit attained, epoch =  12  L2_err=  9.957881185805852e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.42000000000000026\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.4026414e-05\n",
      "eq1_loss =  3.131612e-06\n",
      "bc1_loss =  1.0471668e-05\n",
      "bc2_loss =  4.231332e-07\n",
      "L2_err=  0.00010928606077323243\n",
      "\n",
      "loss limit attained, epoch =  12  L2_err=  9.892770235595775e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.42500000000000027\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.4018843e-05\n",
      "eq1_loss =  3.104947e-06\n",
      "bc1_loss =  1.0351657e-05\n",
      "bc2_loss =  5.622395e-07\n",
      "L2_err=  0.00010980664965586927\n",
      "\n",
      "loss limit attained, epoch =  256  L2_err=  9.958720232423749e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.43000000000000027\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.3738866e-05\n",
      "eq1_loss =  3.3881022e-06\n",
      "bc1_loss =  1.0154314e-05\n",
      "bc2_loss =  1.9644992e-07\n",
      "L2_err=  0.00011057100996053433\n",
      "\n",
      "loss limit attained, epoch =  12  L2_err=  9.790562191497387e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.4350000000000003\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.4833874e-05\n",
      "eq1_loss =  4.141993e-06\n",
      "bc1_loss =  1.0021794e-05\n",
      "bc2_loss =  6.700871e-07\n",
      "L2_err=  0.00010978322529979876\n",
      "\n",
      "loss limit attained, epoch =  9  L2_err=  9.789786918179437e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.4400000000000003\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.40019165e-05\n",
      "eq1_loss =  3.0740434e-06\n",
      "bc1_loss =  9.987106e-06\n",
      "bc2_loss =  9.4076665e-07\n",
      "L2_err=  0.00010924068096046768\n",
      "\n",
      "loss limit attained, epoch =  229  L2_err=  9.995052911361842e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.4450000000000003\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.2782746e-05\n",
      "eq1_loss =  2.87738e-06\n",
      "bc1_loss =  9.830627e-06\n",
      "bc2_loss =  7.473844e-08\n",
      "L2_err=  0.00011067787336523674\n",
      "\n",
      "loss limit attained, epoch =  9  L2_err=  9.949724759915987e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.4500000000000003\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.3615895e-05\n",
      "eq1_loss =  3.4577592e-06\n",
      "bc1_loss =  9.710272e-06\n",
      "bc2_loss =  4.4786333e-07\n",
      "L2_err=  0.00011071337869668617\n",
      "\n",
      "loss limit attained, epoch =  9  L2_err=  9.964460111552075e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.4550000000000003\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.2844708e-05\n",
      "eq1_loss =  2.8134757e-06\n",
      "bc1_loss =  9.680204e-06\n",
      "bc2_loss =  3.5102872e-07\n",
      "L2_err=  0.00011123263740798323\n",
      "\n",
      "loss limit attained, epoch =  12  L2_err=  9.966442995822886e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.4600000000000003\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.2642208e-05\n",
      "eq1_loss =  2.7994806e-06\n",
      "bc1_loss =  9.562619e-06\n",
      "bc2_loss =  2.8010848e-07\n",
      "L2_err=  0.00011020894451689288\n",
      "\n",
      "loss limit attained, epoch =  322  L2_err=  9.991801523822502e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.4650000000000003\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.2169157e-05\n",
      "eq1_loss =  2.752381e-06\n",
      "bc1_loss =  9.374079e-06\n",
      "bc2_loss =  4.2697096e-08\n",
      "L2_err=  0.00010988982507900881\n",
      "\n",
      "loss limit attained, epoch =  9  L2_err=  9.926180782532422e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.4700000000000003\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.395988e-05\n",
      "eq1_loss =  4.261527e-06\n",
      "bc1_loss =  9.2369855e-06\n",
      "bc2_loss =  4.613663e-07\n",
      "L2_err=  0.00011040996648524156\n",
      "\n",
      "loss limit attained, epoch =  9  L2_err=  9.975817914165064e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.4750000000000003\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.2172173e-05\n",
      "eq1_loss =  2.6687949e-06\n",
      "bc1_loss =  9.2304645e-06\n",
      "bc2_loss =  2.7291375e-07\n",
      "L2_err=  0.00011208022594033222\n",
      "\n",
      "loss limit attained, epoch =  15  L2_err=  9.974111112045039e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.4800000000000003\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.1976307e-05\n",
      "eq1_loss =  2.6594425e-06\n",
      "bc1_loss =  9.110254e-06\n",
      "bc2_loss =  2.0661115e-07\n",
      "L2_err=  0.00011107062553419078\n",
      "\n",
      "loss limit attained, epoch =  209  L2_err=  9.986009263705885e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.4850000000000003\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.416808e-05\n",
      "eq1_loss =  4.7950375e-06\n",
      "bc1_loss =  8.881785e-06\n",
      "bc2_loss =  4.9125737e-07\n",
      "L2_err=  0.00011189538323664289\n",
      "\n",
      "loss limit attained, epoch =  312  L2_err=  9.997529213336081e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.4900000000000003\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.1473644e-05\n",
      "eq1_loss =  2.5819274e-06\n",
      "bc1_loss =  8.831762e-06\n",
      "bc2_loss =  5.99544e-08\n",
      "L2_err=  0.00011045089320627229\n",
      "\n",
      "loss limit attained, epoch =  11  L2_err=  9.887008835456645e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.49500000000000033\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.1818095e-05\n",
      "eq1_loss =  2.703458e-06\n",
      "bc1_loss =  8.7804665e-06\n",
      "bc2_loss =  3.3417135e-07\n",
      "L2_err=  0.00010825498323615184\n",
      "\n",
      "loss limit attained, epoch =  9  L2_err=  9.87216657728057e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.5000000000000003\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.1870628e-05\n",
      "eq1_loss =  2.6511434e-06\n",
      "bc1_loss =  8.689919e-06\n",
      "bc2_loss =  5.2956625e-07\n",
      "L2_err=  0.00010936698213002491\n",
      "\n",
      "loss limit attained, epoch =  15  L2_err=  9.985313030572227e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "t =  0.5050000000000003\n",
      " \n",
      "epoch =  0\n",
      "loss =  1.1234615e-05\n",
      "eq1_loss =  2.4713534e-06\n",
      "bc1_loss =  8.618378e-06\n",
      "bc2_loss =  1.4488317e-07\n",
      "L2_err=  0.00011072873127983713\n",
      "\n",
      "loss limit attained, epoch =  180  L2_err=  9.978032820684516e-05\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "time elapsed =  396.9083194732666\n"
     ]
    }
   ],
   "source": [
    "N_x = 51\n",
    "N_bc = 35\n",
    "N_t = 101\n",
    "del_t = 0.005\n",
    "x_l = 0\n",
    "x_r = 1\n",
    "T_r = 0\n",
    "T_l = 1\n",
    "t_i = 0\n",
    "accuracy_cap = 0.0001\n",
    "N_x_test = 101\n",
    "\n",
    "# Neural network params\n",
    "layer_size = [1, 25, 25, 25, 1]\n",
    "\n",
    "# material params\n",
    "k1 = 0.05\n",
    "\n",
    "# Training data and initial data\n",
    "model = ANN(layer_size)\n",
    "print(model)\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total trainable parameters in the model:\", total_trainable_params)\n",
    "\n",
    "# # Setup Loss function and Optimiser\n",
    "lr = 7e-5\n",
    "epochs = 20000\n",
    "optimiser1 = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training model\n",
    "start = time.time()\n",
    "loss_store, T_store_pred, T_store_an, x_test_np = train_model(model, optimiser1, epochs, T_r, T_l, k1, N_x, x_l, x_r, N_t, N_bc, accuracy_cap, N_x_test)\n",
    "end = time.time()\n",
    "time_elapsed = end - start\n",
    "print(\"time elapsed = \", time_elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890d895a",
   "metadata": {},
   "source": [
    "# Results Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6502e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = 0\n",
    "plt.plot(x_test_np, T_store_pred[j])\n",
    "plt.plot(x_test_np, T_store_an[j])\n",
    "Title = \"Transient Heat Conduction using Time stepping, \" + \"time = \" + str(j*del_t) + \", delta_t = \" + str(del_t)\n",
    "plt.title(Title)\n",
    "plt.legend([\"PINN\", \"Analytical\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a47df8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
