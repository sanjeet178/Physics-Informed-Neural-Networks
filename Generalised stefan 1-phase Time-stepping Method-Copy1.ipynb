{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f34de8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.init as init\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f0ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_torch(arr):\n",
    "    \n",
    "    arr = torch.FloatTensor(arr)\n",
    "    arr = arr.unsqueeze(-1)\n",
    "    arr = arr.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def x_train_data(N_tot, x_l, x_r):\n",
    "    \n",
    "    x_train = np.linspace(x_l,x_r,N_tot)\n",
    "#     x_train = np.random.uniform(low=x_l+0.000001, high=x_r, size=N_x-2)\n",
    "#     x_train = np.concatenate((np.ones(1)*x_l,x_train,np.ones(1)*x_r), axis=0)\n",
    "#     x_train = np.sort(x_train)\n",
    "    x_train = np_to_torch(x_train)\n",
    "    return x_train\n",
    "\n",
    "def initial_temp(N_tot, T_l, T_r):\n",
    "    \n",
    "    T_prev = np.ones(N_tot)*T_r\n",
    "    T_prev[0] = T_l\n",
    "    T_prev = np_to_torch(T_prev)\n",
    "    \n",
    "    return T_prev\n",
    "\n",
    "def initial_fraction(N_tot, s_initial, x_train):\n",
    "    \n",
    "    x_train = x_train.detach().numpy()\n",
    "    f_prev = np.zeros(N_tot)\n",
    "    for i in range(N_tot):\n",
    "        if x_train[i]<=s_initial:\n",
    "            f_prev[i] = 1\n",
    "    f_prev = np_to_torch(f_prev)\n",
    "    \n",
    "    return f_prev\n",
    "            \n",
    "def xavier_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0.05)\n",
    "    \n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, layer_size):\n",
    "        super(ANN, self).__init__()\n",
    "        \n",
    "        # Fully conected model-1\n",
    "        modules_1 = []\n",
    "        for i in range(len(layer_size) - 1):\n",
    "            modules_1.append(nn.Linear(layer_size[i], layer_size[i+1]))  \n",
    "            if i!=len(layer_size) - 2:\n",
    "                modules_1.append(nn.Tanh())\n",
    "                \n",
    "        self.fc_1 = nn.Sequential(*modules_1)\n",
    "        self.fc_1.apply(xavier_init)\n",
    "        \n",
    "        # Fully conected model-2\n",
    "        modules_2 = []\n",
    "        for i in range(len(layer_size) - 1):\n",
    "            modules_2.append(nn.Linear(layer_size[i], layer_size[i+1]))  \n",
    "            if i != len(layer_size) - 2 :\n",
    "                modules_2.append(nn.Tanh())\n",
    "                \n",
    "        self.fc_2 = nn.Sequential(*modules_2)\n",
    "        self.fc_2.apply(xavier_init)\n",
    "        \n",
    "    def forward(self, x_train):\n",
    "        \n",
    "        T = self.fc_1( x_train )\n",
    "        dTdx = torch.autograd.grad(T, x_train, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "        d2Tdx2 = torch.autograd.grad(dTdx, x_train, grad_outputs=torch.ones_like(dTdx), create_graph=True)[0]\n",
    "        \n",
    "        f = self.fc_2( x_train )\n",
    "        dfdx = torch.autograd.grad(f, x_train, grad_outputs=torch.ones_like(f), create_graph=True)[0]\n",
    "        df2dx2 = torch.autograd.grad(dfdx, x_train, grad_outputs=torch.ones_like(dfdx), create_graph=True)[0]\n",
    "        \n",
    "        return T, dTdx, d2Tdx2, f\n",
    "    \n",
    "def get_loss(x_train, k1, k2, T_l, T_r, x_l, x_r, t_i, w1, w2, w3, w4, w5, T_prev, f_prev, f_lim, N_x, mat_1, mat_2, N_1, N_2):\n",
    "\n",
    "    T, dTdx, d2Tdx2, f = model(x_train)\n",
    "\n",
    "    mat_1 = torch.where(f >= f_lim,1,0)\n",
    "    N_1 = torch.sum( mat_1 )\n",
    "    mat_2 = torch.where(f < f_lim,1,0)\n",
    "    N_2 = torch.sum( mat_2 )\n",
    "    \n",
    "    eq1 = w1*torch.sum( torch.square( torch.mul(mat_1, T - T_prev - del_t*k1*d2Tdx2) ) )/(N_1)\n",
    "    eq2 = w2*torch.sum( torch.square( torch.mul(mat_2, f - f_prev - del_t*k2*d2Tdx2 ) ) )/(N_2)\n",
    "    bc1 = w3*torch.sum( torch.square( torch.mul(torch.where(x_train == 0,1,0), T - T_l ) ) )\n",
    "    bc2 = w4*torch.sum( torch.square( torch.mul(mat_2, T - T_r  ) ) )/(N_2)  \n",
    "    bc3 = w5*torch.sum( torch.square( torch.mul(mat_2, dTdx  ) ) )/(N_2) \n",
    "    \n",
    "    if (N_1 == 0):\n",
    "        eq1 = 0\n",
    "    if (N_2 == 0):\n",
    "        eq2 = 0\n",
    "        bc3 = 0\n",
    "\n",
    "    loss = eq1 + eq2 + bc1 + bc2 \n",
    "    \n",
    "    return loss, eq1, eq2, bc1, bc2, bc3\n",
    "\n",
    "def print_loss(epoch, loss, eq1, eq2, bc1, bc2, bc3):\n",
    "    print('epoch = ',epoch)\n",
    "    print('loss = ',loss)\n",
    "    print('eq1_loss = ',eq1)\n",
    "    print('eq2_loss = ',eq2)\n",
    "    print('bc1_loss = ',bc1)\n",
    "    print('bc2_loss = ',bc2)\n",
    "#     print('bc3_loss = ',bc3)\n",
    "    \n",
    "def lambda_analytical(k1, k2):\n",
    "    x = []\n",
    "    er = []\n",
    "    cnt = 0\n",
    "    for i in np.arange(0.1, 5, 0.001):\n",
    "        x.append(i)\n",
    "        er.append(math.erf(x[-1]))\n",
    "        cnt = cnt+1\n",
    "\n",
    "    x = np.array(x)\n",
    "    er = np.array(er)\n",
    "    y =[]\n",
    "    y = np.exp(-x*x)/(er*math.sqrt(math.pi))-x*k1/(k2)\n",
    "\n",
    "    for i in range(1,cnt):\n",
    "        if(y[i]*y[i-1]<0):\n",
    "            lam = x[i]\n",
    "            break\n",
    "    \n",
    "    return lam\n",
    "\n",
    "def analytical(N_x_test, x_test, t_test, T_r, k1, k2, T_l):\n",
    "\n",
    "    x_test = x_test.detach().numpy()\n",
    "    y_an = np.zeros((N_x_test, 1))\n",
    "    lam = lambda_analytical(k1, k2)\n",
    "    s = np.sqrt(k1*t_test)*2*lam\n",
    "    \n",
    "    for j in range(N_x_test):\n",
    "        if(x_test[j]<s):\n",
    "            y_an[j] = T_l - T_l*math.erf( x_test[j]/( 2*np.sqrt(k1*t_test) ) )/ math.erf(lam) \n",
    "        else:\n",
    "            y_an[j] = T_r\n",
    "            \n",
    "    y_an = np.reshape(y_an, (N_x_test, 1))\n",
    "    \n",
    "    return y_an, s\n",
    "    \n",
    "def temperature_fraction_correction(f_new, f_prev, T_new, N_x, T_r):\n",
    "    \n",
    "    for i in range(N_x):\n",
    "        if f_new[i][0]>=1 or f_prev[i][0]>=1:\n",
    "            f_new[i][0] = 1\n",
    "            continue\n",
    "            \n",
    "        if f_new[i][0]<0:\n",
    "            f_new[i][0] = 0\n",
    "            \n",
    "#         if f_new[i][0]<1 :\n",
    "#             T_new[i][0] = T_r\n",
    "    \n",
    "    f_new = torch.FloatTensor(f_new)  \n",
    "    T_new = torch.FloatTensor(T_new)   \n",
    "    return T_new, f_new\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6370e2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN(\n",
      "  (fc_1): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=3, bias=True)\n",
      "    (1): Softsign()\n",
      "    (2): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (3): Softsign()\n",
      "    (4): Linear(in_features=3, out_features=1, bias=True)\n",
      "  )\n",
      "  (fc_2): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=3, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=3, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Total trainable parameters in the model: 44\n"
     ]
    }
   ],
   "source": [
    "N_x = 400\n",
    "N_t = 40\n",
    "\n",
    "x_l = 0\n",
    "x_r = 0.15\n",
    "\n",
    "T_r = 0\n",
    "T_l = 1\n",
    "\n",
    "t_i = 0\n",
    "t_f = 0.2\n",
    "\n",
    "accuracy_cap = 0.0004\n",
    "del_t = 0.001\n",
    "s_initial = 0.01\n",
    "f_lim = 1\n",
    "\n",
    "# Neural network params\n",
    "layer_size = [1, 3, 3, 1]\n",
    "\n",
    "# material params\n",
    "k1 = 0.05\n",
    "k2 = 0.8\n",
    "\n",
    "# Training data and initial data\n",
    "model = ANN(layer_size)\n",
    "print(model)\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total trainable parameters in the model:\", total_trainable_params)\n",
    "\n",
    "# # Setup Loss function and Optimiser\n",
    "lr = 3e-4\n",
    "epochs = 50001\n",
    "optimiser1 = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a424493",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t =  0.001\n",
      " \n",
      "epoch =  0\n",
      "loss =  tensor(0.9611, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  0\n",
      "eq2_loss =  tensor(0.0648, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.8951, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0011, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  2000\n",
      "loss =  tensor(0.2539, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  0\n",
      "eq2_loss =  tensor(0.0497, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0627, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.1415, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  4000\n",
      "loss =  tensor(0.2146, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  0\n",
      "eq2_loss =  tensor(0.0462, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0281, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.1402, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  6000\n",
      "loss =  tensor(0.1903, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  0\n",
      "eq2_loss =  tensor(0.0413, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0210, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.1279, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  8000\n",
      "loss =  tensor(0.1397, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  0\n",
      "eq2_loss =  tensor(0.0215, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0157, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.1025, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  10000\n",
      "loss =  tensor(0.1260, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  0\n",
      "eq2_loss =  tensor(0.0269, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0132, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0859, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  12000\n",
      "loss =  tensor(0.1019, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  0\n",
      "eq2_loss =  tensor(0.0244, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0110, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0665, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  14000\n",
      "loss =  tensor(0.0917, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  0\n",
      "eq2_loss =  tensor(0.0207, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0084, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0627, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  16000\n",
      "loss =  tensor(0.2025, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.1060, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0063, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0820, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  18000\n",
      "loss =  tensor(0.1053, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0804, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0061, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0182, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0005, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  20000\n",
      "loss =  tensor(0.1042, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0829, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0049, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0156, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0008, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  22000\n",
      "loss =  tensor(0.1226, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0982, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0028, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0208, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0008, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  24000\n",
      "loss =  tensor(0.1276, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.1004, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0024, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0238, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0009, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  26000\n",
      "loss =  tensor(0.1283, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0984, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0036, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0253, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0010, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  28000\n",
      "loss =  tensor(0.1305, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.1015, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0040, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0239, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0011, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  30000\n",
      "loss =  tensor(0.1294, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0987, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0042, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0255, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0010, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  32000\n",
      "loss =  tensor(0.1309, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0990, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0045, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0263, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0010, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  34000\n",
      "loss =  tensor(0.1292, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.1004, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0033, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0245, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0011, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  36000\n",
      "loss =  tensor(0.1251, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0965, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0036, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0239, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0012, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  38000\n",
      "loss =  tensor(0.1290, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.1003, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0039, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0235, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0013, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  40000\n",
      "loss =  tensor(0.1303, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0992, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0061, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0238, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0013, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  42000\n",
      "loss =  tensor(0.1279, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0981, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0041, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0243, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0013, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  44000\n",
      "loss =  tensor(0.1320, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0972, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0269, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0012, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  46000\n",
      "loss =  tensor(0.1319, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0977, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0068, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0261, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0013, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  48000\n",
      "loss =  tensor(0.1293, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.1009, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0043, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0227, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0014, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  50000\n",
      "loss =  tensor(0.1247, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0982, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0018, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0234, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0013, grad_fn=<DivBackward0>)\n",
      "\n",
      "t =  0.002\n",
      " \n",
      "epoch =  0\n",
      "loss =  tensor(0.1036, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0204, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0581, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0237, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0014, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  2000\n",
      "loss =  tensor(0.0083, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0059, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0009, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0011, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0005, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  4000\n",
      "loss =  tensor(0.0063, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0045, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0007, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0005, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  6000\n",
      "loss =  tensor(0.0058, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0041, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0006, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0005, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  8000\n",
      "loss =  tensor(0.0058, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0041, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0006, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0006, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  10000\n",
      "loss =  tensor(0.0069, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0043, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0015, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0005, grad_fn=<DivBackward0>)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10872/748378505.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m#Backpropogation and optimisation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         loss, eq1, eq2, bc1, bc2, bc3 = get_loss(x_train, k1, k2, T_l, T_r, x_l, x_r, t_i, \n\u001b[0m\u001b[0;32m     42\u001b[0m                                             w1, w2, w3, w4, w5, T_prev, f_prev, f_lim, N_x, mat_1, mat_2, N_1, N_2)\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10872/4065991992.py\u001b[0m in \u001b[0;36mget_loss\u001b[1;34m(x_train, k1, k2, T_l, T_r, x_l, x_r, t_i, w1, w2, w3, w4, w5, T_prev, f_prev, f_lim, N_x, mat_1, mat_2, N_1, N_2)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf_lim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmat_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmat_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m     \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdTdx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md2Tdx2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0mmat_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mf_lim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10872/4065991992.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x_train)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc_1\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mx_train\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[0mdTdx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0md2Tdx2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdTdx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdTdx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_store = []\n",
    "T_store_an = []\n",
    "T_store_pred = []\n",
    "f_store_pred = []\n",
    "t_store = []\n",
    "s_store_an = []\n",
    "model.train()  \n",
    "\n",
    "# Initial conditions\n",
    "x_train = x_train_data(N_x, x_l, x_r)\n",
    "T_prev = initial_temp(N_x, t_i, t_f)\n",
    "f_prev = initial_fraction(N_x, s_initial, x_train)\n",
    "t_test = 0\n",
    "T_store_pred.append(T_prev)\n",
    "f_store_pred.append(f_prev)\n",
    "\n",
    "# Loss function weights\n",
    "w1 = 1\n",
    "w2 = 1\n",
    "w3 = 1\n",
    "w4 = 1\n",
    "w5 = 1\n",
    "\n",
    "for i in range(N_t):\n",
    "\n",
    "    t_test = t_test + del_t\n",
    "    t_store.append(t_test)\n",
    "    print(\"t = \", t_test)\n",
    "    print(\" \")\n",
    "\n",
    "    mat_1 = torch.where(f_prev >= f_lim,1,0)\n",
    "    N_1 = torch.sum( mat_1 )\n",
    "    mat_2 = torch.where(f_prev < f_lim,1,0)\n",
    "    N_2 = torch.sum( mat_2 )\n",
    "    \n",
    "    if(i>1):\n",
    "        epochs = 2001\n",
    "\n",
    "    for epoch in range(epochs):        \n",
    "        #Backpropogation and optimisation\n",
    "        loss, eq1, eq2, bc1, bc2, bc3 = get_loss(x_train, k1, k2, T_l, T_r, x_l, x_r, t_i, \n",
    "                                            w1, w2, w3, w4, w5, T_prev, f_prev, f_lim, N_x, mat_1, mat_2, N_1, N_2)\n",
    "        \n",
    "        optimiser1.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser1.step()  \n",
    "        loss_store.append(loss.detach().numpy())\n",
    "\n",
    "        if epoch%2000==0:\n",
    "            print_loss(epoch, loss, eq1, eq2, bc1, bc2, bc3)\n",
    "            print(\"\")\n",
    "\n",
    "#             if loss.detach().numpy()<8e-5:\n",
    "#                 break\n",
    "\n",
    "    # Store the results after each time step\n",
    "    T_prev, dTdx, d2Tdx2, f_prev = model(x_train)\n",
    "    T_an, s_an = analytical(N_x, x_train, t_test, T_r, k1, k2, T_l)\n",
    "\n",
    "    T_prev, f_prev = temperature_fraction_correction(f_prev.detach().numpy(), f_store_pred[-1], T_prev.detach().numpy(), N_x, T_r)\n",
    "\n",
    "    T_store_pred.append(T_prev.detach().numpy())\n",
    "    T_store_an.append(T_an)\n",
    "    s_store_an.append(s_an)\n",
    "    f_store_pred.append(f_prev.detach().numpy())\n",
    "\n",
    "    T_prev = torch.FloatTensor(T_store_pred[-1]).clone().detach().requires_grad_(False)\n",
    "    f_prev = torch.FloatTensor(f_store_pred[-1]).clone().detach().requires_grad_(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "657c15a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 0\n",
    "j = N_x\n",
    "k = 0\n",
    "plt.plot(x_train[i:j].detach().numpy(), T_store_pred[k+1][i:j])\n",
    "plt.plot(x_train[i:j].detach().numpy(), T_store_an[k][i:j])\n",
    "plt.plot(x_train[i:j].detach().numpy(), f_store_pred[k+1][i:j])\n",
    "Title = \"Time = \" + str( \"{:.3f}\".format (t_store[k]))\n",
    "plt.title(Title)\n",
    "plt.legend([\"PINN\", \"Analytical\", \"Fraction\"])\n",
    "plt.ylim(-0.05, 1.055)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daac0ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_store_pred[1][i:j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8863e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Title = \"Time = \" + str( \"{:.3f}\".format (t_store[k]))\n",
    "plt.plot(x_test_np[i:j], f_store_pred[k][i:j])\n",
    "plt.title(Title)\n",
    "plt.xlabel(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
