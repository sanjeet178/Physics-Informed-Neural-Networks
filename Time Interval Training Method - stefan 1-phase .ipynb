{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4ac27d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.init as init\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8819113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_torch(arr):\n",
    "    \n",
    "    arr = torch.FloatTensor(arr)\n",
    "    arr = arr.unsqueeze(-1)\n",
    "    arr = arr.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def x_train_data(N_x, x_l, x_r, N_bc, N_ic, N_t, null):\n",
    "\n",
    "    mse = nn.MSELoss(reduction='sum')\n",
    "    x_train = np.geomspace(x_l+0.001, x_r, N_x)\n",
    "    x_train = np.tile(x_train, N_t)\n",
    "    x_bc1 = np.zeros(N_bc)\n",
    "    x_bc2 = np.ones(N_bc)*x_r\n",
    "    x_ic = np.random.uniform(low=x_l, high=x_r, size=(N_ic,))\n",
    "    x_train = np.concatenate((x_train,x_bc1,x_bc2,x_ic),0)\n",
    "    x_train = np_to_torch(x_train)\n",
    "    N_xl = mse( torch.where(x_train == x_l,1,0), null ).detach().numpy().item()\n",
    "    N_xr = mse( torch.where(x_train == x_r,1,0), null ).detach().numpy().item()\n",
    "    \n",
    "    return x_train, N_xl, N_xr\n",
    "\n",
    "def t_train_data(t_i,t_f,N_t,N_x,N_bc,N_ic):\n",
    "    \n",
    "    t_train = np.linspace(t_i, t_f, N_t)\n",
    "    t_train = np.repeat(t_train, N_x)\n",
    "    t_bc1 = np.random.uniform(low=t_i, high=t_f, size=(N_bc,))\n",
    "    t_bc2 = np.random.uniform(low=t_i, high=t_f, size=(N_bc,))\n",
    "    t_ic = np.zeros(N_ic)\n",
    "    t_train = np.concatenate((t_train,t_bc1,t_bc2,t_ic),0)\n",
    "    t_train = np_to_torch(t_train)\n",
    "\n",
    "    return t_train\n",
    "            \n",
    "def xavier_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0.05)\n",
    "    \n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, layer_size_1, layer_size_2):\n",
    "        super(ANN, self).__init__()\n",
    "        \n",
    "        # Fully conected model\n",
    "        modules_1 = []\n",
    "        for i in range(len(layer_size_1) - 1):\n",
    "            modules_1.append(nn.Linear(layer_size_1[i], layer_size_1[i+1]))  \n",
    "            modules_1.append(nn.Tanh())\n",
    "            \n",
    "        modules_2 = []\n",
    "        for i in range(len(layer_size_2) - 1):\n",
    "            modules_2.append(nn.Linear(layer_size_2[i], layer_size_2[i+1]))  \n",
    "            modules_2.append(nn.Tanh())\n",
    "\n",
    "        self.fc_1 = nn.Sequential(*modules_1)\n",
    "#         self.fc_1.apply(xavier_init)\n",
    "        \n",
    "        self.fc_2 = nn.Sequential(*modules_2)\n",
    "#         self.fc_2.apply(xavier_init)\n",
    "\n",
    "        for layer in self.fc_1.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                 layer.weight.data.normal_(mean=0, std=0.2)\n",
    "\n",
    "        for layer in self.fc_2.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                 layer.weight.data.normal_(mean=0, std=0.2)\n",
    "        \n",
    "    def forward(self, x_train, t_train):\n",
    "        T = self.fc_1( torch.cat((x_train, t_train), 1) )\n",
    "        dTdx = torch.autograd.grad(T, x_train, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "        d2Tdx2 = torch.autograd.grad(dTdx, x_train, grad_outputs=torch.ones_like(dTdx), create_graph=True)[0]\n",
    "        dTdt = torch.autograd.grad(T, t_train, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "        \n",
    "        s = self.fc_2(t_train)\n",
    "        Ts = self.fc_1( torch.cat((s, t_train),1) )\n",
    "        dTsdx = torch.autograd.grad(Ts, s, grad_outputs=torch.ones_like(Ts), create_graph=True)[0]\n",
    "        dsdt = torch.autograd.grad(s, t_train, grad_outputs=torch.ones_like(s), create_graph=True)[0]\n",
    "    \n",
    "        \n",
    "        return T, dTdt, d2Tdx2, s, Ts, dTsdx, dsdt\n",
    "    \n",
    "def get_loss(x_train, t_train, k1, k2, N_tot, N_ic, T_l, T_r, N_xl, N_xr, x_l, x_r, T_ini, s_ini, null):\n",
    "    \n",
    "    mse = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    w1 = 1\n",
    "    w2 = 1\n",
    "    w3 = 1\n",
    "    w4 = 1\n",
    "    w5 = 1\n",
    "    w6 = 1\n",
    "    T, dTdt, d2Tdx2, s, Ts, dTsdx, dsdt = model(x_train, t_train)\n",
    "    eq1 = w1*mse(torch.mul ( 1/(1 + torch.exp(-50*(s - x_train))), (dTdt-k1*d2Tdx2)), null) /(N_tot)\n",
    "    ic1 = w2*( mse( torch.mul( torch.where(t_train == 0,1,0), (T - T_ini) ), null ) )/(N_ic)\n",
    "    bc1 = w3*mse( torch.mul(torch.where(x_train == x_l,1,0),(T - T_l)), null )/(N_xl)\n",
    "    bc2 = w4*mse( Ts - T_r, null )/(N_tot)\n",
    "    eq2 = w5*mse( dsdt+k2*dTsdx, null )/(N_tot)\n",
    "    ic2 = w6*mse( torch.mul(torch.where(t_train == 0,1,0),(s - s_ini)), null ) /(N_ic)\n",
    "    loss = eq1 + eq2 + bc1 + bc2 + ic1 + ic2\n",
    "    \n",
    "    return loss, eq1, eq2, bc1, bc2, ic1, ic2\n",
    "\n",
    "def print_loss(epoch, loss, eq1, eq2, bc1, bc2, ic1, ic2):\n",
    "    print('epoch = ',epoch)\n",
    "    print('loss = ',loss.detach().numpy())\n",
    "    print('eq1_loss = ',eq1.detach().numpy())\n",
    "    print('eq2_loss = ',eq2.detach().numpy())\n",
    "    print('bc1_loss = ',bc1.detach().numpy())\n",
    "    print('bc2_loss = ',bc2.detach().numpy())\n",
    "    print('ic1_loss = ',ic1.detach().numpy())\n",
    "    print('ic2_loss = ',ic2.detach().numpy())\n",
    "    \n",
    "def L2_err(N_x_test, N_t_test, x_test, t_test, y_an, model):\n",
    "    \n",
    "    x_test = np_to_torch(x_test)\n",
    "    t_test = np_to_torch(t_test)\n",
    "    y_pred,_,_,s_pred,_,_,_  = model(x_test, t_test)\n",
    "    y_pred = y_pred.detach().numpy()\n",
    "    s_pred = s_pred.detach().numpy()\n",
    "    x_test = x_test.detach().numpy()\n",
    "    \n",
    "    total_err = 0\n",
    "    cnt = 0\n",
    "    for i in range(N_x_test*N_t_test):\n",
    "        if(s_pred[i]>x_test[i]):\n",
    "            cnt = cnt+1\n",
    "            total_err += (y_pred[i] - y_an[i])**2\n",
    "    \n",
    "    if(cnt==0):\n",
    "        return 0\n",
    "    L2_err =  total_err/(cnt)\n",
    "    return L2_err[0]\n",
    "\n",
    "def lamb_analytical(k1, k2):\n",
    "    x = []\n",
    "    er = []\n",
    "    cnt = 0\n",
    "    for i in np.arange(0.1, 5, 0.001):\n",
    "        x.append(i)\n",
    "        er.append(math.erf(x[-1]))\n",
    "        cnt = cnt+1\n",
    "\n",
    "    x = np.array(x)\n",
    "    er = np.array(er)\n",
    "    y =[]\n",
    "    y = np.exp(-x*x)/(er*math.sqrt(math.pi))-x*k1/k2\n",
    "\n",
    "    for i in range(1,cnt):\n",
    "        if(y[i]*y[i-1]<0):\n",
    "            lam = x[i]\n",
    "            break\n",
    "    \n",
    "    return lam\n",
    "\n",
    "def analytical(N_x_test, N_t_test, x_l, x_r, t_i, t_f, k1, k2):\n",
    "    \n",
    "    x_test = np.linspace(x_l, x_r, N_x_test)\n",
    "    x_test = np.tile(x_test, N_t_test)\n",
    "    t_test = np.linspace(t_i, t_f, N_t_test)\n",
    "    t_test = np.repeat(t_test, N_x_test)\n",
    "\n",
    "    lam = lamb_analytical(k1, k2)\n",
    "    \n",
    "    s_an = np.sqrt(k1*t_test)*2*lam\n",
    "    y_an = np.zeros(N_x_test*N_t_test)\n",
    "    for j in range(N_x_test*N_t_test):\n",
    "        if(x_test[j]<s_an[j]):\n",
    "            y_an[j] = 1 - math.erf( x_test[j]/( 2*np.sqrt(k1*t_test[j]) ) )/ math.erf(lam) \n",
    "       \n",
    "    y_an = y_an.reshape(N_x_test*N_t_test,1)\n",
    "    \n",
    "    return y_an, x_test, t_test\n",
    "    \n",
    "    \n",
    "def train_model(model, optimiser1, epochs, T_r, T_l, k1, k2, N_x, x_l, x_r, N_t, N_bc, N_ic, t_arr, t_i, T_ini, accuracy_cap, N_x_test, N_t_test, s_ini):\n",
    "    \n",
    "    loss_store = []\n",
    "    mse = nn.MSELoss(reduction='sum')\n",
    "    model.train()  \n",
    "    \n",
    "    start = time.time()\n",
    "    y_an1, x_test1, t_test1 = analytical(N_x_test, N_t_test, x_l, x_r, t_i, t_arr[-1], k1, k2)\n",
    "    for j in range(len(t_arr)-1):\n",
    "        \n",
    "        if(j==0):\n",
    "            epochs = 20000\n",
    "        else:\n",
    "            epochs = 2000\n",
    "            \n",
    "        N_t_fin = int( ( t_arr[j+1] - t_arr[j] )*N_t )\n",
    "        N_bc_fin = int( ( t_arr[j+1] - t_arr[j] )*N_bc )\n",
    "        N_ic_fin = int( ( t_arr[j+1] - t_arr[j] )*N_ic )\n",
    "        print(\"N_t_fin = \", N_t_fin)\n",
    "            \n",
    "        # Training data    \n",
    "        N_tot = N_x*N_t_fin + 2*N_bc_fin + N_ic_fin\n",
    "        null = torch.zeros(N_tot).unsqueeze(-1)\n",
    "        x_train, N_xl, N_xr = x_train_data(N_x, x_l, x_r, N_bc_fin, N_ic_fin, N_t_fin, null)\n",
    "        t_train = t_train_data(t_i,t_arr[j+1],N_t_fin,N_x,N_bc_fin,N_ic_fin)\n",
    "        N_ic_fin = mse( torch.where(t_train == t_i,1,0) , null).detach().numpy().item()\n",
    "        print(\"x_train = \", x_train.shape)\n",
    "        print(\"t_train = \", t_train.shape)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Test Data\n",
    "        N_t_test_2 = int( N_t_test*( t_arr[j+1] - t_arr[j] ) )\n",
    "        y_an2, x_test2, t_test2 = analytical(N_x_test, N_t_test_2, x_l, x_r, t_arr[j], t_arr[j+1], k1, k2)\n",
    "\n",
    "        # Adam optimiser loop\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            #Backpropogation and optimisation\n",
    "            loss, eq1, eq2, bc1, bc2, ic1, ic2 = get_loss(x_train, t_train, k1, k2, N_tot, N_ic, T_l, T_r, N_xl, N_xr, x_l, x_r, T_ini, s_ini, null)\n",
    "            optimiser1.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser1.step()  \n",
    "            loss_store.append(loss.detach().numpy())\n",
    "\n",
    "#             L2_norm_err_1 = L2_err(N_x_test, N_t_test, x_test1, t_test1, y_an1, model)\n",
    "            L2_norm_err_2 = L2_err(N_x_test, N_t_test_2, x_test2, t_test2, y_an2, model)\n",
    "            \n",
    "            if epoch%400==0:\n",
    "                print_loss(epoch, loss, eq1, eq2, bc1, bc2, ic1, ic2)\n",
    "#                 print(\"L2_err_1= \", L2_norm_err_1 )\n",
    "                print(\"L2_err_2= \", L2_norm_err_2 )\n",
    "                print(\"\")\n",
    "                \n",
    "            if epoch%1000==0:\n",
    "                print(\"time = \", time.time() - start)\n",
    "                print(\"\")\n",
    "            \n",
    "            if L2_norm_err_2<0.004 and j==0 and epoch>5000:\n",
    "                print(\"loss limit attained, epoch = \", epoch)\n",
    "                print(\"\")\n",
    "                break\n",
    "            if L2_norm_err_2<accuracy_cap and j>0:\n",
    "                print(\"loss limit attained, epoch = \", epoch)\n",
    "                print(\"\")\n",
    "                break\n",
    "            \n",
    "        print(\"broke inner loop\")\n",
    "        print(\"\")\n",
    "        \n",
    "#         if(L2_norm_err_1<accuracy_cap):\n",
    "#             break\n",
    "        \n",
    "    end = time.time()\n",
    "\n",
    "    return end-start, loss_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa5eeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN(\n",
      "  (fc_1): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=15, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=15, out_features=15, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=15, out_features=15, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=15, out_features=15, bias=True)\n",
      "    (7): Tanh()\n",
      "    (8): Linear(in_features=15, out_features=1, bias=True)\n",
      "    (9): Tanh()\n",
      "  )\n",
      "  (fc_2): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=15, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=15, out_features=15, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=15, out_features=15, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=15, out_features=15, bias=True)\n",
      "    (7): Tanh()\n",
      "    (8): Linear(in_features=15, out_features=1, bias=True)\n",
      "    (9): Tanh()\n",
      "  )\n",
      ")\n",
      "Total trainable parameters in the model: 1547\n",
      "N_t_fin =  80\n",
      "x_train =  torch.Size([3040, 1])\n",
      "t_train =  torch.Size([3040, 1])\n",
      "\n",
      "epoch =  0\n",
      "loss =  0.73015374\n",
      "eq1_loss =  1.0475043e-12\n",
      "eq2_loss =  0.0005778917\n",
      "bc1_loss =  0.41158834\n",
      "bc2_loss =  0.13194221\n",
      "ic1_loss =  0.14579071\n",
      "ic2_loss =  0.04025461\n",
      "L2_err_2=  0\n",
      "\n",
      "time =  0.13077616691589355\n",
      "\n",
      "epoch =  400\n",
      "loss =  0.62107044\n",
      "eq1_loss =  0.0025825757\n",
      "eq2_loss =  0.008751403\n",
      "bc1_loss =  0.4039012\n",
      "bc2_loss =  0.12532987\n",
      "ic1_loss =  0.07855109\n",
      "ic2_loss =  0.0019543425\n",
      "L2_err_2=  0.2545990351258195\n",
      "\n",
      "epoch =  800\n",
      "loss =  0.52000356\n",
      "eq1_loss =  0.009955556\n",
      "eq2_loss =  0.004295857\n",
      "bc1_loss =  0.30988938\n",
      "bc2_loss =  0.10007156\n",
      "ic1_loss =  0.082202144\n",
      "ic2_loss =  0.013589075\n",
      "L2_err_2=  0.06618123898913046\n",
      "\n",
      "time =  72.10309934616089\n",
      "\n",
      "epoch =  1200\n",
      "loss =  0.41097227\n",
      "eq1_loss =  0.012544506\n",
      "eq2_loss =  0.006970624\n",
      "bc1_loss =  0.19859275\n",
      "bc2_loss =  0.040316\n",
      "ic1_loss =  0.13626304\n",
      "ic2_loss =  0.016285354\n",
      "L2_err_2=  0.038200262087261544\n",
      "\n",
      "epoch =  1600\n",
      "loss =  0.36055002\n",
      "eq1_loss =  0.019288914\n",
      "eq2_loss =  0.003796738\n",
      "bc1_loss =  0.15133503\n",
      "bc2_loss =  0.03228781\n",
      "ic1_loss =  0.1379992\n",
      "ic2_loss =  0.015842319\n",
      "L2_err_2=  0.02807509602190629\n",
      "\n",
      "epoch =  2000\n",
      "loss =  0.3457246\n",
      "eq1_loss =  0.019401928\n",
      "eq2_loss =  0.008938923\n",
      "bc1_loss =  0.14740042\n",
      "bc2_loss =  0.026349565\n",
      "ic1_loss =  0.12530348\n",
      "ic2_loss =  0.018330311\n",
      "L2_err_2=  0.026259313481637882\n",
      "\n",
      "time =  154.52729678153992\n",
      "\n",
      "epoch =  2400\n",
      "loss =  0.32742634\n",
      "eq1_loss =  0.016583556\n",
      "eq2_loss =  0.010391176\n",
      "bc1_loss =  0.14679328\n",
      "bc2_loss =  0.020581618\n",
      "ic1_loss =  0.1122375\n",
      "ic2_loss =  0.020839216\n",
      "L2_err_2=  0.02521444199380427\n",
      "\n",
      "epoch =  2800\n",
      "loss =  0.29003552\n",
      "eq1_loss =  0.010810545\n",
      "eq2_loss =  0.00857836\n",
      "bc1_loss =  0.14135927\n",
      "bc2_loss =  0.014601938\n",
      "ic1_loss =  0.09254953\n",
      "ic2_loss =  0.022135872\n",
      "L2_err_2=  0.02449413106084292\n",
      "\n",
      "time =  239.87539386749268\n",
      "\n",
      "epoch =  3200\n",
      "loss =  0.24134946\n",
      "eq1_loss =  0.0106803365\n",
      "eq2_loss =  0.007847847\n",
      "bc1_loss =  0.11781869\n",
      "bc2_loss =  0.0117553435\n",
      "ic1_loss =  0.07359167\n",
      "ic2_loss =  0.019655555\n",
      "L2_err_2=  0.02045695871638883\n",
      "\n",
      "epoch =  3600\n",
      "loss =  0.19508599\n",
      "eq1_loss =  0.016631566\n",
      "eq2_loss =  0.004846118\n",
      "bc1_loss =  0.096056476\n",
      "bc2_loss =  0.008916258\n",
      "ic1_loss =  0.05406886\n",
      "ic2_loss =  0.0145667\n",
      "L2_err_2=  0.015059025075464721\n",
      "\n",
      "epoch =  4000\n",
      "loss =  0.16813041\n",
      "eq1_loss =  0.02170516\n",
      "eq2_loss =  0.0017849297\n",
      "bc1_loss =  0.091032244\n",
      "bc2_loss =  0.009069692\n",
      "ic1_loss =  0.03678573\n",
      "ic2_loss =  0.007752639\n",
      "L2_err_2=  0.012628851787196527\n",
      "\n",
      "time =  335.30490827560425\n",
      "\n",
      "epoch =  4400\n",
      "loss =  0.15413992\n",
      "eq1_loss =  0.022938918\n",
      "eq2_loss =  0.0016479096\n",
      "bc1_loss =  0.08709937\n",
      "bc2_loss =  0.008446259\n",
      "ic1_loss =  0.028662099\n",
      "ic2_loss =  0.0053453636\n",
      "L2_err_2=  0.011756142074995288\n",
      "\n",
      "epoch =  4800\n",
      "loss =  0.14252514\n",
      "eq1_loss =  0.022815187\n",
      "eq2_loss =  0.0016874265\n",
      "bc1_loss =  0.08309908\n",
      "bc2_loss =  0.007628444\n",
      "ic1_loss =  0.02317654\n",
      "ic2_loss =  0.004118464\n",
      "L2_err_2=  0.010964225132572978\n",
      "\n",
      "time =  425.1792731285095\n",
      "\n",
      "epoch =  5200\n",
      "loss =  0.13492164\n",
      "eq1_loss =  0.02341066\n",
      "eq2_loss =  0.0019800162\n",
      "bc1_loss =  0.07809393\n",
      "bc2_loss =  0.0070859296\n",
      "ic1_loss =  0.020788863\n",
      "ic2_loss =  0.0035622343\n",
      "L2_err_2=  0.010105574145966035\n",
      "\n",
      "epoch =  5600\n",
      "loss =  0.128246\n",
      "eq1_loss =  0.022667864\n",
      "eq2_loss =  0.0018355354\n",
      "bc1_loss =  0.07418691\n",
      "bc2_loss =  0.006267466\n",
      "ic1_loss =  0.01968886\n",
      "ic2_loss =  0.0035993548\n",
      "L2_err_2=  0.009478711210922294\n",
      "\n",
      "epoch =  6000\n",
      "loss =  0.118535556\n",
      "eq1_loss =  0.019767148\n",
      "eq2_loss =  0.0013802142\n",
      "bc1_loss =  0.06913997\n",
      "bc2_loss =  0.0047627827\n",
      "ic1_loss =  0.019183319\n",
      "ic2_loss =  0.00430212\n",
      "L2_err_2=  0.008823525976429521\n",
      "\n",
      "time =  507.70872235298157\n",
      "\n",
      "epoch =  6400\n",
      "loss =  0.10082109\n",
      "eq1_loss =  0.01359989\n",
      "eq2_loss =  0.000873262\n",
      "bc1_loss =  0.059547286\n",
      "bc2_loss =  0.0025427896\n",
      "ic1_loss =  0.018470382\n",
      "ic2_loss =  0.0057874867\n",
      "L2_err_2=  0.007322466969119572\n",
      "\n",
      "epoch =  6800\n",
      "loss =  0.087135814\n",
      "eq1_loss =  0.012394335\n",
      "eq2_loss =  0.0009514059\n",
      "bc1_loss =  0.050997995\n",
      "bc2_loss =  0.002147372\n",
      "ic1_loss =  0.015722698\n",
      "ic2_loss =  0.0049220114\n",
      "L2_err_2=  0.006140081147167349\n",
      "\n",
      "time =  593.4151265621185\n",
      "\n",
      "epoch =  7200\n",
      "loss =  0.07887684\n",
      "eq1_loss =  0.011645332\n",
      "eq2_loss =  0.00075674074\n",
      "bc1_loss =  0.047173716\n",
      "bc2_loss =  0.0019197953\n",
      "ic1_loss =  0.013314723\n",
      "ic2_loss =  0.004066533\n",
      "L2_err_2=  0.005639119476209892\n",
      "\n",
      "epoch =  7600\n",
      "loss =  0.07125134\n",
      "eq1_loss =  0.010034652\n",
      "eq2_loss =  0.0005762574\n",
      "bc1_loss =  0.044388544\n",
      "bc2_loss =  0.0016957464\n",
      "ic1_loss =  0.011263078\n",
      "ic2_loss =  0.0032930593\n",
      "L2_err_2=  0.005241787101741298\n",
      "\n",
      "epoch =  8000\n",
      "loss =  0.06337546\n",
      "eq1_loss =  0.008428714\n",
      "eq2_loss =  0.00031550333\n",
      "bc1_loss =  0.040944256\n",
      "bc2_loss =  0.0013801892\n",
      "ic1_loss =  0.009566432\n",
      "ic2_loss =  0.0027403608\n",
      "L2_err_2=  0.004787713955730889\n",
      "\n",
      "time =  685.692259311676\n",
      "\n",
      "epoch =  8400\n",
      "loss =  0.057667233\n",
      "eq1_loss =  0.008160281\n",
      "eq2_loss =  0.00021387596\n",
      "bc1_loss =  0.037060857\n",
      "bc2_loss =  0.0011699777\n",
      "ic1_loss =  0.008629784\n",
      "ic2_loss =  0.002432457\n",
      "L2_err_2=  0.004208034246708005\n",
      "\n",
      "loss limit attained, epoch =  8556\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "N_t_fin =  10\n",
      "x_train =  torch.Size([380, 1])\n",
      "t_train =  torch.Size([380, 1])\n",
      "\n",
      "epoch =  0\n",
      "loss =  0.041085977\n",
      "eq1_loss =  0.015990013\n",
      "eq2_loss =  0.0005546888\n",
      "bc1_loss =  0.0153841125\n",
      "bc2_loss =  0.0006796507\n",
      "ic1_loss =  0.007550688\n",
      "ic2_loss =  0.0009268261\n",
      "L2_err_2=  0.0018922801080737222\n",
      "\n",
      "time =  734.0957911014557\n",
      "\n",
      "epoch =  400\n",
      "loss =  0.03261604\n",
      "eq1_loss =  0.011661576\n",
      "eq2_loss =  0.00016407466\n",
      "bc1_loss =  0.016443273\n",
      "bc2_loss =  0.00087038014\n",
      "ic1_loss =  0.0028497996\n",
      "ic2_loss =  0.00062693714\n",
      "L2_err_2=  0.0013939933019971496\n",
      "\n",
      "epoch =  800\n",
      "loss =  0.03073971\n",
      "eq1_loss =  0.010329568\n",
      "eq2_loss =  0.00014508833\n",
      "bc1_loss =  0.01621433\n",
      "bc2_loss =  0.0008900827\n",
      "ic1_loss =  0.0025831736\n",
      "ic2_loss =  0.0005774666\n",
      "L2_err_2=  0.0014340483902070197\n",
      "\n",
      "time =  756.8095693588257\n",
      "\n",
      "epoch =  1200\n",
      "loss =  0.029322332\n",
      "eq1_loss =  0.009664324\n",
      "eq2_loss =  0.0001287532\n",
      "bc1_loss =  0.015738847\n",
      "bc2_loss =  0.0008964746\n",
      "ic1_loss =  0.0023642809\n",
      "ic2_loss =  0.00052965124\n",
      "L2_err_2=  0.0014808769733935744\n",
      "\n",
      "epoch =  1600\n",
      "loss =  0.027944844\n",
      "eq1_loss =  0.009178562\n",
      "eq2_loss =  0.00010802353\n",
      "bc1_loss =  0.015126191\n",
      "bc2_loss =  0.00087365735\n",
      "ic1_loss =  0.0021703506\n",
      "ic2_loss =  0.00048805954\n",
      "L2_err_2=  0.001493007245724691\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "N_t_fin =  10\n",
      "x_train =  torch.Size([380, 1])\n",
      "t_train =  torch.Size([380, 1])\n",
      "\n",
      "epoch =  0\n",
      "loss =  0.018641582\n",
      "eq1_loss =  0.008184497\n",
      "eq2_loss =  0.00012944445\n",
      "bc1_loss =  0.0071637006\n",
      "bc2_loss =  0.0006650282\n",
      "ic1_loss =  0.002038984\n",
      "ic2_loss =  0.00045992783\n",
      "L2_err_2=  0.0014683705307791876\n",
      "\n",
      "time =  782.1603178977966\n",
      "\n",
      "epoch =  400\n",
      "loss =  0.015506491\n",
      "eq1_loss =  0.006347936\n",
      "eq2_loss =  0.00014939485\n",
      "bc1_loss =  0.0071128635\n",
      "bc2_loss =  0.00059308356\n",
      "ic1_loss =  0.00094162894\n",
      "ic2_loss =  0.0003615839\n",
      "L2_err_2=  0.0012878377147459562\n",
      "\n",
      "epoch =  800\n",
      "loss =  0.014657035\n",
      "eq1_loss =  0.00578255\n",
      "eq2_loss =  0.00012313372\n",
      "bc1_loss =  0.0070353327\n",
      "bc2_loss =  0.00055039517\n",
      "ic1_loss =  0.0008289852\n",
      "ic2_loss =  0.00033663766\n",
      "L2_err_2=  0.001312853941364552\n",
      "\n",
      "time =  807.6308875083923\n",
      "\n",
      "epoch =  1200\n",
      "loss =  0.014123086\n",
      "eq1_loss =  0.0053585754\n",
      "eq2_loss =  0.0002644902\n",
      "bc1_loss =  0.0069156727\n",
      "bc2_loss =  0.00050296326\n",
      "ic1_loss =  0.0007667044\n",
      "ic2_loss =  0.00031467888\n",
      "L2_err_2=  0.0013199512582310051\n",
      "\n",
      "epoch =  1600\n",
      "loss =  0.013428445\n",
      "eq1_loss =  0.005087344\n",
      "eq2_loss =  8.831427e-05\n",
      "bc1_loss =  0.006773333\n",
      "bc2_loss =  0.0004686336\n",
      "ic1_loss =  0.0007053208\n",
      "ic2_loss =  0.00030549895\n",
      "L2_err_2=  0.001309316006702436\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broke inner loop\n",
      "\n",
      "time elapsed =  833.1876223087311\n"
     ]
    }
   ],
   "source": [
    "N_x = 35\n",
    "N_bc = 101\n",
    "N_ic = 101\n",
    "N_t = 101\n",
    "x_l = 0\n",
    "x_r = 1\n",
    "t_i = 0\n",
    "T_r = 0\n",
    "T_l = 1\n",
    "T_ini = 0\n",
    "s_ini = 0\n",
    "accuracy_cap = 0.0015\n",
    "N_x_test = 100\n",
    "N_t_test = 100\n",
    "t_arr = [0, 0.8, 0.9, 1]\n",
    "# t_arr = [0, 1]\n",
    "\n",
    "# Neural network params\n",
    "layer_size_1 = [2, 15, 15, 15, 15, 1]\n",
    "layer_size_2 = [1, 15, 15, 15, 15, 1]\n",
    "\n",
    "# material params\n",
    "k1 = 0.05\n",
    "k2 = 0.8\n",
    "\n",
    "# Training data and initial data\n",
    "model = ANN(layer_size_1, layer_size_2)\n",
    "print(model)\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total trainable parameters in the model:\", total_trainable_params)\n",
    "\n",
    "# # Setup Loss function and Optimiser\n",
    "lr = 6e-5\n",
    "epochs = 15000\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training model\n",
    "time_elapsed, loss_store = train_model(model, optimiser, epochs, T_r, T_l, k1, k2, N_x, x_l, x_r, N_t, N_bc, N_ic, t_arr, t_i, T_ini, accuracy_cap, N_x_test, N_t_test, s_ini)\n",
    "print(\"time elapsed = \", time_elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890d895a",
   "metadata": {},
   "source": [
    "# Results Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5b5645f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAsPElEQVR4nO3deXiM5/7H8fc3k00qsQYhloREEGKJpXbHFtRWy6GqpX5UVXva6qLltEq11fVUq7vSaku11FJba6tSVOxia+yxBrULidy/PyatJILBJE9m8n1dl+vKzH1n5vMIH+NZ7keMMSillHJ9HlYHUEop5Rxa6Eop5Sa00JVSyk1ooSullJvQQldKKTfhadUbFy1a1JQrV86qt1dKKZe0du3a48aYwKzGLCv0cuXKERsba9XbK6WUSxKRfdcb010uSinlJrTQlVLKTWihK6WUm7BsH7pSyr0kJyeTkJBAUlKS1VHcgq+vL8HBwXh5eTn8PVroSimnSEhIwN/fn3LlyiEiVsdxacYYTpw4QUJCAiEhIQ5/3013uYjIFyJyTES2XGdcRGSsiMSLyCYRqXkLuZVSbiIpKYkiRYpomTuBiFCkSJFb/t+OI/vQJwIxNxhvA4Sl/RoAfHRLCZRSbkPL3Hlu5/fypoVujFkGnLzBlI7AV8ZuFVBQRIJuOYmjjsbBwhGgy/4qpVQGzjjLpRRwIN3jhLTnriEiA0QkVkRiExMTb+/d9i6H5e/Cjrm39/1KKbdls9moXr06kZGRdOvWjQsXLgCQP39+APbu3YuI8P777//zPYMHD2bixIkA9OnTh1KlSnHp0iUAjh8/jitd0Z6jpy0aYz41xkQbY6IDA7O8cvXmoh+CwAhY8AIk69F0pdRV+fLlY8OGDWzZsgVvb28+/vjja+YUK1aM9957j8uXL2f5GjabjS+++CK7o2YLZxT6QaB0usfBac9lD5sXxLwOf+2FVeOy7W2UUq6tUaNGxMfHX/N8YGAgzZs358svv8zy+5544gneffddUlJSsjui0znjtMVZwGARmQLUBU4bYw474XWvr3wziLgHlr0NUT0hoGS2vp1S6ta8PDuOrYfOOPU1K5cM4KX2VRyam5KSwrx584iJyfp8jueee442bdrw0EMPXTNWpkwZGjZsyKRJk2jfvv0dZc5pjpy2OBlYCVQUkQQR6SciA0VkYNqUucBuIB74DBiUbWnTazUKUpNh4cs58nZKqdzv4sWLVK9enejoaMqUKUO/fv2ynBcaGkrdunX59ttvsxx//vnnefPNN0lNTc3OuE5300/oxpieNxk3wKNOS+SowqFw92BY/g7U7gel6+R4BKVU1hz9JO1sf+9Dd8QLL7xA165dadKkyTVjYWFhVK9enalTpzo5YfZy7bVcGg0B/yCY9xy42L+kSilrRUREULlyZWbPnp3l+LBhw3jrrbdyONWdce1C98kPLV6GQ+tgY9b/dVJKqesZNmwYCQkJWY5VqVKFmjVd68J3MRZdoBMdHW2ccoMLY2B8K/tZL4+tBd+AO39NpdQt27ZtG5UqVbI6hlvJ6vdURNYaY6Kzmu/an9ABRKDNGDifCMvesDqNUkpZxvULHaBUTajRC1Z9DMf/tDqNUkpZwj0KHaD5S+CVz34FqVJK5UEuWehZ7vfPXwyaPAt//gw7F+R8KKWUspjLFfqsjYfoNG4FSclXrh2s8zAUCYP5z0NK1us0KKWUu3K5Qi+Yz4uNCaf55Nfd1w56ekPMa3ByF6zWZdmVUnmLyxV64/BA2keVZNzSePYcP3/thLCWEB4Dv74BZ4/kfECllKVmzJiBiLB9+/bbfo0+ffrwww8/3HDOq6++muFx/fr1b+u9RowY4bQLmFyu0AH+264SPjYPXpy5Jev96a1fhZRLus6LUnnQ5MmTadiwIZMnT87W98lc6L///nu2vp8jXLLQiwX48kxMRX778zizNh66dkKR8nD3o/arRxOccPGSUsolnDt3juXLlzN+/HimTJkCwNKlS2natCldu3YlIiKCXr16/fNBcOTIkdSuXZvIyEgGDBhwzQfExYsX06lTp38e//LLL3Tu3JmhQ4f+sxBYr169gKs30QAYM2YMVatWJSoqiqFDhwLw2WefUbt2baKioujSpcs/N99wJmcsn2uJXnXLMm1tAqN+2kbTisUokM8r44TGT8PGKTD3Gfi/ReDhkv92KeWa5g2FI5ud+5olqkKb1284ZebMmcTExBAeHk6RIkVYu3YtAOvXrycuLo6SJUvSoEEDVqxYQcOGDRk8eDAvvvgiAL179+ann37KsGRus2bNGDRoEImJiQQGBjJhwgQeeugh2rdvzwcffJDlQmDz5s1j5syZrF69Gj8/P06etN/B895776V///4ADB8+nPHjx/PYY48543fmHy7bcjYPYXTnqpw8f4m3Fuy4doKPP7Qcqeu8KJWHTJ48mR49egDQo0ePf3a71KlTh+DgYDw8PKhevTp79+4FYMmSJdStW5eqVauyePFi4uLiMryeiNC7d2++/vprTp06xcqVK2nTps0NMyxcuJC+ffvi5+cHQOHChQHYsmULjRo1omrVqnzzzTfXvJczuOwndIDIUgV4sH45Jv6+ly61gqleumDGCdW6w5rP7TeVrtQefAtYEVOpvOcmn6Szw8mTJ1m8eDGbN29GRLhy5QoiQrt27fDx8flnns1mIyUlhaSkJAYNGkRsbCylS5dmxIgRJCVde1vLvn370r59e3x9fenWrRuenrdXm3369GHGjBlERUUxceJEli5derubel0u+wn9b0+1DKeYvw8vTN9MypVMS+iKQNs34Pxx+1kvSim39cMPP9C7d2/27dvH3r17OXDgACEhIfz2229Zzv+7vIsWLcq5c+eue1ZLyZIlKVmyJK+88gp9+/b953kvLy+Sk5Ovmd+yZUsmTJjwzz7yv3e5nD17lqCgIJKTk/nmm2/uaFuvx+UL3d/XixHtq7D18Bm+XLnv2gkla0DNB2D1x5CYxa4ZpZRbmDx5Mp07d87wXJcuXa57tkvBggXp378/kZGRtG7dmtq1a1/3tXv16kXp0qUzrHw4YMAAqlWr9s9B0b/FxMTQoUMHoqOjqV69+j+nJI4aNYq6devSoEEDIiIibnczb8j1l8/FvhTAQxPX8Meekywc0oSgAvkyTjh/HN6vCSVrQu8f7Z/clVJO5c7L5w4ePJgaNWpc95Z22SXvLZ+L/cDFyI6RXDGGkbO3XjvhrqLQbBjsXgLb5+R8QKWUy6pVqxabNm3i/vvvtzrKTblFoQOULuzHY/8KY96WIyzefvTaCdH9ILASLHgeki/mfECllEtau3Yty5Yty3BgNbdym0IH6N8olArF8vPizDguXs60eJfN036A9NR+WDHWmoBKuTmrduG6o9v5vXSrQvf29GB0p0gS/rrI2MVZ3OgipDFU7gjL37EXu1LKaXx9fTlx4oSWuhMYYzhx4gS+vr639H0ufR56VuqGFqFrrWA+W7abzjVKEV7cP+OEVqNh58+wYBj8e5I1IZVyQ8HBwSQkJJCYmGh1FLfg6+tLcHDwLX2P2xU6wAttK7Fw21GG/biZ7wbcjYdHurNaCpaGRk/BktGweymENrUqplJuxcvLi5CQEKtj5Glutcvlb4Xv8ub5NhGs2fsXP6xLuHZC/cehUDmY+yxcufbCAKWUckVuWegA3WqVJrpsIV6bu42T5zPdvcjLF1q/Bsd3wOpPrAmolFJO5raF7uEhvNI5krNJKbw+b9u1Eyq2gQotYOnrcDaL0xyVUsrFuG2hA0SUCKBfoxCmxibwx56TGQdFIGYMpCTBwpesCaiUUk7k1oUO8J/mYZQqmI/hMzZzOSXT4l1FK0D9wbBxMuxfbU1ApZRyErcvdD9vT17uUIWdR88xfvmeayc0ehr8S8LcpyH1yrXjSinlIty+0AFaVC5Oq8rFeW/RTg6czHTbJ5/80PoVOLIJ1k6wJqBSSjmBQ4UuIjEiskNE4kVkaBbjZURkiYisF5FNItLW+VHvzIgOVfAQ4aVZcddeyVblXijXCBaNgvMnrAmolFJ36KaFLiI2YBzQBqgM9BSRypmmDQemGmNqAD2AD50d9E6VLJiPp1qGs3j7MRbEHck4KAJt34TL52DRCEvyKaXUnXLkE3odIN4Ys9sYcxmYAnTMNMcAAWlfFwAOOS+i8/SpX45KQQGMmLWVc5dSMg4WqwR1B8K6SZCw1pqASil1Bxwp9FLAgXSPE9KeS28EcL+IJABzgSxvZS0iA0QkVkRirVjvwdPmwaudIzl6Nom3f87i7kVNnoP8xWHuED1AqpRyOc46KNoTmGiMCQbaApNE5JrXNsZ8aoyJNsZEBwYGOumtb02NMoW4v25Zvvx9L5sTTmcc9A2AVq/AofWw7ktL8iml1O1ypNAPAqXTPQ5Oey69fsBUAGPMSsAXKOqMgNnhmZiKFMnvw/M/brr2xtJVu0LZhrBopB4gVUq5FEcKfQ0QJiIhIuKN/aDnrExz9gPNAUSkEvZCz7VraAb4evFS+8psOXiGrzLfWFoE2r0FSWf0AKlSyqXctNCNMSnAYGABsA372SxxIjJSRDqkTRsC9BeRjcBkoI/J5avct6saRNOKgbz98w4Oncp0S7pilaDeI7DuKziwxpqASil1i8Sq3o2OjjaxsbGWvPffDpy8QMt3f6VhhUA+e6AWIunWTb90Fj6oDfmLQf8l4GGzLqhSSqURkbXGmOisxvLElaLXU7qwH0+2CGfhtqPXnpvu4w+tR8PhjRD7hTUBlVLqFuTpQgd4qGEIlYICeGlWHGeSMt3sosq99vuQLhoF545ZE1AppRyU5wvdy+bBa/dW5djZS7w5P9O56SLQ9m1IvgC/vGhNQKWUclCeL3SA6qUL8uDd5fh69T7W7su0bnpgODR43L7E7t4V1gRUSikHaKGnebp1RYICfHl+ehbrpjd6GgqUgTlD9B6kSqlcSws9TX4fT17pHMnOo+f4+NddGQe9/aDNGEjcBqty3bpjSikFaKFn8K+I4txTLYgPFscTf+xcxsGIthDeBpaOgVMHsn4BpZSykBZ6Ji+1r0I+bxsvTN9Mamqmc/TbjAGTCvOvWRJeKaUsp4WeSaC/D8PaVeKPvSf59o/9GQcLlYWmz8H2n2DHPGsCKqXUdWihZ6FbrWAaVijK6/O2c/h0pmUB6j0KgREw91m4fN6agEoplQUt9CyICK92rsqVVMN/Z2zJeMs6T29o9w6c3g+/vmFdSKWUykQL/TrKFPFjSKtwFm47xk+bDmccLNcAqveClR/A0a3WBFRKqUy00G+gb4MQooILMGJWHCfPX8442HIU+ATAT09CamrWL6CUUjlIC/0GbB7CmK7VOJOUzMuz4zIO3lUEWo2CA6tg/SRrAiqlVDpa6DcRUSKAR5tVYOaGQyzcejTjYPVeULaBfZ2Xc7n2fh5KqTxCC90Bg5pWIKKEP8NmbOb0xXSX/ovAPe/az3b5ebh1AZVSCi10h3h7evBG12oknr3E6DmZDoIGVoSGT8CmKbBriSX5lFIKtNAdVi24IAMal2dqbALLdmbavdJoCBQOtR8gTb6Y9QsopVQ200K/BU+0CKN84F0MnbaJs+lvhuGVz77r5a89sOxN6wIqpfI0LfRb4Otl481uURw5k8Src7dnHAxtClE9YcV7em66UsoSWui3qGaZQvRvFMrkP/az/M/jGQdbjbafmz77P3puulIqx2mh34YnW4YTGngXz2Xe9XJXEWj9KiT8AbHjrQuolMqTtNBvg6+Xjbe6RXH49EVenbst42BUDwhtBgtfhtMHrQmolMqTtNBvU80yhejfOJTJfxzg1/Rnvfx9bnpqCsx9Goy5/osopZQTaaHfgSdbhBNWLD/P/bAp4wVHhUOg2QuwYy5snWldQKVUnqKFfgf+3vWSeO4SI2dnOrOl3iAIioJ5z8LFv6wJqJTKU7TQ71BU6YI80qQ809YlZFzrxeYJHd6H88fh5/9aF1AplWdooTvB483DqBQUwNDpmzMusxsUBQ0et6/GuHupZfmUUnmDFroTeHt68E73KE5fvHztHY6aPAeFy8Osx/WWdUqpbKWF7iSVggJ4okU4czYfZnb6Oxx55bPvejm1DxaPti6gUsrtOVToIhIjIjtEJF5Ehl5nTncR2SoicSLyrXNjuoaHG4dSvXRB/jtjC0dOJ10dKNcAovvBqg/hwBrrAiql3NpNC11EbMA4oA1QGegpIpUzzQkDngcaGGOqAE84P2ru52mz73q5lHKFZ6dtyrjrpcUICCgFMx+FlEuWZVRKuS9HPqHXAeKNMbuNMZeBKUDHTHP6A+OMMX8BGGOOOTem6wgNzM8LbSuxbGciX6/ef3XANwDavwfHd8Cvb1gXUCnlthwp9FLAgXSPE9KeSy8cCBeRFSKySkRisnohERkgIrEiEpuY6L63bOtdryyNwory6pxt7Dme7kBoWAv7beuWvwuHNliWTynlnpx1UNQTCAOaAj2Bz0SkYOZJxphPjTHRxpjowMBAJ7117iMivNk1Cm9PD578bgMpV9KtvNh6NNxVNG3Xy+Xrv4hSSt0iRwr9IFA63ePgtOfSSwBmGWOSjTF7gJ3YCz7PKlHAl1GdItlw4BTjluy6OpCvkH2tl6NbYPk71gVUSrkdRwp9DRAmIiEi4g30AGZlmjMD+6dzRKQo9l0wu50X0zV1iCpJx+olGbv4TzYcOHV1IKIdVO1mv7vR4U2W5VNKuZebFroxJgUYDCwAtgFTjTFxIjJSRDqkTVsAnBCRrcAS4BljzInsCu1KRnaMpLi/D09+t4ELl1OuDrR5A/IVhhmDdNeLUsopHNqHboyZa4wJN8aUN8aMTnvuRWPMrLSvjTHmKWNMZWNMVWPMlOwM7UoK5PPire5R7D1xntFz0q2d7lcY2v8Pjm6G3962LJ9Syn3olaI5oH75ovRvFMo3q/dnXMAroh1U7Q6/vQWHN1oXUCnlFrTQc8iQVuFUCgrguWmbSDyb7sKiNmPAryj8+IhecKSUuiNa6DnEx9PGez2qc+5SCs/+sPHqVaR+haHDWDgWB0tftzakUsqlaaHnoPDi/rzQthJLdiQyadW+dAOtoUZvWPE/XetFKXXbtNBz2AN3l6VpxUBGz9nGzqNnrw60ftW+1suMgXD5gnUBlVIuSws9h4kIb3StRn4fTx6fvJ6k5Cv2Ad8A6DgOTsTDwhGWZlRKuSYtdAsU8/flrW5RbD9yljHzt18dCG0CdQfCH5/ArsXWBVRKuSQtdIs0iyhGn/rlmLBiL0t2pFucssUIKBoOMx7Vm0srpW6JFrqFhraJIKKEP898v/HqqYxe+aDzJ3D+GMx9xtqASimXooVuIV8vG2N71uBsUgpDvt9IamraqYylakLjZ2Hz97BlmrUhlVIuQwvdYuHF/fnvPZVZtjOR8cv3XB1oNARKRcNPT8LpBOsCKqVchhZ6LtCrbhliqpTgjQXb2ZRwyv6kzRPu/RSupMCPAyE19YavoZRSWui5gIjwepeqBOb34bHJ6zmblGwfKFIe2rwOe3+DlR9YG1IpletpoecSBf28+V+PGhw4eYH/zthydWmAGr0h4h5YNFLXTldK3ZAWei5SJ6QwT7QIZ8aGQ3y/Nm2/uQh0eB/8isC0fnoVqVLqurTQc5lHm1WgfvkivDhzC3/+vTSAX2G49xM4/icseMHagEqpXEsLPZexeQj/+3d17vL2ZPC36ZYGCG0KDR6HtRNg22xLMyqlcict9FyoWIAvb3ePYsfRs7w8O+7qQLPhULIGzHoMTme+T7dSKq/TQs+lmlYsxiNNyzP5jwPMWJ9W3p7e0GW8/R6k0/tD6hVrQyqlchUt9FxsSMtwapcrxAs/bmZX4jn7k0XKwz3vwL4VsOxNawMqpXIVLfRczNPmwdieNfD1svHoN+uu7k+P6gFRPeHXMbB3ubUhlVK5hhZ6LhdUIB/vdLcvtTtiVrr96W3fgsKhMO3/4PwJ6wIqpXINLXQX0LRiMR5tVp4paw7ww9/np/vkh64T4MIJ+12OdGkApfI8LXQX8WSLcO4OLcLwGZvZfuSM/cmgavZb1/35M/z+nrUBlVKW00J3EZ42D97rWR1/Xy8e+Xrd1fVeav8fVO4Ei0bBvpWWZlRKWUsL3YUU8/flg5412H/yAs9N22Rf70UEOoyFgmXgh4fg/HGrYyqlLKKF7mLqhhbh2dYVmbv5yNX1030LQPcv4cJxmD5Az09XKo/SQndBAxqHElOlBK/N287q3WlnuARFQZsxsGuRnp+uVB6lhe6CRIQ3u1WjbGE/Hv12PUfPJNkHavWFaj1g6esQv9DakEqpHKeF7qL8fb34uHctzl9KYdA367ickmrfn37Pu1CsEkzrD6cOWB1TKZWDHCp0EYkRkR0iEi8iQ28wr4uIGBGJdl5EdT3hxf15s1s11u77i5E/pV105O0H3SfBlWT4/kFIuWRtSKVUjrlpoYuIDRgHtAEqAz1FpHIW8/yB/wCrnR1SXd891UrycONQvl61n6lr0j6RF60AnT+Cg2th3nPWBlRK5RhHPqHXAeKNMbuNMZeBKUDHLOaNAsYASU7MpxzwbEwEjcKKMnzGFjYcOGV/slJ7aPiUff30dV9Zmk8plTMcKfRSQPqdsQlpz/1DRGoCpY0xc5yYTTnI5iGM7VGDYgE+DJy0lmN/HyT913AIbQZzhkDCWmtDKqWy3R0fFBURD+AdYIgDcweISKyIxCYmJt7pW6t0Ct3lzae9ozl9MZmBX6/lUsoV8LBB1y8gfwmY2hvOHbM6plIqGzlS6AeB0ukeB6c99zd/IBJYKiJ7gXrArKwOjBpjPjXGRBtjogMDA28/tcpS5ZIBvNM9inX7TzH8xy32K0n9CkOPr+HCSZj6gP3mGEopt+RIoa8BwkQkRES8gR7ArL8HjTGnjTFFjTHljDHlgFVAB2NMbLYkVjfUpmoQjzcP4/u1CUxYsdf+ZFAUdPwA9q+Eec9amk8plX1uWujGmBRgMLAA2AZMNcbEichIEemQ3QHVrXuieRitKhfnlTlbWbojbTdL1a7Q4An7QdI14y3Np5TKHmKMseSNo6OjTWysfojPLucvpdD145UknLzA9EH1CSvub1/j5dt/w+4l8MBMKNfQ6phKqVskImuNMVle66NXirqpu3w8+fzBaHy8bPT7MpaT5y/bD5J2+RwKhcB3veHkHqtjKqWcSAvdjZUqmI/PHqjFkTNJDJyUduZLvoJw33dgUmFyD0g6Y3VMpZSTaKG7uRplCvFWtyj+2HuSodM22898KVIeun8FJ+JhWj9dblcpN6GFngd0iCrJM60r8uP6g7y78E/7k6FNoM0b9tvXLXjB2oBKKafwtDqAyhmDmpZn34nzjF30J2UK+9G1VjDU7gcndsGqcVC4PNQdYHVMpdQd0ELPI0SE0Z2rcvDURYZO20TxAB8ahQVCq1Hw1x6Y/xwUKgvhra2OqpS6TbrLJQ/xsnnw0f21qFAsP498vY64Q6ftZ77c+xkUj7Tfk/TwRqtjKqVukxZ6HhPg68XEvnUI8PWkz4Q1HDh5AXzyw31TwbcgfNNdb4yhlIvSQs+DShTwZeJDdbiUfIUHJ/xhP0c9IAju/wGSL8I3XeHiX1bHVErdIi30PCq8uD+fPRBNwl8X6TtxDecvpdhvXdfja/uB0in3692OlHIxWuh5WN3QInzQswabE05dXXI3pDF0+gj2LYfp/fUcdaVciBZ6HteqSgle71KN3/48zlNTN3Il1UC1btBqNGydab+FnUXr/Silbo2etqjoHl2aUxcu8+rc7fj7ePLavVWR+oPh3FH4fSzkLw5NnrE6plLqJrTQFQADGpfnzMUUPlgSj5+3J/+9pxLS4mU4nwhLXrHfKKN2P6tjKqVuQAtd/WNIq3DOXUrhixV7yO9j46lWFaHD+/YzXuYMAd8C9nXVlVK5kha6+oeI8OI9lblwOYWxi+Px8bLxaLMK0G0ifN0VfnwYfPz1alKlcik9KKoy8PAQXru3Gp2ql+TNBTv4+Ndd4JUPek62X0069QHY85vVMZVSWdBCV9eweQhvdYuifVRJXp+3nc9/2w2+AXD/dChY1n7XowN/WB1TKZWJFrrKkqfNg3e7R9GuahCvzNnGZ8t2w11F4MFZ4F8cvu4Ch9ZbHVMplY4WurouT5sH/+tRnXbVghg9dxsfLo0H/xLwwCz7ui+TOsORLVbHVEql0UJXN+Rl8+C9f1enY/WSvDF/B+8t/BMKlrZ/UvfMB191gKNxVsdUSqGFrhzgafPgne7VubdmKd5duJMx87djCpWDPj+BzQe+bA9Ht1odU6k8TwtdOcTmIbzVNYqedcrw0dJdvDgzjtRCoWml7q2lrlQuoIWuHObhIbzaOZKHG4cyadU+hny/keSCIdBnDti8YGI7vUGGUhbSQle3REQY2iaCp1uF8+P6gwyctJaL/uWg71zwvsv+Sf3gWqtjKpUnaaGrWyYiDP5XGKM6RbJ4xzHuH7+aU77B9lLPVwi+6gT7V1kdU6k8Rwtd3bbe9coy7r6abE44TbePV3KIQOgzF/IXs5/SGL/I6ohK5Sla6OqOtK0axMSHanPkdBKdP1zB1vP+0HceFC5vv6J060yrIyqVZ2ihqztWv3xRvn/kbjxE6P7JSn49JNBnNpSsAd/3gXWTrI6oVJ6gha6cIqJEAD8OakDpwn48NHENkzefhQdmQEgTmDUYfntb73ykVDbTQldOU6KAL1MfrkfDCkV5fvpmRv28jys9v4PIrrBoJMwfCqmpVsdUym05VOgiEiMiO0QkXkSGZjH+lIhsFZFNIrJIRMo6P6pyBf6+Xox/MJo+9csxfvke+n+zibPtPoR6g2D1xzCtHyQnWR1TKbd000IXERswDmgDVAZ6ikjlTNPWA9HGmGrAD8Abzg6qXIenzYMRHarwSqdIft2ZyL0frWJvrWHQchTETYdJneDCSatjKuV2HPmEXgeIN8bsNsZcBqYAHdNPMMYsMcZcSHu4Cgh2bkzliu6vV5ZJD9Xh+LlLdBi3gl8De0LXCXBwHYxvCSf3WB1RKbfiSKGXAg6ke5yQ9tz19APmZTUgIgNEJFZEYhMTEx1PqVxW/QpFmTW4ISUL5qPvhD/48Hg1zAMz4MIJ+Lw57FtpdUSl3IZTD4qKyP1ANPBmVuPGmE+NMdHGmOjAwEBnvrXKxUoX9mP6oPq0rRrEG/N38PCv3pzrPT/tqtIOsGGy1RGVcguOFPpBoHS6x8Fpz2UgIi2AYUAHY8wl58RT7sLP25P3e9bgv/dUZvH2Y9zzzWF2tP8RytSDGQPhl5cg9YrVMZVyaY4U+hogTERCRMQb6AHMSj9BRGoAn2Av82POj6ncgYjQr2EIkwfU48LlK3T4PI7vKv4PU6svrPgfTO4BF09ZHVMpl3XTQjfGpACDgQXANmCqMSZOREaKSIe0aW8C+YHvRWSDiMy6zsspRe1yhZnzeCPqhBTmuRnb+c+5B0lq/RbsWmzfr564w+qISrkkMRZdvRcdHW1iY2MteW+VO6SmGj5cGs87v+ykTGE/Pm9ymQpLB0HKJeg0Dip3vPmLKJXHiMhaY0x0VmN6paiyjIeHfRneKQPu5nJKKjEzrvBVta8wRcNh6gPw83C4kmJ1TKVchha6slydkMLM+09jWlUpzotLT/GAGcG5ag/C7+/bz4I5c9jqiEq5BC10lSsU8PNi3H01eaNLNdYdvMDdG9uxpsZrmEPr4eOGEL/Q6ohK5Xpa6CrXEBG61y7NvP80pmIJf7qtLMvIoA9I8QuEr7vAwpfhSrLVMZXKtbTQVa5Tpogf3z18N8/FRPDNLj8anxzO/nJdYfk78EUMnNxtdUSlciUtdJUr2TyER5qW56fHGxJYuCCNt9/LZyVeJPX4n/BxI9jwra6vrlQmWugqVwsv7s+0R+rzbExF3kyoTOtLr3EsfwTMeASm9obzx62OqFSuoYWucj1PmweDmlZg3n8aUSgolHqHnuCbgP/D7FgAH9aD7XOsjqhUrqCFrlxG+cD8TOlfj9H3RvH6mVa0v/QKxygEU+6D6QN0jXWV52mhK5fi4SH0rFOGRUOaUL5qHRqcGM5Er3+TunkajKsDcTOsjqiUZbTQlUsq5u/Lez1qMKFfA77yuY+2Sa+wN7kQfP8gTL4PTidYHVGpHKeFrlxaw7CizH+iMR1bt6J90gjeuNKL5D8XYcbVgVUf6dIBKk/RQlcuz9vTg0ealufnIf8ioXJ/ml0cw8rkcJg/FPNpE9i/yuqISuUILXTlNoIK5GNszxq8N7ADYwqPYtDlxzmeeAS+aA0/DoSzR62OqFS20kJXbqdW2cL8+GhDWnZ7mJ5eY/kwpQMpG7/nytiasPxdSE6yOqJS2UILXbklDw+hc41gfno6Bmkxgk7mLRYnVYSFI0h5vzZsma5Xmiq3o4Wu3Jqvl41HmpZn0rO9+KPeB/S5Moz408APfUn+pBnsXW51RKWcRu9YpPKUQ6cuMvaX7aRunMKTtqkEyUkuhzTHu+WLULK61fGUuqkb3bFIC13lSXuPn+ejX7ZQKG4CA22zKSjnSAq7B9+Ww6FYJavjKXVdWuhKXUf8sXN8sXADxbeOp59tHvnlIhfD7iFf8+ehRKTV8ZS6hha6Ujex5/h5Jv6ylmJbx/OgxwLyy0XOhcSQv/kzEJzl3x2lLKGFrpSDDp66yKTFG8i/4TPul/kUlPOcKVEP/38NQcJagojVEVUep4Wu1C06ce4SU1ZsI2nVF9yXOpsgOckZ//L4NXoMzxo9wCuf1RFVHqWFrtRtunj5CtNj97Bv2dd0vDCdKh77uOhZAFPzQfzq94eCZayOqPIYLXSl7lBqquHXncdYuWgGNY98T0uPWESEM2WaU6DRw0j5f4GHzeqYKg/QQlfKiXYlnmPWr6vx3/wVnVhMUTnDOd+SeEY/gG/t3lAg2OqIyo1poSuVDc5fSmH2un3sWTGVRqdn09AWRyrCmaAGBNR7EI9K94C3n9UxlZvRQlcqm21OOM385SvJv+172rOUYDnOZZsfyRXaclftnhDSFGyeVsdUbkALXakccvHyFeZvOciW3+cRdnQebT1WEyAXSPIqBJU74BvVBco20HJXt00LXSkLHDp1kdlrd3M4djY1zy2lucc67pJLXPIqiES0wzuyA4Q2BS9fq6MqF3LHhS4iMcB7gA343BjzeqZxH+AroBZwAvi3MWbvjV5TC13lFcYYth85y5y1u/lr40/UTlpBc4/1+MtFUmz5SA1pgnelthDWEgJKWh1X5XJ3VOgiYgN2Ai2BBGAN0NMYszXdnEFANWPMQBHpAXQ2xvz7Rq+rha7yImMMGw6cYv7G/SRu+oWoi6toYVtPKTkOwOUilfGOaAnlm0PpuvrpXV3jTgv9bmCEMaZ12uPnAYwxr6WbsyBtzkoR8QSOAIHmBi+uha7yOmMMcYfOMH/zYXZuXkXIqVU08dhIbdtOvEgh1eYDZesjxSO5mK8ERyWQ86JXqLqDYiFVKVYq5La+90aF7siRmVLAgXSPE4C615tjjEkRkdNAEeB4piADgAEAZcroFXYqbxMRIksVILJUAYiJYN+JbizcdozP4/bgsf937k7ZQqNdcZTdvRw/krm9v/4qN1pdeTjFuj/j9NfN0UPtxphPgU/B/gk9J99bqdyubJG76NcwhH4NQzib1IgV8Sf4Kj4RMVAx4BIVfE4TYLtsdUzlBKFls2fNfUcK/SBQOt3j4LTnspqTkLbLpQD2g6NKqdvg7+tFTGQJYiJLWB1FuRBH7im6BggTkRAR8QZ6ALMyzZkFPJj2dVdg8Y32nyullHK+m35CT9snPhhYgP20xS+MMXEiMhKINcbMAsYDk0QkHjiJvfSVUkrlIIf2oRtj5gJzMz33Yrqvk4Buzo2mlFLqVjiyy0UppZQL0EJXSik3oYWulFJuQgtdKaXchBa6Ukq5CcuWzxWRRGDfbX57UTItK5AH6DbnDbrNecOdbHNZY0xgVgOWFfqdEJHY6y1O4650m/MG3ea8Ibu2WXe5KKWUm9BCV0opN+Gqhf6p1QEsoNucN+g25w3Zss0uuQ9dKaXUtVz1E7pSSqlMtNCVUspN5OpCF5EYEdkhIvEiMjSLcR8R+S5tfLWIlLMgplM5sM1PichWEdkkIotEpKwVOZ3pZtucbl4XETEi4vKnuDmyzSLSPe1nHSci3+Z0Rmdz4M92GRFZIiLr0/58t7Uip7OIyBcickxEtlxnXERkbNrvxyYRqXnHb2qMyZW/sK+9vgsIBbyBjUDlTHMGAR+nfd0D+M7q3Dmwzc0Av7SvH8kL25w2zx9YBqwCoq3OnQM/5zBgPVAo7XExq3PnwDZ/CjyS9nVlYK/Vue9wmxsDNYEt1xlvC8wDBKgHrL7T98zNn9DrAPHGmN3GmMvAFKBjpjkdgS/Tvv4BaC4ikoMZne2m22yMWWKMuZD2cBX2WwK6Mkd+zgCjgDFAUk6GyyaObHN/YJwx5i8AY8yxHM7obI5sswEC0r4uABzKwXxOZ4xZhv2GP9fTEfjK2K0CCopI0J28Z24u9FLAgXSPE9Key3KOMSYFOA0UyZF02cORbU6vH/Z/4V3ZTbc57b+ipY0xc3IyWDZy5OccDoSLyAoRWSUiMTmWLns4ss0jgPtFJAH7DXUey5lolrnVv+835dAdi1TuIyL3A9FAE6uzZCcR8QDeAfpYHCWneWLf7dIU+//ClolIVWPMKStDZbOewERjzNsicjf221pGGmNSrQ7mKnLzJ/SDQOl0j4PTnstyjoh4Yv9v2okcSZc9HNlmRKQFMAzoYIy5lEPZssvNttkfiASWishe7PsaZ7n4gVFHfs4JwCxjTLIxZg+wE3vBuypHtrkfMBXAGLMS8MW+iJW7cujv+63IzYW+BggTkRAR8cZ+0HNWpjmzgAfTvu4KLDZpRxtc1E23WURqAJ9gL3NX368KN9lmY8xpY0xRY0w5Y0w57McNOhhjYq2J6xSO/Nmegf3TOSJSFPsumN05mNHZHNnm/UBzABGphL3QE3M0Zc6aBTyQdrZLPeC0MebwHb2i1UeCb3KUuC32Tya7gGFpz43E/hca7D/w74F44A8g1OrMObDNC4GjwIa0X7Oszpzd25xp7lJc/CwXB3/Ogn1X01ZgM9DD6sw5sM2VgRXYz4DZALSyOvMdbu9k4DCQjP1/XP2AgcDAdD/jcWm/H5ud8edaL/1XSik3kZt3uSillLoFWuhKKeUmtNCVUspNaKErpZSb0EJXSik3oYWulFJuQgtdKaXcxP8DJ6fEskzptYAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_an, x_test, t_test = analytical(N_x_test, N_t_test, x_l, x_r, t_i, 1, k1, k2)\n",
    "x_test = np_to_torch(x_test)\n",
    "t_test = np_to_torch(t_test)\n",
    "y_pred,_,_,s_pred,_,_,_ = model(x_test, t_test)\n",
    "x_test = x_test.detach().numpy()\n",
    "y_pred = y_pred.detach().numpy()\n",
    "s_pred = s_pred.detach().numpy()\n",
    "\n",
    "for i in range(N_x_test*N_t_test):\n",
    "    if (s_pred[i]<x_test[i]):\n",
    "        y_pred[i] = 0\n",
    "        \n",
    "# \n",
    "j = 99\n",
    "\n",
    "plt.plot(x_test[j*N_t_test:(j+1)*N_t_test], y_pred[j*N_t_test:(j+1)*N_t_test])\n",
    "plt.plot(x_test[j*N_t_test:(j+1)*N_t_test], y_an[j*N_t_test:(j+1)*N_t_test])\n",
    "plt.legend([\"PINN\", \"Analytical\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2843e3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class weightedTanh(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         ex = torch.exp(0.5*input)\n",
    "#         return 1-2*((ex-1)/(ex+1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89e9a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Training:\n",
    "#     def __init__(self, model, optimiser1, optimiser2, epochs, T_r, T_l, k1, N_x, x_l, x_r, N_t, N_bc, N_ic, t_arr, t_i, T_ini):\n",
    "        \n",
    "#         self.model = model\n",
    "#         self.epochs = epochs\n",
    "#         self.T_r = T_r\n",
    "#         self.T_l = T_l\n",
    "#         self.k1 = k1\n",
    "#         self.N_x = N_x\n",
    "#         self.x_l = x_l\n",
    "#         self.x_r = x_r\n",
    "#         self.N_t = N_t\n",
    "#         self.N_bc = N_bc\n",
    "#         self.N_ic = N_ic\n",
    "#         self.t_arr = t_arr\n",
    "#         self.t_i = t_i\n",
    "#         self.T_ini = T_ini\n",
    "#         self.w1 = 1\n",
    "#         self.w2 = 1\n",
    "#         self.w3 = 1\n",
    "#         self.w4 = 1\n",
    "#         self.N_tot = self.N_x*self.N_t + 2*self.N_bc + self.N_ic\n",
    "#         self.null = torch.zeros(self.N_tot, 1)\n",
    "#         self.mse = nn.MSELoss(reduction='sum')\n",
    "        \n",
    "#     def x_train_data(self):\n",
    "        \n",
    "#         x_train = np.linspace(self.x_l, self.x_r, self.N_x)\n",
    "#         x_train = np.tile(x_train, N_t)\n",
    "#         x_bc1 = np.zeros(self.N_bc)\n",
    "#         x_bc2 = np.ones(self.N_bc)*self.x_r\n",
    "#         x_ic = np.random.uniform(low=self.x_l, high=self.x_r, size=(self.N_ic,))\n",
    "#         x_train = np.concatenate((x_train,x_bc1,x_bc2,x_ic),0)\n",
    "#         x_train = torch.FloatTensor(x_train)\n",
    "#         x_train = x_train.unsqueeze(-1)\n",
    "#         x_train = x_train.clone().detach().requires_grad_(True)\n",
    "#         N_xl = self.mse( torch.where(x_train == self.x_l,1,0), self.null ).detach().numpy().item()\n",
    "#         N_xr = self.mse( torch.where(x_train == self.x_r,1,0), self.null ).detach().numpy().item()\n",
    "    \n",
    "#         return x_train, N_xl, N_xr, a, b\n",
    "    \n",
    "#     def t_train_data(self, t_start, t_end):\n",
    "        \n",
    "#         t_train = np.linspace(t_start, t_end, self.N_t)\n",
    "#         t_train = np.repeat(t_train, self.N_x)\n",
    "#         t_bc1 = np.random.uniform(low = t_start, high= t_end, size=(self.N_bc,))\n",
    "#         t_bc2 = np.random.uniform(low = t_start, high= t_end, size=(self.N_bc,))\n",
    "#         t_ic = np.zeros(self.N_ic)\n",
    "#         t_train = np.concatenate((t_train,t_bc1,t_bc2,t_ic),0)\n",
    "#         t_train = torch.FloatTensor(t_train)\n",
    "#         t_train = t_train.unsqueeze(-1)\n",
    "#         t_train = t_train.clone().detach().requires_grad_(True)\n",
    "    \n",
    "#         return t_train, \n",
    "    \n",
    "#     def get_loss(x_train, t_train, k1, N_tot, N_ic, a, b, T_l, T_r, N_xl, N_xr, x_l, x_r, T_ini, null):\n",
    "\n",
    "#         T, dTdt, d2Tdx2 = model(x_train, t_train)\n",
    "#         eq1 = self.w1*mse(dTdt, self.k1*d2Tdx2)/(self.N_tot)\n",
    "#         ic = self.w2*( mse( torch.mul(a, T - T_l), null ) + mse( torch.mul(b, T - T_ini), null ) )/(N_ic)\n",
    "#         bc1 = w3*mse( torch.mul(torch.where(x_train == x_l,1,0),(T - T_l)), null )/(N_xl)\n",
    "#         bc2 = w4*mse( torch.mul(torch.where(x_train == x_r,1,0),(T - T_r)), null )/(N_xr)\n",
    "#         loss = eq1 + bc1 + bc2 + ic\n",
    "    \n",
    "#         return loss, eq1, bc1, bc2, ic\n",
    "    \n",
    "#     def abcd(self):\n",
    "#         print(self.t_arr[0])\n",
    "#         print(self.t_arr[1])\n",
    "#         t_train = self.t_train_data(self.t_arr[0], self.t_arr[1])\n",
    "#         return t_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d82fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boom = Training(model, optimiser1, optimiser2, epochs, T_r, T_l, k1, N_x, x_l, x_r, N_t, N_bc, N_ic, t_arr, t_i, T_ini)\n",
    "# t_train = boom.abcd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
