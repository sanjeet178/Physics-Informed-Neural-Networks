{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f34de8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.init as init\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f0ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_torch(arr):\n",
    "    \n",
    "    arr = torch.FloatTensor(arr)\n",
    "    arr = arr.unsqueeze(-1)\n",
    "    arr = arr.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def x_train_data(N_tot, x_l, x_r):\n",
    "    \n",
    "    x_train = np.linspace(x_l,x_r,N_tot)\n",
    "    x_train = np_to_torch(x_train)\n",
    "   \n",
    "    return x_train\n",
    "\n",
    "def initial_temp(N_tot, T_l, T_r):\n",
    "    \n",
    "    T_prev = np.ones(N_tot)*T_r\n",
    "    T_prev[0] = T_l\n",
    "    T_prev = np_to_torch(T_prev)\n",
    "    \n",
    "    return T_prev\n",
    "\n",
    "def initial_fraction(N_tot, s_initial, x_train):\n",
    "    \n",
    "    x_train = x_train.detach().numpy()\n",
    "    f_prev = np.zeros(N_tot)\n",
    "    for i in range(N_tot):\n",
    "        if x_train[i]<=s_initial:\n",
    "            f_prev[i] = 1\n",
    "    f_prev = np_to_torch(f_prev)\n",
    "    \n",
    "    return f_prev\n",
    "            \n",
    "def xavier_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0)\n",
    "    \n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, layer_size):\n",
    "        super(ANN, self).__init__()\n",
    "        \n",
    "        # Fully conected model-1\n",
    "        modules_1 = []\n",
    "        for i in range(len(layer_size) - 1):\n",
    "            modules_1.append(nn.Linear(layer_size[i], layer_size[i+1]))  \n",
    "            if i<len(layer_size) - 2:\n",
    "                modules_1.append(nn.Tanh())\n",
    "        \n",
    "        modules_1.append(nn.CELU())\n",
    "        self.fc_1 = nn.Sequential(*modules_1)\n",
    "        self.fc_1.apply(xavier_init)\n",
    "        \n",
    "        # Fully conected model-2\n",
    "        modules_2 = []\n",
    "        for i in range(len(layer_size) - 1):\n",
    "            modules_2.append(nn.Linear(layer_size[i], layer_size[i+1])) \n",
    "            if i < len(layer_size) - 2 :\n",
    "                modules_2.append(nn.Tanh())\n",
    "        \n",
    "#         modules_2.append(nn.ReLU())\n",
    "        self.fc_2 = nn.Sequential(*modules_2)\n",
    "        self.fc_2.apply(xavier_init)\n",
    "        \n",
    "    def forward(self, x_train):\n",
    "        \n",
    "        T = self.fc_1( x_train )\n",
    "        dTdx = torch.autograd.grad(T, x_train, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "        d2Tdx2 = torch.autograd.grad(dTdx, x_train, grad_outputs=torch.ones_like(dTdx), create_graph=True)[0]\n",
    "        \n",
    "        f = self.fc_2( x_train )\n",
    "        dfdx = torch.autograd.grad(f, x_train, grad_outputs=torch.ones_like(f), create_graph=True)[0]\n",
    "        df2dx2 = torch.autograd.grad(dfdx, x_train, grad_outputs=torch.ones_like(dfdx), create_graph=True)[0]\n",
    "        \n",
    "        return T, dTdx, d2Tdx2, f\n",
    "    \n",
    "def get_loss(x_train, k1, k2, T_l, T_r, x_l, x_r, t_i, w1, w2, w3, w4, w5, T_prev, f_prev, f_lim, N_x, mat_1, mat_2, N_1, N_2):\n",
    "\n",
    "    T, dTdx, d2Tdx2, f = model(x_train)\n",
    "\n",
    "    mat_1 = torch.where(f >= f_lim,1,0)\n",
    "    N_1 = torch.sum( mat_1 )\n",
    "    mat_2 = torch.where(f < f_lim,1,0)\n",
    "    N_2 = torch.sum( mat_2 )\n",
    "    \n",
    "    eq1 = w1*torch.sum( torch.square( torch.mul(mat_1, T - T_prev - del_t*k1*d2Tdx2) ) )/(N_1)\n",
    "    eq2 = w2*torch.sum( torch.square( torch.mul(mat_2, f - f_prev - del_t*k2*d2Tdx2 ) ) )/(N_2)\n",
    "    bc1 = w3*torch.sum( torch.square( torch.mul(torch.where(x_train == 0,1,0), T - T_l ) ) )\n",
    "    bc2 = w4*torch.sum( torch.square( torch.mul(mat_2, T - T_r  ) ) )/(N_2)  \n",
    "    bc3 = w2*torch.sum( torch.square( torch.mul(mat_2, T - T_prev - del_t*k1*d2Tdx2) ) )/(N_2)\n",
    "    \n",
    "    if (N_1 == 0):\n",
    "        eq1 = 0\n",
    "    if (N_2 == 0):\n",
    "        eq2 = 0\n",
    "        bc3 = 0\n",
    "\n",
    "    loss = eq1 + eq2 + bc1 + bc2 \n",
    "    \n",
    "    return loss, eq1, eq2, bc1, bc2, bc3\n",
    "\n",
    "def print_loss(epoch, loss, eq1, eq2, bc1, bc2, bc3):\n",
    "    print('epoch = ',epoch)\n",
    "    print('loss = ',loss)\n",
    "    print('eq1_loss = ',eq1)\n",
    "    print('eq2_loss = ',eq2)\n",
    "    print('bc1_loss = ',bc1)\n",
    "    print('bc2_loss = ',bc2)\n",
    "#     print('bc3_loss = ',bc3)\n",
    "    \n",
    "def lambda_analytical(k1, k2):\n",
    "    x = []\n",
    "    er = []\n",
    "    cnt = 0\n",
    "    for i in np.arange(0.1, 5, 0.001):\n",
    "        x.append(i)\n",
    "        er.append(math.erf(x[-1]))\n",
    "        cnt = cnt+1\n",
    "\n",
    "    x = np.array(x)\n",
    "    er = np.array(er)\n",
    "    y =[]\n",
    "    y = np.exp(-x*x)/(er*math.sqrt(math.pi))-x*k1/(k2)\n",
    "\n",
    "    for i in range(1,cnt):\n",
    "        if(y[i]*y[i-1]<0):\n",
    "            lam = x[i]\n",
    "            break\n",
    "    \n",
    "    return lam\n",
    "\n",
    "def analytical(N_x_test, x_test, t_test, T_r, k1, k2, T_l):\n",
    "\n",
    "    x_test = x_test.detach().numpy()\n",
    "    y_an = np.zeros((N_x_test, 1))\n",
    "    lam = lambda_analytical(k1, k2)\n",
    "    s = np.sqrt(k1*t_test)*2*lam\n",
    "    \n",
    "    for j in range(N_x_test):\n",
    "        if(x_test[j]<s):\n",
    "            y_an[j] = T_l - T_l*math.erf( x_test[j]/( 2*np.sqrt(k1*t_test) ) )/ math.erf(lam) \n",
    "        else:\n",
    "            y_an[j] = T_r\n",
    "            \n",
    "    y_an = np.reshape(y_an, (N_x_test, 1))\n",
    "    \n",
    "    return y_an, s\n",
    "    \n",
    "def temperature_fraction_correction(f_new, f_prev, T_new, N_x, T_r):\n",
    "    \n",
    "    for i in range(N_x):\n",
    "        \n",
    "        if f_new[i][0]>=1 or f_prev[i][0]>=1:\n",
    "            f_new[i][0] = 1\n",
    "            continue\n",
    "            \n",
    "        if f_new[i][0]<0:\n",
    "            f_new[i][0] = 0\n",
    "            \n",
    "        if T_new[i][0]<0:\n",
    "            T_new[i][0] = 0\n",
    "            \n",
    "#         if f_new[i][0]<1 :\n",
    "#             T_new[i][0] = T_r\n",
    "    \n",
    "    f_new = torch.FloatTensor(f_new)  \n",
    "    T_new = torch.FloatTensor(T_new)   \n",
    "    return T_new, f_new\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6370e2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN(\n",
      "  (fc_1): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=3, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=3, out_features=1, bias=True)\n",
      "    (5): CELU(alpha=1.0)\n",
      "  )\n",
      "  (fc_2): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=3, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=3, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Total trainable parameters in the model: 44\n"
     ]
    }
   ],
   "source": [
    "N_x = 400\n",
    "N_t = 20\n",
    "\n",
    "x_l = 0\n",
    "x_r = 0.2\n",
    "\n",
    "T_r = 0\n",
    "T_l = 1\n",
    "\n",
    "t_i = 0\n",
    "\n",
    "accuracy_cap = 0.0004\n",
    "del_t = 0.0005\n",
    "s_initial = 0.015\n",
    "f_lim = 1\n",
    "\n",
    "# Neural network params\n",
    "layer_size = [1, 3, 3, 1]\n",
    "\n",
    "# material params\n",
    "k1 = 0.05\n",
    "k2 = 0.8\n",
    "\n",
    "# Training data and initial data\n",
    "model = ANN(layer_size)\n",
    "print(model)\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total trainable parameters in the model:\", total_trainable_params)\n",
    "\n",
    "# # Setup Loss function and Optimiser\n",
    "lr = 1e-4\n",
    "epochs = 120001\n",
    "optimiser1 = torch.optim.NAdam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a424493",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t =  0.0005\n",
      " \n",
      "epoch =  0\n",
      "loss =  tensor(1.0873, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  0\n",
      "eq2_loss =  tensor(0.0751, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(1., grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0122, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  2000\n",
      "loss =  tensor(0.3050, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  0\n",
      "eq2_loss =  tensor(0.0576, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.1056, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.1418, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  4000\n",
      "loss =  tensor(0.2524, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  0\n",
      "eq2_loss =  tensor(0.0553, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0478, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.1493, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  6000\n",
      "loss =  tensor(0.2489, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  0\n",
      "eq2_loss =  tensor(0.0549, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0375, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.1565, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  8000\n",
      "loss =  tensor(0.2430, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  0\n",
      "eq2_loss =  tensor(0.0534, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0360, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.1536, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  10000\n",
      "loss =  tensor(0.2139, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  0\n",
      "eq2_loss =  tensor(0.0497, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0300, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.1342, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  12000\n",
      "loss =  tensor(0.1773, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  0\n",
      "eq2_loss =  tensor(0.0436, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0211, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.1127, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  14000\n",
      "loss =  tensor(0.1434, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  0\n",
      "eq2_loss =  tensor(0.0361, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0148, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0925, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  16000\n",
      "loss =  tensor(0.1147, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  0\n",
      "eq2_loss =  tensor(0.0305, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0100, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0742, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  18000\n",
      "loss =  tensor(0.0947, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  0\n",
      "eq2_loss =  tensor(0.0251, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0063, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0633, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  20000\n",
      "loss =  tensor(0.4641, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.3072, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0209, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.1113, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0248, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  22000\n",
      "loss =  tensor(0.4167, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.2308, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0169, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.1521, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0170, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  24000\n",
      "loss =  tensor(0.3243, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.1844, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0118, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.1063, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0218, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  26000\n",
      "loss =  tensor(0.2441, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.1443, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0061, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0701, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0236, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  28000\n",
      "loss =  tensor(0.1919, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.1215, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0016, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0481, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0207, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  30000\n",
      "loss =  tensor(0.1545, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.1061, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0011, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0320, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0153, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  32000\n",
      "loss =  tensor(0.1294, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0951, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0008, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0224, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0111, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  34000\n",
      "loss =  tensor(0.1096, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0852, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0006, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0160, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0079, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  36000\n",
      "loss =  tensor(0.0915, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0746, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0110, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0054, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  38000\n",
      "loss =  tensor(0.0751, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0640, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0003, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0072, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0035, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  40000\n",
      "loss =  tensor(0.0620, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0548, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0003, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0045, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0023, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  42000\n",
      "loss =  tensor(0.0502, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0457, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0002, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0027, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0015, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  44000\n",
      "loss =  tensor(0.0411, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0383, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0002, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0015, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0010, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  46000\n",
      "loss =  tensor(0.0331, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0317, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0001, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0007, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0006, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  48000\n",
      "loss =  tensor(0.0282, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0274, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0001, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(0.0003, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0004, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  50000\n",
      "loss =  tensor(0.0253, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0249, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0001, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(6.3192e-05, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0003, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  52000\n",
      "loss =  tensor(0.0234, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0232, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(4.5625e-05, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(3.8063e-06, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0002, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  54000\n",
      "loss =  tensor(0.0227, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0225, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(4.7599e-05, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(3.2960e-07, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0001, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  56000\n",
      "loss =  tensor(0.0224, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0223, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(6.3489e-05, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(1.6136e-06, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(7.3550e-05, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  58000\n",
      "loss =  tensor(0.0256, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0227, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0028, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(9.5166e-06, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(7.2051e-05, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  60000\n",
      "loss =  tensor(0.0218, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0216, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(8.6251e-05, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(3.1001e-06, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(7.2616e-05, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  62000\n",
      "loss =  tensor(0.0214, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0211, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0001, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(3.4054e-06, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(9.2554e-05, grad_fn=<DivBackward0>)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  64000\n",
      "loss =  tensor(0.0239, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0213, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0025, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(4.7445e-06, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0001, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  66000\n",
      "loss =  tensor(0.0232, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0206, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0024, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(3.4001e-06, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0001, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  68000\n",
      "loss =  tensor(0.0223, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0199, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0023, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(2.6022e-06, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0001, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  70000\n",
      "loss =  tensor(0.0214, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0190, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0022, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(1.0875e-06, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0001, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  72000\n",
      "loss =  tensor(0.0202, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0180, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0022, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(1.2447e-06, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(8.8578e-05, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  74000\n",
      "loss =  tensor(0.0192, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0170, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0021, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(2.0006e-06, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0001, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  76000\n",
      "loss =  tensor(0.0183, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0162, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0020, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(1.3551e-06, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0001, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  78000\n",
      "loss =  tensor(0.0174, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0154, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0019, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(5.4821e-07, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0001, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  80000\n",
      "loss =  tensor(0.0166, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0147, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0018, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(3.6327e-07, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0001, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  82000\n",
      "loss =  tensor(0.0159, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0140, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0017, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(3.0490e-07, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0002, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  84000\n",
      "loss =  tensor(0.0134, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0130, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0003, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(1.0857e-07, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0001, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  86000\n",
      "loss =  tensor(0.0147, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0127, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0018, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(1.1090e-06, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0002, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  88000\n",
      "loss =  tensor(0.0120, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0116, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0002, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(3.6991e-07, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0001, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  90000\n",
      "loss =  tensor(0.0112, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0109, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0001, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(5.6224e-09, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0001, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  92000\n",
      "loss =  tensor(0.0104, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0102, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(8.0221e-05, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(6.4111e-08, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0001, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  94000\n",
      "loss =  tensor(0.0121, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0098, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0022, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(3.0913e-07, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(0.0001, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  96000\n",
      "loss =  tensor(0.0089, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0088, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(3.5196e-05, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(2.7989e-07, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(8.5823e-05, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  98000\n",
      "loss =  tensor(0.0081, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0080, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(2.4267e-05, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(2.6129e-07, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(6.4756e-05, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  100000\n",
      "loss =  tensor(0.0101, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0076, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0025, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(2.2341e-07, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(5.4110e-05, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  102000\n",
      "loss =  tensor(0.0067, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0067, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(1.2790e-05, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(1.5223e-06, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(3.2411e-05, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  104000\n",
      "loss =  tensor(0.0061, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0060, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(8.9284e-06, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(1.7378e-07, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(2.6523e-05, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  106000\n",
      "loss =  tensor(0.0055, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0055, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(7.6317e-06, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(1.2133e-07, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(2.5481e-05, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  108000\n",
      "loss =  tensor(0.0050, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0050, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(7.2974e-06, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(9.1394e-08, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(2.9941e-05, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  110000\n",
      "loss =  tensor(0.0045, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0045, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(1.0907e-05, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(2.7102e-08, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(3.9176e-05, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  112000\n",
      "loss =  tensor(0.0041, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0041, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(1.5240e-05, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(5.1788e-08, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(5.1771e-05, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  114000\n",
      "loss =  tensor(0.0038, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0037, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(2.0952e-05, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(5.7663e-09, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(6.3835e-05, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  116000\n",
      "loss =  tensor(0.0035, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0034, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(2.8706e-05, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(1.7226e-08, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(7.6560e-05, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  118000\n",
      "loss =  tensor(0.0033, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0031, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(3.7199e-05, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(4.3458e-09, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(8.6609e-05, grad_fn=<DivBackward0>)\n",
      "\n",
      "epoch =  120000\n",
      "loss =  tensor(0.0030, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.0029, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(4.4043e-05, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(3.3704e-09, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(9.3071e-05, grad_fn=<DivBackward0>)\n",
      "\n",
      "t =  0.001\n",
      " \n",
      "epoch =  0\n",
      "loss =  tensor(0.1545, grad_fn=<AddBackward0>)\n",
      "eq1_loss =  tensor(0.1462, grad_fn=<DivBackward0>)\n",
      "eq2_loss =  tensor(0.0082, grad_fn=<DivBackward0>)\n",
      "bc1_loss =  tensor(6.9832e-09, grad_fn=<MulBackward0>)\n",
      "bc2_loss =  tensor(9.2354e-05, grad_fn=<DivBackward0>)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13808/2839920935.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0moptimiser1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0moptimiser1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mloss_store\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_store = []\n",
    "T_store_an = []\n",
    "T_store_pred = []\n",
    "f_store_pred = []\n",
    "t_store = []\n",
    "s_store_an = []\n",
    "model.train()  \n",
    "\n",
    "# Initial conditions\n",
    "x_train = x_train_data(N_x, x_l, x_r)\n",
    "T_prev = initial_temp(N_x, T_l, T_r)\n",
    "f_prev = initial_fraction(N_x, s_initial, x_train)\n",
    "t_test = 0\n",
    "T_store_pred.append(T_prev)\n",
    "f_store_pred.append(f_prev)\n",
    "\n",
    "# Loss function weights\n",
    "w1 = 1\n",
    "w2 = 1\n",
    "w3 = 1\n",
    "w4 = 1\n",
    "w5 = 1\n",
    "\n",
    "for i in range(N_t):\n",
    "\n",
    "    t_test = t_test + del_t\n",
    "    t_store.append(t_test)\n",
    "    print(\"t = \", t_test)\n",
    "    print(\" \")\n",
    "\n",
    "    mat_1 = torch.where(f_prev >= f_lim,1,0)\n",
    "    N_1 = torch.sum( mat_1 )\n",
    "    mat_2 = torch.where(f_prev < f_lim,1,0)\n",
    "    N_2 = torch.sum( mat_2 )\n",
    "\n",
    "    for epoch in range(epochs):        \n",
    "        #Backpropogation and optimisation\n",
    "        loss, eq1, eq2, bc1, bc2, bc3 = get_loss(x_train, k1, k2, T_l, T_r, x_l, x_r, t_i, \n",
    "                                            w1, w2, w3, w4, w5, T_prev, f_prev, f_lim, N_x, mat_1, mat_2, N_1, N_2)\n",
    "        \n",
    "        optimiser1.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser1.step()  \n",
    "        loss_store.append(loss.detach().numpy())\n",
    "\n",
    "        if epoch%2000==0:\n",
    "            print_loss(epoch, loss, eq1, eq2, bc1, bc2, bc3)\n",
    "            print(\"\")\n",
    "            \n",
    "        if loss.detach().numpy()<=8e-5:\n",
    "            print(\"Loss limit attained, epoch = \",epoch)\n",
    "            break\n",
    "\n",
    "    # Store the results after each time step\n",
    "    T_prev, dTdx, d2Tdx2, f_prev = model(x_train)\n",
    "    T_an, s_an = analytical(N_x, x_train, t_test, T_r, k1, k2, T_l)\n",
    "\n",
    "    T_prev, f_prev = temperature_fraction_correction(f_prev.detach().numpy(), f_store_pred[-1], T_prev.detach().numpy(), N_x, T_r)\n",
    "\n",
    "    T_store_pred.append(T_prev.detach().numpy())\n",
    "    T_store_an.append(T_an)\n",
    "    s_store_an.append(s_an)\n",
    "    f_store_pred.append(f_prev.detach().numpy())\n",
    "    \n",
    "    epochs = int(epochs*0.9)\n",
    "\n",
    "    T_prev = torch.FloatTensor(T_store_pred[-1]).clone().detach().requires_grad_(False)\n",
    "    f_prev = torch.FloatTensor(f_store_pred[-1]).clone().detach().requires_grad_(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657c15a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = N_x\n",
    "k = 0\n",
    "plt.plot(x_train[i:j].detach().numpy(), T_store_pred[k+1][i:j])\n",
    "plt.plot(x_train[i:j].detach().numpy(), T_store_an[k][i:j])\n",
    "plt.plot(x_train[i:j].detach().numpy(), f_store_pred[k+1][i:j])\n",
    "Title = \"Time = \" + str( \"{:.3f}\".format (t_store[k]))\n",
    "plt.title(Title)\n",
    "plt.legend([\"PINN\", \"Analytical\", \"Fraction\"])\n",
    "plt.xlim(x_l-0.01, x_r+0.01)\n",
    "plt.ylim(-0.02, 1.055)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc92a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_store_pred[k+1][i:j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dd60f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab247fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
