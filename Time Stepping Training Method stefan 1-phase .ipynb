{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4ac27d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.init as init\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8819113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_torch(arr):\n",
    "    \n",
    "    arr = torch.FloatTensor(arr)\n",
    "    arr = arr.unsqueeze(-1)\n",
    "    arr = arr.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def x_train_data(N_x, x_l, x_r, s, N_bc):\n",
    "\n",
    "    x_start = np.zeros(1)\n",
    "    x_train = np.linspace(x_l, x_r, N_x-1)\n",
    "    x_bc1 = np.ones(N_bc)*x_l\n",
    "    x_bc2 = np.ones(N_bc)*s[0][0]\n",
    "    x_train = np.concatenate((x_start,x_train,x_bc1,x_bc2),0)\n",
    "    x_train = np_to_torch(x_train)\n",
    "    N_xl = torch.sum( torch.where(x_train == x_l,1,0) ).detach().numpy().item()\n",
    "    N_xr = torch.sum( torch.where(x_train == s[0][0],1,0) ).detach().numpy().item()\n",
    "    \n",
    "    return x_train, N_xl, N_xr\n",
    "\n",
    "def initial_temp(N_x, N_bc, T_l, T_r):\n",
    "\n",
    "    n_tp = 10\n",
    "    T_tp = np.geomspace(T_l, 0.01, n_tp)\n",
    "    T_prev_1 = np.zeros(N_x-n_tp)\n",
    "    T_prev_2 = np.ones(N_bc)*T_l\n",
    "    T_prev_3 = np.ones(N_bc)*T_r\n",
    "    T_prev = np.concatenate((T_tp, T_prev_1, T_prev_2, T_prev_3),0)\n",
    "    T_prev = np_to_torch(T_prev)\n",
    "    \n",
    "    return T_prev\n",
    "\n",
    "def x_test_data(N_x_test, x_l, x_r):\n",
    "\n",
    "    x_test = np.linspace(x_l, x_r, N_x_test)\n",
    "    x_test_np = x_test.reshape(N_x_test,1)\n",
    "    x_test_torch = np_to_torch(x_test)\n",
    "    \n",
    "    return x_test_np, x_test_torch\n",
    "            \n",
    "def xavier_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0.05)\n",
    "    \n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, layer_size):\n",
    "        super(ANN, self).__init__()\n",
    "        \n",
    "        # Fully conected model\n",
    "        modules = []\n",
    "        for i in range(len(layer_size) - 1):\n",
    "            modules.append(nn.Linear(layer_size[i], layer_size[i+1]))  \n",
    "            modules.append(nn.Tanh())\n",
    "\n",
    "        self.fc = nn.Sequential(*modules)\n",
    "        self.fc.apply(xavier_init)\n",
    "        \n",
    "    def forward(self, x_train, s):\n",
    "        op = self.fc( x_train )\n",
    "        op_x = torch.autograd.grad(op, x_train, grad_outputs=torch.ones_like(op), create_graph=True)[0]\n",
    "        op_x2 = torch.autograd.grad(op_x, x_train, grad_outputs=torch.ones_like(op_x), create_graph=True)[0]\n",
    "        Ts = self.fc( s )\n",
    "        Ts_x = torch.autograd.grad(Ts, s, grad_outputs=torch.ones_like(Ts), create_graph=True)[0]\n",
    "        \n",
    "        return op, op_x2, Ts, Ts_x\n",
    "    \n",
    "def get_loss(x_train, k1, N_tot, T_l, T_r, N_xl, N_xr, x_l, x_r, T_prev, del_t, s):\n",
    "    \n",
    "    mse = nn.MSELoss(reduction='sum')\n",
    "    w1 = 1\n",
    "    w2 = 1\n",
    "    w3 = 1\n",
    "    T, d2Tdx2, Ts, Ts_x = model(x_train, s)\n",
    "    s_np = s.detach().numpy()\n",
    "    eq1 = w1*torch.sum( torch.square(  T - T_prev - del_t*k1*d2Tdx2 ) ) /N_tot\n",
    "    bc1 = w2*torch.sum( torch.square( torch.mul(torch.where(x_train == x_l,1,0),(T - T_l)) ) )/(N_xl)\n",
    "    bc2 = w3*torch.sum( torch.square( T_r - Ts ) )/(N_xl)\n",
    "    loss = eq1 + bc1 + bc2\n",
    "    \n",
    "    return loss, eq1, bc1, bc2\n",
    "\n",
    "def print_loss(epoch, loss, eq1, bc1, bc2):\n",
    "    print('epoch = ',epoch)\n",
    "    print('loss = ',loss.detach().numpy())\n",
    "    print('eq1_loss = ',eq1.detach().numpy())\n",
    "    print('bc1_loss = ',bc1.detach().numpy())\n",
    "    print('bc2_loss = ',bc2.detach().numpy())\n",
    "\n",
    "def L2_err(N_x_test, x_test, y_an, model, s, T_r):\n",
    "    \n",
    "    y_pred,_,_,_  = model(x_test, s)\n",
    "    y_pred = y_pred.detach().numpy()\n",
    "    s = s.detach().numpy()\n",
    "    x_test = x_test.detach().numpy()\n",
    "    \n",
    "    for i in range(N_x_test):\n",
    "        if s[0]<x_test[i]:\n",
    "            y_an[i][0] = T_r\n",
    "            y_pred[i][0] = T_r\n",
    "            continue\n",
    "\n",
    "    L2_err =  np.sum((y_an - y_pred)**2)/N_x_test\n",
    "    return L2_err\n",
    "\n",
    "def lamb_analytical(k1, k2):\n",
    "    x = []\n",
    "    er = []\n",
    "    cnt = 0\n",
    "    for i in np.arange(0.1, 5, 0.001):\n",
    "        x.append(i)\n",
    "        er.append(math.erf(x[-1]))\n",
    "        cnt = cnt+1\n",
    "\n",
    "    x = np.array(x)\n",
    "    er = np.array(er)\n",
    "    y =[]\n",
    "    y = np.exp(-x*x)/(er*math.sqrt(math.pi))-x*k1/k2\n",
    "\n",
    "    for i in range(1,cnt):\n",
    "        if(y[i]*y[i-1]<0):\n",
    "            lam = x[i]\n",
    "            break\n",
    "    \n",
    "    return lam\n",
    "\n",
    "def analytical(N_x_test, x_test, t_test, T_r, k1, k2):\n",
    "\n",
    "    y_an = np.zeros((N_x_test, 1))\n",
    "    lam = lamb_analytical(k1, k2)\n",
    "    s = np.sqrt(k1*t_test)*2*lam\n",
    "    \n",
    "    for j in range(N_x_test):\n",
    "        if(x_test[j]<s):\n",
    "            y_an[j] = 0.5 - 0.5*math.erf( x_test[j]/( 2*np.sqrt(k1*t_test) ) )/ math.erf(lam) \n",
    "        else:\n",
    "            y_an[j] = T_r\n",
    "            \n",
    "    y_an = np.reshape(y_an, (N_x_test, 1))\n",
    "    \n",
    "    return y_an\n",
    "    \n",
    "    \n",
    "def train_model(model, optimiser1, epochs, T_r, T_l, k1, k2, N_x, x_l, x_r, s_i, N_t, N_bc, accuracy_cap, N_x_test):\n",
    "    \n",
    "    loss_store = []\n",
    "    T_store_pred = []\n",
    "    T_store_an = []\n",
    "    s_store_pred = []\n",
    "    mse = nn.MSELoss(reduction='sum')\n",
    "    model.train()  \n",
    "    \n",
    "    N_tot = N_x + 2*N_bc\n",
    "    print(\"N_tot = \", N_tot)\n",
    "    x_test_np, x_test_torch = x_test_data(N_x_test, x_l, x_r)\n",
    "    t_test = 0\n",
    "    \n",
    "    for i in range(N_t):\n",
    "        \n",
    "        if(i==0):\n",
    "            T_prev = initial_temp(N_x, N_bc, T_l, T_r)\n",
    "            s_np = np.ones(1)*s_i\n",
    "            s_np = np.reshape(s_np, (1,1))\n",
    "            s_torch = torch.FloatTensor(s_np)\n",
    "            s_torch = s_torch.clone().detach().requires_grad_(True)\n",
    "        else:\n",
    "            T_prev,_,_,_ = model(x_train, s_torch)  \n",
    "            T_prev = T_prev.clone().detach().requires_grad_(False)\n",
    "            Ts_x = Ts_x.detach().numpy()\n",
    "            s_np = s_np - del_t*k2*Ts_x\n",
    "            s_torch = torch.FloatTensor(s_np)\n",
    "            s_torch = s_torch.clone().detach().requires_grad_(True)\n",
    "            t_test = t_test + del_t\n",
    "            \n",
    "        print(\"s = \", s_np[0][0])\n",
    "        print(\" \")\n",
    "        s_store_pred.append(s_np)\n",
    "        x_train, N_xl, N_xr = x_train_data(N_x, x_l, x_r, s_np, N_bc)\n",
    "        print(\"t = \", t_test)\n",
    "        print(\" \")\n",
    "        y_an = analytical(N_x_test, x_test_np, t_test, T_r, k1, k2)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            #Backpropogation and optimisation\n",
    "            loss, eq1, bc1, bc2 = get_loss(x_train, k1, N_tot, T_l, T_r, N_xl, N_xr, x_l, x_r, T_prev, del_t, s_torch)\n",
    "            optimiser1.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser1.step()  \n",
    "            loss_store.append(loss.detach().numpy())\n",
    "\n",
    "            L2_norm_err = L2_err(N_x_test, x_test_torch, y_an, model, s_torch, T_r)\n",
    "            \n",
    "            if epoch%2000==0:\n",
    "                print_loss(epoch, loss, eq1, bc1, bc2)\n",
    "                print(\"L2_err= \", L2_norm_err )\n",
    "                print(\"\")\n",
    "            \n",
    "            if L2_norm_err<accuracy_cap and i>2 :\n",
    "                print(\"loss limit attained, epoch = \", epoch,\" L2_err= \", L2_norm_err)\n",
    "                print(\"\")\n",
    "                break\n",
    "            \n",
    "        # Store the results after each time step\n",
    "        T_st,_,_,Ts_x = model(x_test_torch,s_torch)\n",
    "        T_st = T_st.detach().numpy()\n",
    "        T_store_pred.append(T_st)\n",
    "        T_an = analytical(N_x_test, x_test_np, t_test, T_r, k1, k2)\n",
    "        T_store_an.append(T_an)\n",
    "        \n",
    "        print(\"broke inner loop\")\n",
    "        print(\"\")\n",
    "\n",
    "    return loss_store, T_store_pred, T_store_an, x_test_np, s_store_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fa5eeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=25, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=25, out_features=25, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=25, out_features=25, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=25, out_features=1, bias=True)\n",
      "    (7): Tanh()\n",
      "  )\n",
      ")\n",
      "Total trainable parameters in the model: 1376\n",
      "N_tot =  271\n",
      "s =  0.01\n",
      " \n",
      "t =  0\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.8057955\n",
      "eq1_loss =  0.4068423\n",
      "bc1_loss =  0.3983578\n",
      "bc2_loss =  0.0005953795\n",
      "L2_err=  0.00020627775099958177\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "s =  0.01003093541454291\n",
      " \n",
      "t =  0.001\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.015429879\n",
      "eq1_loss =  4.4957008e-07\n",
      "bc1_loss =  0.011414208\n",
      "bc2_loss =  0.004015221\n",
      "L2_err=  0.0006057052148138448\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "s =  0.0100696631068422\n",
      " \n",
      "t =  0.002\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.006930094\n",
      "eq1_loss =  1.0771146e-06\n",
      "bc1_loss =  0.00137813\n",
      "bc2_loss =  0.005550887\n",
      "L2_err=  0.00044267141220545944\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "s =  0.010111056077003014\n",
      " \n",
      "t =  0.003\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.0063475682\n",
      "eq1_loss =  1.2712887e-06\n",
      "bc1_loss =  0.00035962186\n",
      "bc2_loss =  0.005986675\n",
      "L2_err=  0.0003241270109316254\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "s =  0.010154049677803414\n",
      " \n",
      "t =  0.004\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.00629978\n",
      "eq1_loss =  1.2580458e-06\n",
      "bc1_loss =  0.00020195755\n",
      "bc2_loss =  0.0060965647\n",
      "L2_err=  0.00024580731953834534\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "s =  0.010198933994397521\n",
      " \n",
      "t =  0.005\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.006283967\n",
      "eq1_loss =  1.1529175e-06\n",
      "bc1_loss =  0.00016418753\n",
      "bc2_loss =  0.0061186263\n",
      "L2_err=  0.00019369131590555807\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "s =  0.01024655154629727\n",
      " \n",
      "t =  0.006\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.006264291\n",
      "eq1_loss =  9.942459e-07\n",
      "bc1_loss =  0.00014945578\n",
      "bc2_loss =  0.0061138407\n",
      "L2_err=  0.00015754965925599058\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "s =  0.010298605329589918\n",
      " \n",
      "t =  0.007\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.0062329886\n",
      "eq1_loss =  9.4007714e-07\n",
      "bc1_loss =  0.00013622413\n",
      "bc2_loss =  0.0060958243\n",
      "L2_err=  0.00013080729357527556\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "s =  0.01036016584635945\n",
      " \n",
      "t =  0.008\n",
      " \n",
      "epoch =  0\n",
      "loss =  0.006170157\n",
      "eq1_loss =  3.8398684e-06\n",
      "bc1_loss =  0.00010967084\n",
      "bc2_loss =  0.0060566464\n",
      "L2_err=  0.00011015422620941966\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10968/2075889244.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# Training model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mloss_store\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_store_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_store_an\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_store_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimiser1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_bc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_cap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_x_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10968/1062481441.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimiser1, epochs, T_r, T_l, k1, k2, N_x, x_l, x_r, s_i, N_t, N_bc, accuracy_cap, N_x_test)\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mloss_store\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m             \u001b[0mL2_norm_err\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL2_err\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_x_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_torch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_an\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_torch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_r\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10968/1062481441.py\u001b[0m in \u001b[0;36mL2_err\u001b[1;34m(N_x_test, x_test, y_an, model, s, T_r)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mL2_err\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_x_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_an\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_r\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10968/1062481441.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x_train, s)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mx_train\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mop_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mop_x2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mTs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m     return Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    235\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         inputs, allow_unused, accumulate_grad=False)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_x = 201\n",
    "N_bc = 35\n",
    "N_t = 30\n",
    "del_t = 0.001\n",
    "x_l = 0\n",
    "x_r = 1\n",
    "T_r = 0\n",
    "T_l = 0.5\n",
    "t_i = 0\n",
    "s_i = 0.01\n",
    "accuracy_cap = 0.0001\n",
    "N_x_test = 1001\n",
    "\n",
    "# Neural network params\n",
    "layer_size = [1, 25, 25, 25, 1]\n",
    "\n",
    "# material params\n",
    "k2 = 0.04\n",
    "k1 = 0.8\n",
    "\n",
    "# Training data and initial data\n",
    "model = ANN(layer_size)\n",
    "print(model)\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total trainable parameters in the model:\", total_trainable_params)\n",
    "\n",
    "# # Setup Loss function and Optimiser\n",
    "lr = 8e-5\n",
    "epochs = 2000\n",
    "optimiser1 = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training model\n",
    "loss_store, T_store_pred, T_store_an, x_test_np, s_store_pred = train_model(model, optimiser1, epochs, T_r, T_l, k1, k2, N_x, x_l, x_r, s_i, N_t, N_bc, accuracy_cap, N_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890d895a",
   "metadata": {},
   "source": [
    "# Results Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6502e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(N_t):\n",
    "    for j in range(N_x_test):\n",
    "        if(x_test_np[j]>s_store_pred[i]):\n",
    "            T_store_pred[i][j][0] = T_r\n",
    "\n",
    "j = 29\n",
    "plt.plot(x_test_np, T_store_pred[j])\n",
    "plt.plot(x_test_np, T_store_an[j])\n",
    "Title = \"Transient Heat Conduction using Time stepping, \" + \"time = \" + str(j*del_t) + \", delta_t = \" + str(del_t)\n",
    "plt.title(Title)\n",
    "plt.legend([\"PINN\", \"Analytical\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e284b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
