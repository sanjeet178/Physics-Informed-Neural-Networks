{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4ac27d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.init as init\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8819113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_torch(arr):\n",
    "    \n",
    "    arr = torch.FloatTensor(arr)\n",
    "    arr = arr.unsqueeze(-1)\n",
    "    arr = arr.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def x_train_data(N_x, x_l, x_r, N_bc):\n",
    "\n",
    "    x_train = np.geomspace(x_l, x_r, N_x)\n",
    "    x_bc1 = np.zeros(N_bc)\n",
    "    x_bc2 = np.ones(N_bc)*x_r\n",
    "    x_train = np_to_torch(x_train)\n",
    "    x_bc1 = np_to_torch(x_bc1)\n",
    "    x_bc2 = np_to_torch(x_bc2)\n",
    "    \n",
    "    return x_train, x_bc1, x_bc2\n",
    "            \n",
    "def xavier_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0.05)\n",
    "    \n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, layer_size_1):\n",
    "        super(ANN, self).__init__()\n",
    "        \n",
    "        # Fully conected model\n",
    "        modules_1 = []\n",
    "        for i in range(len(layer_size_1) - 1):\n",
    "            modules_1.append(nn.Linear(layer_size_1[i], layer_size_1[i+1]))  \n",
    "            modules_1.append(nn.Tanh())\n",
    "\n",
    "        self.fc_1 = nn.Sequential(*modules_1)\n",
    "#         self.fc_1.apply(xavier_init)\n",
    "\n",
    "        for layer in self.fc_2.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                 layer.weight.data.normal_(mean=0, std=0.2)\n",
    "        \n",
    "    def forward(self, x_train):\n",
    "        T = self.fc_1( x_train )\n",
    "        dTdx = torch.autograd.grad(T, x_train, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "        d2Tdx2 = torch.autograd.grad(dTdx, x_train, grad_outputs=torch.ones_like(dTdx), create_graph=True)[0]\n",
    "        \n",
    "        return T, d2Tdx2\n",
    "    \n",
    "def get_loss(x_train, t_train, k1, k2, N_tot, N_ic, T_l, T_r, N_xl, N_xr, x_l, x_r, T_ini, s_ini, null):\n",
    "    \n",
    "    mse = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    w1 = 1\n",
    "    w2 = 1\n",
    "    w3 = 1\n",
    "    w4 = 1\n",
    "    w5 = 1\n",
    "    w6 = 1\n",
    "    T, dTdt, d2Tdx2 = model(x_train, t_train)\n",
    "    eq1 = w1*mse(torch.mul ( 1/(1 + torch.exp(-50*(s - x_train))), (dTdt-k1*d2Tdx2)), null) /(N_tot)\n",
    "    bc1 = w3*mse( torch.mul(torch.where(x_train == x_l,1,0),(T - T_l)), null )/(N_xl)\n",
    "    bc2 = w4*mse( Ts - T_r, null )/(N_tot)\n",
    "    eq2 = w5*mse( dsdt+k2*dTsdx, null )/(N_tot)\n",
    "    ic2 = w6*mse( torch.mul(torch.where(t_train == 0,1,0),(s - s_ini)), null ) /(N_ic)\n",
    "    loss = eq1 + eq2 + bc1 + bc2 + ic1 + ic2\n",
    "    \n",
    "    return loss, eq1, eq2, bc1, bc2, ic1, ic2\n",
    "\n",
    "def print_loss(epoch, loss, eq1, eq2, bc1, bc2, ic1, ic2):\n",
    "    print('epoch = ',epoch)\n",
    "    print('loss = ',loss.detach().numpy())\n",
    "    print('eq1_loss = ',eq1.detach().numpy())\n",
    "    print('eq2_loss = ',eq2.detach().numpy())\n",
    "    print('bc1_loss = ',bc1.detach().numpy())\n",
    "    print('bc2_loss = ',bc2.detach().numpy())\n",
    "    print('ic1_loss = ',ic1.detach().numpy())\n",
    "    print('ic2_loss = ',ic2.detach().numpy())\n",
    "    \n",
    "def L2_err(N_x_test, N_t_test, x_test, t_test, y_an, model):\n",
    "    \n",
    "    x_test = np_to_torch(x_test)\n",
    "    t_test = np_to_torch(t_test)\n",
    "    y_pred,_,_,s_pred,_,_,_  = model(x_test, t_test)\n",
    "    y_pred = y_pred.detach().numpy()\n",
    "    s_pred = s_pred.detach().numpy()\n",
    "    x_test = x_test.detach().numpy()\n",
    "    \n",
    "    total_err = 0\n",
    "    cnt = 0\n",
    "    for i in range(N_x_test*N_t_test):\n",
    "        if(s_pred[i]>x_test[i]):\n",
    "            cnt = cnt+1\n",
    "            total_err += (y_pred[i] - y_an[i])**2\n",
    "    \n",
    "    if(cnt==0):\n",
    "        return 0\n",
    "    L2_err =  total_err/(cnt)\n",
    "    return L2_err[0]\n",
    "\n",
    "def lamb_analytical(k1, k2):\n",
    "    x = []\n",
    "    er = []\n",
    "    cnt = 0\n",
    "    for i in np.arange(0.1, 5, 0.001):\n",
    "        x.append(i)\n",
    "        er.append(math.erf(x[-1]))\n",
    "        cnt = cnt+1\n",
    "\n",
    "    x = np.array(x)\n",
    "    er = np.array(er)\n",
    "    y =[]\n",
    "    y = np.exp(-x*x)/(er*math.sqrt(math.pi))-x*k1/k2\n",
    "\n",
    "    for i in range(1,cnt):\n",
    "        if(y[i]*y[i-1]<0):\n",
    "            lam = x[i]\n",
    "            break\n",
    "    \n",
    "    return lam\n",
    "\n",
    "def analytical(N_x_test, N_t_test, x_l, x_r, t_i, t_f, k1, k2):\n",
    "    \n",
    "    x_test = np.linspace(x_l, x_r, N_x_test)\n",
    "    x_test = np.tile(x_test, N_t_test)\n",
    "    t_test = np.linspace(t_i, t_f, N_t_test)\n",
    "    t_test = np.repeat(t_test, N_x_test)\n",
    "\n",
    "    lam = lamb_analytical(k1, k2)\n",
    "    \n",
    "    s_an = np.sqrt(k1*t_test)*2*lam\n",
    "    y_an = np.zeros(N_x_test*N_t_test)\n",
    "    for j in range(N_x_test*N_t_test):\n",
    "        if(x_test[j]<s_an[j]):\n",
    "            y_an[j] = 1 - math.erf( x_test[j]/( 2*np.sqrt(k1*t_test[j]) ) )/ math.erf(lam) \n",
    "       \n",
    "    y_an = y_an.reshape(N_x_test*N_t_test,1)\n",
    "    \n",
    "    return y_an, x_test, t_test\n",
    "    \n",
    "    \n",
    "def train_model(model, optimiser1, epochs, T_r, T_l, k1, k2, N_x, x_l, x_r, N_t, N_bc, N_ic, t_arr, t_i, T_ini, accuracy_cap, N_x_test, N_t_test, s_ini):\n",
    "    \n",
    "    loss_store = []\n",
    "    mse = nn.MSELoss(reduction='sum')\n",
    "    model.train()  \n",
    "    \n",
    "    start = time.time()\n",
    "    y_an1, x_test1, t_test1 = analytical(N_x_test, N_t_test, x_l, x_r, t_i, t_arr[-1], k1, k2)\n",
    "    for j in range(len(t_arr)-1):\n",
    "        \n",
    "        if(j==0):\n",
    "            epochs = 15000\n",
    "        else:\n",
    "            epochs = 2000\n",
    "            \n",
    "        N_t_fin = int( ( t_arr[j+1] - t_arr[j] )*N_t )\n",
    "        N_bc_fin = int( ( t_arr[j+1] - t_arr[j] )*N_bc )\n",
    "        N_ic_fin = int( ( t_arr[j+1] - t_arr[j] )*N_ic )\n",
    "        print(\"N_t_fin = \", N_t_fin)\n",
    "            \n",
    "        # Training data    \n",
    "        N_tot = N_x*N_t_fin + 2*N_bc_fin + N_ic_fin\n",
    "        null = torch.zeros(N_tot).unsqueeze(-1)\n",
    "        x_train, N_xl, N_xr = x_train_data(N_x, x_l, x_r, N_bc_fin, N_ic_fin, N_t_fin, null)\n",
    "        t_train = t_train_data(t_i,t_arr[j+1],N_t_fin,N_x,N_bc_fin,N_ic_fin)\n",
    "        N_ic_fin = mse( torch.where(t_train == t_i,1,0) , null).detach().numpy().item()\n",
    "        print(\"x_train = \", x_train.shape)\n",
    "        print(\"t_train = \", t_train.shape)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Test Data\n",
    "        N_t_test_2 = int( N_t_test*( t_arr[j+1] - t_arr[j] ) )\n",
    "        y_an2, x_test2, t_test2 = analytical(N_x_test, N_t_test_2, x_l, x_r, t_arr[j], t_arr[j+1], k1, k2)\n",
    "\n",
    "        # Adam optimiser loop\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            #Backpropogation and optimisation\n",
    "            loss, eq1, eq2, bc1, bc2, ic1, ic2 = get_loss(x_train, t_train, k1, k2, N_tot, N_ic, T_l, T_r, N_xl, N_xr, x_l, x_r, T_ini, s_ini, null)\n",
    "            optimiser1.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser1.step()  \n",
    "            loss_store.append(loss.detach().numpy())\n",
    "\n",
    "#             L2_norm_err_1 = L2_err(N_x_test, N_t_test, x_test1, t_test1, y_an1, model)\n",
    "            L2_norm_err_2 = L2_err(N_x_test, N_t_test_2, x_test2, t_test2, y_an2, model)\n",
    "            \n",
    "            if epoch%400==0:\n",
    "                print_loss(epoch, loss, eq1, eq2, bc1, bc2, ic1, ic2)\n",
    "#                 print(\"L2_err_1= \", L2_norm_err_1 )\n",
    "                print(\"L2_err_2= \", L2_norm_err_2 )\n",
    "                print(\"\")\n",
    "                \n",
    "            if epoch%1000==0:\n",
    "                print(\"time = \", time.time() - start)\n",
    "                print(\"\")\n",
    "            \n",
    "            \n",
    "            if L2_norm_err_2<accuracy_cap and epoch>5000:\n",
    "                print(\"loss limit attained, epoch = \", epoch)\n",
    "                print(\"\")\n",
    "                break\n",
    "            \n",
    "        print(\"broke inner loop\")\n",
    "        print(\"\")\n",
    "        \n",
    "#         if(L2_norm_err_1<accuracy_cap):\n",
    "#             break\n",
    "        \n",
    "    end = time.time()\n",
    "\n",
    "    return end-start, loss_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa5eeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN(\n",
      "  (fc_1): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=25, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=25, out_features=25, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=25, out_features=25, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=25, out_features=25, bias=True)\n",
      "    (7): Tanh()\n",
      "    (8): Linear(in_features=25, out_features=1, bias=True)\n",
      "    (9): Tanh()\n",
      "  )\n",
      "  (fc_2): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=25, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=25, out_features=25, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=25, out_features=25, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=25, out_features=25, bias=True)\n",
      "    (7): Tanh()\n",
      "    (8): Linear(in_features=25, out_features=1, bias=True)\n",
      "    (9): Tanh()\n",
      "  )\n",
      ")\n",
      "Total trainable parameters in the model: 4077\n",
      "N_t_fin =  50\n",
      "x_train =  torch.Size([1900, 1])\n",
      "t_train =  torch.Size([1900, 1])\n",
      "\n",
      "epoch =  0\n",
      "loss =  1.2092016\n",
      "eq1_loss =  7.860709e-05\n",
      "eq2_loss =  8.710294e-06\n",
      "bc1_loss =  1.1817535\n",
      "bc2_loss =  0.008011602\n",
      "ic1_loss =  0.0065971348\n",
      "ic2_loss =  0.012752077\n",
      "L2_err_2=  0.6060589550819069\n",
      "\n",
      "time =  0.10451626777648926\n",
      "\n",
      "epoch =  400\n",
      "loss =  0.55075973\n",
      "eq1_loss =  0.00044370128\n",
      "eq2_loss =  0.00011012469\n",
      "bc1_loss =  0.35840043\n",
      "bc2_loss =  0.107091196\n",
      "ic1_loss =  0.069015525\n",
      "ic2_loss =  0.01569878\n",
      "L2_err_2=  0.08409412856705255\n",
      "\n",
      "epoch =  800\n",
      "loss =  0.38743973\n",
      "eq1_loss =  0.0044519994\n",
      "eq2_loss =  0.006254904\n",
      "bc1_loss =  0.17821223\n",
      "bc2_loss =  0.040335197\n",
      "ic1_loss =  0.1265438\n",
      "ic2_loss =  0.031641614\n",
      "L2_err_2=  0.05143929291840282\n",
      "\n",
      "time =  78.11677432060242\n",
      "\n",
      "epoch =  1200\n",
      "loss =  0.36658356\n",
      "eq1_loss =  0.005951992\n",
      "eq2_loss =  0.00037590327\n",
      "bc1_loss =  0.14446825\n",
      "bc2_loss =  0.041731432\n",
      "ic1_loss =  0.14317183\n",
      "ic2_loss =  0.030884143\n",
      "L2_err_2=  0.04957671940804069\n",
      "\n",
      "epoch =  1600\n",
      "loss =  0.35235596\n",
      "eq1_loss =  0.006587824\n",
      "eq2_loss =  0.00023871697\n",
      "bc1_loss =  0.1348969\n",
      "bc2_loss =  0.039133176\n",
      "ic1_loss =  0.14229035\n",
      "ic2_loss =  0.02920898\n",
      "L2_err_2=  0.04625603867985126\n",
      "\n",
      "epoch =  2000\n",
      "loss =  0.33501574\n",
      "eq1_loss =  0.007270085\n",
      "eq2_loss =  0.00035549575\n",
      "bc1_loss =  0.12911783\n",
      "bc2_loss =  0.034917194\n",
      "ic1_loss =  0.13691981\n",
      "ic2_loss =  0.026435338\n",
      "L2_err_2=  0.041526191433104605\n",
      "\n",
      "time =  152.05837202072144\n",
      "\n",
      "epoch =  2400\n",
      "loss =  0.31374633\n",
      "eq1_loss =  0.008376694\n",
      "eq2_loss =  0.0003533791\n",
      "bc1_loss =  0.12579152\n",
      "bc2_loss =  0.029223382\n",
      "ic1_loss =  0.12795807\n",
      "ic2_loss =  0.022043275\n",
      "L2_err_2=  0.03545447553324557\n",
      "\n",
      "epoch =  2800\n",
      "loss =  0.29296702\n",
      "eq1_loss =  0.011017516\n",
      "eq2_loss =  0.0011186714\n",
      "bc1_loss =  0.12505871\n",
      "bc2_loss =  0.022999516\n",
      "ic1_loss =  0.11536996\n",
      "ic2_loss =  0.017402649\n",
      "L2_err_2=  0.030490246236943037\n",
      "\n",
      "time =  227.00269603729248\n",
      "\n",
      "epoch =  3200\n",
      "loss =  0.27725768\n",
      "eq1_loss =  0.013022793\n",
      "eq2_loss =  0.00060400536\n",
      "bc1_loss =  0.12727967\n",
      "bc2_loss =  0.018776923\n",
      "ic1_loss =  0.103781246\n",
      "ic2_loss =  0.013793042\n",
      "L2_err_2=  0.028316008003514515\n",
      "\n",
      "epoch =  3600\n",
      "loss =  0.26566744\n",
      "eq1_loss =  0.0133916\n",
      "eq2_loss =  0.0004510932\n",
      "bc1_loss =  0.12910087\n",
      "bc2_loss =  0.016926711\n",
      "ic1_loss =  0.094609514\n",
      "ic2_loss =  0.011187659\n",
      "L2_err_2=  0.02749221650609941\n",
      "\n",
      "epoch =  4000\n",
      "loss =  0.2556069\n",
      "eq1_loss =  0.012873002\n",
      "eq2_loss =  0.0004997067\n",
      "bc1_loss =  0.12860966\n",
      "bc2_loss =  0.016100565\n",
      "ic1_loss =  0.08780368\n",
      "ic2_loss =  0.009720294\n",
      "L2_err_2=  0.026650372684560438\n",
      "\n",
      "time =  310.73627638816833\n",
      "\n",
      "epoch =  4400\n",
      "loss =  0.24015991\n",
      "eq1_loss =  0.011335657\n",
      "eq2_loss =  0.00040953478\n",
      "bc1_loss =  0.12223633\n",
      "bc2_loss =  0.014414669\n",
      "ic1_loss =  0.08321833\n",
      "ic2_loss =  0.00854539\n",
      "L2_err_2=  0.024371680902448643\n",
      "\n",
      "epoch =  4800\n",
      "loss =  0.20301491\n",
      "eq1_loss =  0.008506257\n",
      "eq2_loss =  0.0002683429\n",
      "bc1_loss =  0.0981175\n",
      "bc2_loss =  0.0091303475\n",
      "ic1_loss =  0.07949143\n",
      "ic2_loss =  0.0075010327\n",
      "L2_err_2=  0.017220139345738645\n",
      "\n",
      "time =  382.5773947238922\n",
      "\n",
      "epoch =  5200\n",
      "loss =  0.1822981\n",
      "eq1_loss =  0.00942867\n",
      "eq2_loss =  0.0002545814\n",
      "bc1_loss =  0.082383126\n",
      "bc2_loss =  0.006517469\n",
      "ic1_loss =  0.07751041\n",
      "ic2_loss =  0.006203833\n",
      "L2_err_2=  0.014168589572440741\n",
      "\n",
      "epoch =  5600\n",
      "loss =  0.16911235\n",
      "eq1_loss =  0.0074302442\n",
      "eq2_loss =  0.00046271938\n",
      "bc1_loss =  0.07602944\n",
      "bc2_loss =  0.0056709847\n",
      "ic1_loss =  0.07419743\n",
      "ic2_loss =  0.005321525\n",
      "L2_err_2=  0.013311030013689495\n",
      "\n",
      "epoch =  6000\n",
      "loss =  0.1561624\n",
      "eq1_loss =  0.005466718\n",
      "eq2_loss =  0.0005446795\n",
      "bc1_loss =  0.06790158\n",
      "bc2_loss =  0.004547675\n",
      "ic1_loss =  0.072763614\n",
      "ic2_loss =  0.004938127\n",
      "L2_err_2=  0.011753411189848786\n",
      "\n",
      "time =  453.8224461078644\n",
      "\n",
      "epoch =  6400\n",
      "loss =  0.14412116\n",
      "eq1_loss =  0.0060675954\n",
      "eq2_loss =  0.00053028145\n",
      "bc1_loss =  0.061282977\n",
      "bc2_loss =  0.0031589745\n",
      "ic1_loss =  0.06839155\n",
      "ic2_loss =  0.0046897805\n",
      "L2_err_2=  0.009949475508933527\n",
      "\n",
      "epoch =  6800\n",
      "loss =  0.13611227\n",
      "eq1_loss =  0.007874104\n",
      "eq2_loss =  0.00055215386\n",
      "bc1_loss =  0.05699759\n",
      "bc2_loss =  0.0025189947\n",
      "ic1_loss =  0.06387144\n",
      "ic2_loss =  0.0042979857\n",
      "L2_err_2=  0.008794909011536753\n",
      "\n",
      "time =  532.5191903114319\n",
      "\n",
      "epoch =  7200\n",
      "loss =  0.13027036\n",
      "eq1_loss =  0.008318851\n",
      "eq2_loss =  0.0005854819\n",
      "bc1_loss =  0.0543014\n",
      "bc2_loss =  0.002221619\n",
      "ic1_loss =  0.06089023\n",
      "ic2_loss =  0.003952772\n",
      "L2_err_2=  0.008245990894195929\n",
      "\n",
      "epoch =  7600\n",
      "loss =  0.12419458\n",
      "eq1_loss =  0.007792253\n",
      "eq2_loss =  0.00061822304\n",
      "bc1_loss =  0.05219524\n",
      "bc2_loss =  0.0019696427\n",
      "ic1_loss =  0.05795998\n",
      "ic2_loss =  0.0036592418\n",
      "L2_err_2=  0.007873083834061214\n",
      "\n",
      "epoch =  8000\n",
      "loss =  0.116622426\n",
      "eq1_loss =  0.0066617015\n",
      "eq2_loss =  0.0006891883\n",
      "bc1_loss =  0.050320223\n",
      "bc2_loss =  0.0016490187\n",
      "ic1_loss =  0.053848516\n",
      "ic2_loss =  0.003453776\n",
      "L2_err_2=  0.007423661562277604\n",
      "\n",
      "time =  610.388356924057\n",
      "\n",
      "epoch =  8400\n",
      "loss =  0.107525885\n",
      "eq1_loss =  0.005594291\n",
      "eq2_loss =  0.0007917649\n",
      "bc1_loss =  0.04851805\n",
      "bc2_loss =  0.0013915704\n",
      "ic1_loss =  0.047973774\n",
      "ic2_loss =  0.0032564332\n",
      "L2_err_2=  0.006947264837205171\n",
      "\n",
      "epoch =  8800\n",
      "loss =  0.09979723\n",
      "eq1_loss =  0.0057163606\n",
      "eq2_loss =  0.00084517105\n",
      "bc1_loss =  0.04706204\n",
      "bc2_loss =  0.0012916274\n",
      "ic1_loss =  0.041804593\n",
      "ic2_loss =  0.003077432\n",
      "L2_err_2=  0.0064145020782487416\n",
      "\n",
      "time =  688.1355679035187\n",
      "\n",
      "epoch =  9200\n",
      "loss =  0.092676386\n",
      "eq1_loss =  0.0056790356\n",
      "eq2_loss =  0.00085152563\n",
      "bc1_loss =  0.045131084\n",
      "bc2_loss =  0.0010766633\n",
      "ic1_loss =  0.03703041\n",
      "ic2_loss =  0.0029076645\n",
      "L2_err_2=  0.005999645960515771\n",
      "\n",
      "epoch =  9600\n",
      "loss =  0.085240446\n",
      "eq1_loss =  0.005221502\n",
      "eq2_loss =  0.0007208462\n",
      "bc1_loss =  0.04284074\n",
      "bc2_loss =  0.00080428465\n",
      "ic1_loss =  0.03281621\n",
      "ic2_loss =  0.0028368584\n",
      "L2_err_2=  0.005618572007176372\n",
      "\n",
      "epoch =  10000\n",
      "loss =  0.07837982\n",
      "eq1_loss =  0.00493304\n",
      "eq2_loss =  0.000677435\n",
      "bc1_loss =  0.040948115\n",
      "bc2_loss =  0.00070624886\n",
      "ic1_loss =  0.028417047\n",
      "ic2_loss =  0.0026979405\n",
      "L2_err_2=  0.005266367790001724\n",
      "\n",
      "time =  768.9729084968567\n",
      "\n",
      "epoch =  10400\n",
      "loss =  0.07260723\n",
      "eq1_loss =  0.005114157\n",
      "eq2_loss =  0.0006237497\n",
      "bc1_loss =  0.040045444\n",
      "bc2_loss =  0.00070027326\n",
      "ic1_loss =  0.023679828\n",
      "ic2_loss =  0.0024437734\n",
      "L2_err_2=  0.004935682113359198\n",
      "\n",
      "epoch =  10800\n",
      "loss =  0.06720104\n",
      "eq1_loss =  0.0052597094\n",
      "eq2_loss =  0.00058095716\n",
      "bc1_loss =  0.039206486\n",
      "bc2_loss =  0.00070421956\n",
      "ic1_loss =  0.019366868\n",
      "ic2_loss =  0.0020828028\n",
      "L2_err_2=  0.004670957434472793\n",
      "\n",
      "time =  852.9614672660828\n",
      "\n",
      "epoch =  11200\n",
      "loss =  0.061913222\n",
      "eq1_loss =  0.005225774\n",
      "eq2_loss =  0.0004958615\n",
      "bc1_loss =  0.037847992\n",
      "bc2_loss =  0.00068303646\n",
      "ic1_loss =  0.015850635\n",
      "ic2_loss =  0.0018099244\n",
      "L2_err_2=  0.004406062046764027\n",
      "\n",
      "epoch =  11600\n",
      "loss =  0.05715862\n",
      "eq1_loss =  0.0051796385\n",
      "eq2_loss =  0.00048281928\n",
      "bc1_loss =  0.035603683\n",
      "bc2_loss =  0.00066368596\n",
      "ic1_loss =  0.013608254\n",
      "ic2_loss =  0.0016205356\n",
      "L2_err_2=  0.004111448922704994\n",
      "\n",
      "epoch =  12000\n",
      "loss =  0.05332204\n",
      "eq1_loss =  0.005252232\n",
      "eq2_loss =  0.0004966988\n",
      "bc1_loss =  0.03306552\n",
      "bc2_loss =  0.00064411847\n",
      "ic1_loss =  0.012367911\n",
      "ic2_loss =  0.0014955598\n",
      "L2_err_2=  0.0038191895555183602\n",
      "\n",
      "time =  936.6300292015076\n",
      "\n",
      "epoch =  12400\n",
      "loss =  0.05036151\n",
      "eq1_loss =  0.005376845\n",
      "eq2_loss =  0.0005052765\n",
      "bc1_loss =  0.030845806\n",
      "bc2_loss =  0.0006146277\n",
      "ic1_loss =  0.011607205\n",
      "ic2_loss =  0.0014117514\n",
      "L2_err_2=  0.0035810215163994095\n",
      "\n",
      "epoch =  12800\n",
      "loss =  0.047965083\n",
      "eq1_loss =  0.0054790378\n",
      "eq2_loss =  0.0005368389\n",
      "bc1_loss =  0.028991662\n",
      "bc2_loss =  0.00058151595\n",
      "ic1_loss =  0.011048992\n",
      "ic2_loss =  0.0013270389\n",
      "L2_err_2=  0.00341227058025685\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time =  1012.0595164299011\n",
      "\n",
      "epoch =  13200\n",
      "loss =  0.046070673\n",
      "eq1_loss =  0.005610294\n",
      "eq2_loss =  0.00053452025\n",
      "bc1_loss =  0.02751358\n",
      "bc2_loss =  0.0005459276\n",
      "ic1_loss =  0.010603769\n",
      "ic2_loss =  0.0012625832\n",
      "L2_err_2=  0.0032917901989484273\n",
      "\n",
      "epoch =  13600\n",
      "loss =  0.04454458\n",
      "eq1_loss =  0.005739441\n",
      "eq2_loss =  0.0005293651\n",
      "bc1_loss =  0.02628605\n",
      "bc2_loss =  0.0005125671\n",
      "ic1_loss =  0.010270616\n",
      "ic2_loss =  0.0012065421\n",
      "L2_err_2=  0.0031951573073597185\n",
      "\n",
      "epoch =  14000\n",
      "loss =  0.04327198\n",
      "eq1_loss =  0.0058735465\n",
      "eq2_loss =  0.00051142124\n",
      "bc1_loss =  0.025307069\n",
      "bc2_loss =  0.00048425957\n",
      "ic1_loss =  0.009940082\n",
      "ic2_loss =  0.0011556089\n",
      "L2_err_2=  0.003111726312381714\n",
      "\n",
      "time =  1088.4079163074493\n",
      "\n",
      "epoch =  14400\n",
      "loss =  0.0421667\n",
      "eq1_loss =  0.0059642647\n",
      "eq2_loss =  0.0004796053\n",
      "bc1_loss =  0.024467537\n",
      "bc2_loss =  0.00045786615\n",
      "ic1_loss =  0.0096796155\n",
      "ic2_loss =  0.0011178069\n",
      "L2_err_2=  0.0030395231159195428\n",
      "\n",
      "epoch =  14800\n",
      "loss =  0.04113908\n",
      "eq1_loss =  0.005978209\n",
      "eq2_loss =  0.0004595493\n",
      "bc1_loss =  0.023745332\n",
      "bc2_loss =  0.00043489246\n",
      "ic1_loss =  0.009445968\n",
      "ic2_loss =  0.001075129\n",
      "L2_err_2=  0.0029773205306163307\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "N_t_fin =  10\n",
      "x_train =  torch.Size([380, 1])\n",
      "t_train =  torch.Size([380, 1])\n",
      "\n",
      "epoch =  0\n",
      "loss =  0.029795043\n",
      "eq1_loss =  0.012371266\n",
      "eq2_loss =  0.0007630687\n",
      "bc1_loss =  0.0063267844\n",
      "bc2_loss =  0.0004823985\n",
      "ic1_loss =  0.00927927\n",
      "ic2_loss =  0.0005722542\n",
      "L2_err_2=  0.0013191845823197202\n",
      "\n",
      "time =  1166.5493779182434\n",
      "\n",
      "epoch =  400\n",
      "loss =  0.023071283\n",
      "eq1_loss =  0.006912633\n",
      "eq2_loss =  0.00039285718\n",
      "bc1_loss =  0.008797629\n",
      "bc2_loss =  0.00068750867\n",
      "ic1_loss =  0.0056035393\n",
      "ic2_loss =  0.00067711895\n",
      "L2_err_2=  0.0013609703996309143\n",
      "\n",
      "epoch =  800\n",
      "loss =  0.019333871\n",
      "eq1_loss =  0.005120778\n",
      "eq2_loss =  0.00029354493\n",
      "bc1_loss =  0.008904667\n",
      "bc2_loss =  0.000555273\n",
      "ic1_loss =  0.004023726\n",
      "ic2_loss =  0.00043588257\n",
      "L2_err_2=  0.0013465764116495668\n",
      "\n",
      "time =  1196.0931901931763\n",
      "\n",
      "epoch =  1200\n",
      "loss =  0.016284924\n",
      "eq1_loss =  0.0038886692\n",
      "eq2_loss =  0.00025621508\n",
      "bc1_loss =  0.008654882\n",
      "bc2_loss =  0.0002391563\n",
      "ic1_loss =  0.0030519655\n",
      "ic2_loss =  0.00019403607\n",
      "L2_err_2=  0.0014417560009964369\n",
      "\n",
      "epoch =  1600\n",
      "loss =  0.015337632\n",
      "eq1_loss =  0.0034897067\n",
      "eq2_loss =  0.00030315065\n",
      "bc1_loss =  0.008218589\n",
      "bc2_loss =  0.00018226726\n",
      "ic1_loss =  0.0029876297\n",
      "ic2_loss =  0.00015628862\n",
      "L2_err_2=  0.0015028182034523565\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "N_t_fin =  10\n",
      "x_train =  torch.Size([380, 1])\n",
      "t_train =  torch.Size([380, 1])\n",
      "\n",
      "epoch =  0\n",
      "loss =  0.043015238\n",
      "eq1_loss =  0.0037528044\n",
      "eq2_loss =  0.005095134\n",
      "bc1_loss =  0.031085154\n",
      "bc2_loss =  0.00022157827\n",
      "ic1_loss =  0.0027400039\n",
      "ic2_loss =  0.00012056406\n",
      "L2_err_2=  0.0012570343734097265\n",
      "\n",
      "time =  1225.9992036819458\n",
      "\n",
      "epoch =  400\n",
      "loss =  0.033155944\n",
      "eq1_loss =  0.0032241573\n",
      "eq2_loss =  0.0007574271\n",
      "bc1_loss =  0.02173038\n",
      "bc2_loss =  0.0002427241\n",
      "ic1_loss =  0.0068783076\n",
      "ic2_loss =  0.00032294376\n",
      "L2_err_2=  0.0013997462323698821\n",
      "\n",
      "epoch =  800\n",
      "loss =  0.03213746\n",
      "eq1_loss =  0.003109819\n",
      "eq2_loss =  0.0006550532\n",
      "bc1_loss =  0.021056596\n",
      "bc2_loss =  0.00025349497\n",
      "ic1_loss =  0.006738033\n",
      "ic2_loss =  0.00032446414\n",
      "L2_err_2=  0.001360155382961784\n",
      "\n",
      "time =  1256.1000182628632\n",
      "\n",
      "epoch =  1200\n",
      "loss =  0.031294327\n",
      "eq1_loss =  0.003038701\n",
      "eq2_loss =  0.000601628\n",
      "bc1_loss =  0.020616552\n",
      "bc2_loss =  0.00026043257\n",
      "ic1_loss =  0.0064587593\n",
      "ic2_loss =  0.00031825426\n",
      "L2_err_2=  0.001351144120975111\n",
      "\n",
      "epoch =  1600\n",
      "loss =  0.030653331\n",
      "eq1_loss =  0.0029360417\n",
      "eq2_loss =  0.0006563088\n",
      "bc1_loss =  0.020054568\n",
      "bc2_loss =  0.0002647025\n",
      "ic1_loss =  0.006416438\n",
      "ic2_loss =  0.00032527457\n",
      "L2_err_2=  0.0013401461573385245\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "N_t_fin =  10\n",
      "x_train =  torch.Size([380, 1])\n",
      "t_train =  torch.Size([380, 1])\n",
      "\n",
      "epoch =  0\n",
      "loss =  0.046335496\n",
      "eq1_loss =  0.0053392635\n",
      "eq2_loss =  0.0015391329\n",
      "bc1_loss =  0.032926124\n",
      "bc2_loss =  0.00028416884\n",
      "ic1_loss =  0.0059366035\n",
      "ic2_loss =  0.00031020393\n",
      "L2_err_2=  0.0011608377195880223\n",
      "\n",
      "time =  1285.975647687912\n",
      "\n",
      "epoch =  400\n",
      "loss =  0.041136995\n",
      "eq1_loss =  0.0025136573\n",
      "eq2_loss =  0.00046196106\n",
      "bc1_loss =  0.0282844\n",
      "bc2_loss =  0.00024139685\n",
      "ic1_loss =  0.009242634\n",
      "ic2_loss =  0.00039294295\n",
      "L2_err_2=  0.001252672168291699\n",
      "\n",
      "epoch =  800\n",
      "loss =  0.04018595\n",
      "eq1_loss =  0.002370056\n",
      "eq2_loss =  0.00034988511\n",
      "bc1_loss =  0.027455637\n",
      "bc2_loss =  0.00020876934\n",
      "ic1_loss =  0.009405752\n",
      "ic2_loss =  0.00039585354\n",
      "L2_err_2=  0.0011877689216499836\n",
      "\n",
      "time =  1316.2880494594574\n",
      "\n",
      "epoch =  1200\n",
      "loss =  0.03945718\n",
      "eq1_loss =  0.0023110183\n",
      "eq2_loss =  0.00031681336\n",
      "bc1_loss =  0.0267636\n",
      "bc2_loss =  0.000178496\n",
      "ic1_loss =  0.009484246\n",
      "ic2_loss =  0.00040300918\n",
      "L2_err_2=  0.001136574866634656\n",
      "\n",
      "epoch =  1600\n",
      "loss =  0.038783744\n",
      "eq1_loss =  0.0022636733\n",
      "eq2_loss =  0.00032107643\n",
      "bc1_loss =  0.02618846\n",
      "bc2_loss =  0.00015955475\n",
      "ic1_loss =  0.009452078\n",
      "ic2_loss =  0.00039890522\n",
      "L2_err_2=  0.0010833556991645692\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "N_t_fin =  10\n",
      "x_train =  torch.Size([380, 1])\n",
      "t_train =  torch.Size([380, 1])\n",
      "\n",
      "epoch =  0\n",
      "loss =  0.019675687\n",
      "eq1_loss =  0.0033050615\n",
      "eq2_loss =  0.000632652\n",
      "bc1_loss =  0.005934427\n",
      "bc2_loss =  0.00016492396\n",
      "ic1_loss =  0.009259745\n",
      "ic2_loss =  0.00037887765\n",
      "L2_err_2=  0.0008678269096140711\n",
      "\n",
      "time =  1347.0523982048035\n",
      "\n",
      "epoch =  400\n",
      "loss =  0.014619017\n",
      "eq1_loss =  0.002191099\n",
      "eq2_loss =  0.0007858563\n",
      "bc1_loss =  0.007427661\n",
      "bc2_loss =  0.0003766922\n",
      "ic1_loss =  0.0035565377\n",
      "ic2_loss =  0.00028117088\n",
      "L2_err_2=  0.0010305935974098247\n",
      "\n",
      "epoch =  800\n",
      "loss =  0.012738146\n",
      "eq1_loss =  0.0020570888\n",
      "eq2_loss =  0.00029067474\n",
      "bc1_loss =  0.0072426787\n",
      "bc2_loss =  0.0002565237\n",
      "ic1_loss =  0.0027165464\n",
      "ic2_loss =  0.00017463432\n",
      "L2_err_2=  0.001063568849707424\n",
      "\n",
      "time =  1377.5015635490417\n",
      "\n",
      "epoch =  1200\n",
      "loss =  0.012259073\n",
      "eq1_loss =  0.0019451285\n",
      "eq2_loss =  0.00027375168\n",
      "bc1_loss =  0.00704553\n",
      "bc2_loss =  0.0002542137\n",
      "ic1_loss =  0.0025682133\n",
      "ic2_loss =  0.00017223568\n",
      "L2_err_2=  0.0010748087085516165\n",
      "\n",
      "epoch =  1600\n",
      "loss =  0.011578664\n",
      "eq1_loss =  0.0018667352\n",
      "eq2_loss =  0.00023231893\n",
      "bc1_loss =  0.006919657\n",
      "bc2_loss =  0.00019515843\n",
      "ic1_loss =  0.002238732\n",
      "ic2_loss =  0.00012606246\n",
      "L2_err_2=  0.0010519077906066586\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "N_t_fin =  10\n",
      "x_train =  torch.Size([380, 1])\n",
      "t_train =  torch.Size([380, 1])\n",
      "\n",
      "epoch =  0\n",
      "loss =  0.05615961\n",
      "eq1_loss =  0.012872316\n",
      "eq2_loss =  0.0048239254\n",
      "bc1_loss =  0.03601966\n",
      "bc2_loss =  0.0001819631\n",
      "ic1_loss =  0.0021477956\n",
      "ic2_loss =  0.00011395271\n",
      "L2_err_2=  0.0007553017874802109\n",
      "\n",
      "time =  1407.867782831192\n",
      "\n",
      "epoch =  400\n",
      "loss =  0.03501796\n",
      "eq1_loss =  0.0020765627\n",
      "eq2_loss =  0.00037623118\n",
      "bc1_loss =  0.025218721\n",
      "bc2_loss =  0.00022275482\n",
      "ic1_loss =  0.006859007\n",
      "ic2_loss =  0.00026468033\n",
      "L2_err_2=  0.0010249652682570532\n",
      "\n",
      "epoch =  800\n",
      "loss =  0.034217164\n",
      "eq1_loss =  0.0020415422\n",
      "eq2_loss =  0.00032476886\n",
      "bc1_loss =  0.024392994\n",
      "bc2_loss =  0.00019472791\n",
      "ic1_loss =  0.0069913403\n",
      "ic2_loss =  0.0002717901\n",
      "L2_err_2=  0.0009496950471889961\n",
      "\n",
      "time =  1438.1554474830627\n",
      "\n",
      "epoch =  1200\n",
      "loss =  0.037106503\n",
      "eq1_loss =  0.0020333934\n",
      "eq2_loss =  0.0037956948\n",
      "bc1_loss =  0.02361\n",
      "bc2_loss =  0.00018515112\n",
      "ic1_loss =  0.007214624\n",
      "ic2_loss =  0.00026763798\n",
      "L2_err_2=  0.0009488252406629151\n",
      "\n",
      "epoch =  1600\n",
      "loss =  0.033024814\n",
      "eq1_loss =  0.001998126\n",
      "eq2_loss =  0.00025318746\n",
      "bc1_loss =  0.023239058\n",
      "bc2_loss =  0.00014613844\n",
      "ic1_loss =  0.0071038324\n",
      "ic2_loss =  0.00028447478\n",
      "L2_err_2=  0.0009291754227696455\n",
      "\n",
      "broke inner loop\n",
      "\n",
      "time elapsed =  1468.5258901119232\n"
     ]
    }
   ],
   "source": [
    "N_x = 35\n",
    "N_bc = 101\n",
    "N_ic = 101\n",
    "N_t = 101\n",
    "x_l = 0\n",
    "x_r = 1\n",
    "t_i = 0\n",
    "T_r = 0\n",
    "T_l = 1\n",
    "T_ini = 0\n",
    "s_ini = 0\n",
    "accuracy_cap = 0.0001\n",
    "N_x_test = 100\n",
    "N_t_test = 100\n",
    "t_arr = [0, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "# t_arr = [0, 1]\n",
    "\n",
    "# Neural network params\n",
    "layer_size_1 = [1, 25, 25, 25, 25, 1]\n",
    "layer_size_2 = [1, 25, 25, 25, 25, 1]\n",
    "\n",
    "# material params\n",
    "k1 = 0.05\n",
    "k2 = 0.8\n",
    "\n",
    "# Training data and initial data\n",
    "model = ANN(layer_size_1, layer_size_2)\n",
    "print(model)\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total trainable parameters in the model:\", total_trainable_params)\n",
    "\n",
    "# # Setup Loss function and Optimiser\n",
    "lr = 6e-5\n",
    "epochs = 15000\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training model\n",
    "time_elapsed, loss_store = train_model(model, optimiser, epochs, T_r, T_l, k1, k2, N_x, x_l, x_r, N_t, N_bc, N_ic, t_arr, t_i, T_ini, accuracy_cap, N_x_test, N_t_test, s_ini)\n",
    "print(\"time elapsed = \", time_elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890d895a",
   "metadata": {},
   "source": [
    "# Results Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5b5645f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_an, x_test, t_test = analytical(N_x_test, N_t_test, x_l, x_r, t_i, 1, k1, k2)\n",
    "x_test = np_to_torch(x_test)\n",
    "t_test = np_to_torch(t_test)\n",
    "y_pred,_,_,s_pred,_,_,_ = model(x_test, t_test)\n",
    "x_test = x_test.detach().numpy()\n",
    "y_pred = y_pred.detach().numpy()\n",
    "s_pred = s_pred.detach().numpy()\n",
    "\n",
    "for i in range(N_x_test*N_t_test):\n",
    "    if (s_pred[i]<x_test[i]):\n",
    "        y_pred[i] = 0\n",
    "        \n",
    "# \n",
    "j = 10\n",
    "\n",
    "plt.plot(x_test[j*N_t_test:(j+1)*N_t_test], y_pred[j*N_t_test:(j+1)*N_t_test])\n",
    "plt.plot(x_test[j*N_t_test:(j+1)*N_t_test], y_an[j*N_t_test:(j+1)*N_t_test])\n",
    "plt.legend([\"PINN\", \"Analytical\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2843e3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class weightedTanh(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         ex = torch.exp(0.5*input)\n",
    "#         return 1-2*((ex-1)/(ex+1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89e9a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Training:\n",
    "#     def __init__(self, model, optimiser1, optimiser2, epochs, T_r, T_l, k1, N_x, x_l, x_r, N_t, N_bc, N_ic, t_arr, t_i, T_ini):\n",
    "        \n",
    "#         self.model = model\n",
    "#         self.epochs = epochs\n",
    "#         self.T_r = T_r\n",
    "#         self.T_l = T_l\n",
    "#         self.k1 = k1\n",
    "#         self.N_x = N_x\n",
    "#         self.x_l = x_l\n",
    "#         self.x_r = x_r\n",
    "#         self.N_t = N_t\n",
    "#         self.N_bc = N_bc\n",
    "#         self.N_ic = N_ic\n",
    "#         self.t_arr = t_arr\n",
    "#         self.t_i = t_i\n",
    "#         self.T_ini = T_ini\n",
    "#         self.w1 = 1\n",
    "#         self.w2 = 1\n",
    "#         self.w3 = 1\n",
    "#         self.w4 = 1\n",
    "#         self.N_tot = self.N_x*self.N_t + 2*self.N_bc + self.N_ic\n",
    "#         self.null = torch.zeros(self.N_tot, 1)\n",
    "#         self.mse = nn.MSELoss(reduction='sum')\n",
    "        \n",
    "#     def x_train_data(self):\n",
    "        \n",
    "#         x_train = np.linspace(self.x_l, self.x_r, self.N_x)\n",
    "#         x_train = np.tile(x_train, N_t)\n",
    "#         x_bc1 = np.zeros(self.N_bc)\n",
    "#         x_bc2 = np.ones(self.N_bc)*self.x_r\n",
    "#         x_ic = np.random.uniform(low=self.x_l, high=self.x_r, size=(self.N_ic,))\n",
    "#         x_train = np.concatenate((x_train,x_bc1,x_bc2,x_ic),0)\n",
    "#         x_train = torch.FloatTensor(x_train)\n",
    "#         x_train = x_train.unsqueeze(-1)\n",
    "#         x_train = x_train.clone().detach().requires_grad_(True)\n",
    "#         N_xl = self.mse( torch.where(x_train == self.x_l,1,0), self.null ).detach().numpy().item()\n",
    "#         N_xr = self.mse( torch.where(x_train == self.x_r,1,0), self.null ).detach().numpy().item()\n",
    "    \n",
    "#         return x_train, N_xl, N_xr, a, b\n",
    "    \n",
    "#     def t_train_data(self, t_start, t_end):\n",
    "        \n",
    "#         t_train = np.linspace(t_start, t_end, self.N_t)\n",
    "#         t_train = np.repeat(t_train, self.N_x)\n",
    "#         t_bc1 = np.random.uniform(low = t_start, high= t_end, size=(self.N_bc,))\n",
    "#         t_bc2 = np.random.uniform(low = t_start, high= t_end, size=(self.N_bc,))\n",
    "#         t_ic = np.zeros(self.N_ic)\n",
    "#         t_train = np.concatenate((t_train,t_bc1,t_bc2,t_ic),0)\n",
    "#         t_train = torch.FloatTensor(t_train)\n",
    "#         t_train = t_train.unsqueeze(-1)\n",
    "#         t_train = t_train.clone().detach().requires_grad_(True)\n",
    "    \n",
    "#         return t_train, \n",
    "    \n",
    "#     def get_loss(x_train, t_train, k1, N_tot, N_ic, a, b, T_l, T_r, N_xl, N_xr, x_l, x_r, T_ini, null):\n",
    "\n",
    "#         T, dTdt, d2Tdx2 = model(x_train, t_train)\n",
    "#         eq1 = self.w1*mse(dTdt, self.k1*d2Tdx2)/(self.N_tot)\n",
    "#         ic = self.w2*( mse( torch.mul(a, T - T_l), null ) + mse( torch.mul(b, T - T_ini), null ) )/(N_ic)\n",
    "#         bc1 = w3*mse( torch.mul(torch.where(x_train == x_l,1,0),(T - T_l)), null )/(N_xl)\n",
    "#         bc2 = w4*mse( torch.mul(torch.where(x_train == x_r,1,0),(T - T_r)), null )/(N_xr)\n",
    "#         loss = eq1 + bc1 + bc2 + ic\n",
    "    \n",
    "#         return loss, eq1, bc1, bc2, ic\n",
    "    \n",
    "#     def abcd(self):\n",
    "#         print(self.t_arr[0])\n",
    "#         print(self.t_arr[1])\n",
    "#         t_train = self.t_train_data(self.t_arr[0], self.t_arr[1])\n",
    "#         return t_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d82fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boom = Training(model, optimiser1, optimiser2, epochs, T_r, T_l, k1, N_x, x_l, x_r, N_t, N_bc, N_ic, t_arr, t_i, T_ini)\n",
    "# t_train = boom.abcd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
