{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4ac27d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.init as init\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8819113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_torch(arr):\n",
    "    \n",
    "    arr = torch.FloatTensor(arr)\n",
    "    arr = arr.unsqueeze(-1)\n",
    "    arr = arr.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def x_train_data(N, x_l, x_r):\n",
    "    \n",
    "    x_train = np.linspace(x_l,x_r,N)   \n",
    "    x_train = np.concatenate((x_train,np.ones(1)*x_r,np.ones(1)*x_l),0)\n",
    "    x_train = np_to_torch(x_train)\n",
    "    return x_train\n",
    "\n",
    "def initial_temp(N_tot, T_l, T_r):\n",
    "    \n",
    "    T_prev = np.ones(N_tot)*T_r\n",
    "    T_prev[0] = T_l\n",
    "    T_prev = np_to_torch(T_prev)\n",
    "    \n",
    "    return T_prev\n",
    "\n",
    "def initial_interface(N, s):\n",
    "\n",
    "    s_interface = torch.full((N, 1), s)\n",
    "    s_interface = s_interface.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    return s_interface, s_interface\n",
    "            \n",
    "def xavier_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0)\n",
    "    \n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, layer_size):\n",
    "        super(ANN, self).__init__()\n",
    "        \n",
    "        # Fully conected model\n",
    "        modules = []\n",
    "        for i in range(len(layer_size) - 2):\n",
    "            modules.append(nn.Linear(layer_size[i], layer_size[i+1]))  \n",
    "            modules.append(nn.Tanh())\n",
    "        modules.append(nn.Linear(layer_size[-2], layer_size[-1])) \n",
    "\n",
    "        self.fc = nn.Sequential(*modules)\n",
    "#         for layer in self.fc.modules():\n",
    "#             if isinstance(layer, nn.Linear):\n",
    "#                  layer.weight.data.normal_(mean=0, std=0.2)\n",
    "        self.fc.apply(xavier_init)\n",
    "        \n",
    "    def forward(self, x_train, s):\n",
    "        \n",
    "        T = self.fc( x_train )\n",
    "        dTdx = torch.autograd.grad(T, x_train, grad_outputs=torch.ones_like(T), create_graph=True)[0]\n",
    "        d2Tdx2 = torch.autograd.grad(dTdx, x_train, grad_outputs=torch.ones_like(dTdx), create_graph=True)[0]\n",
    "        Ts = self.fc( s )\n",
    "        \n",
    "        return T, dTdx, d2Tdx2, Ts\n",
    "    \n",
    "def get_loss(x_train, k1, k2, T_l, T_r, x_l, x_r, T_prev, del_t, t_test, s, N_eq1):\n",
    "    \n",
    "    mse = nn.MSELoss(reduction='sum')\n",
    "    w1 = 1\n",
    "    w2 = 1\n",
    "    w3 = 1\n",
    "        \n",
    "    T, dTdx, d2Tdx2, Ts  = model(x_train, s)\n",
    "    eq1 = w1*( torch.sum( torch.square( torch.mul(torch.where(x_train <= s,1,0),T - T_prev - del_t*k1*d2Tdx2 ) ) ) )/(N_eq1)\n",
    "    bc1 = w2*torch.sum( torch.square( torch.mul(torch.where(x_train == x_l,1,0),(T - T_l)) ) )\n",
    "    bc2 = w3*torch.sum( torch.square( T_r - Ts ) )\n",
    "\n",
    "    loss = eq1 + bc1 + bc2   \n",
    "    \n",
    "    return loss, eq1, bc1, bc2\n",
    "\n",
    "def print_loss(epoch, loss, eq1, bc1, bc2):\n",
    "    print('epoch = ',epoch)\n",
    "    print('loss = ',loss.detach().numpy())\n",
    "    print('eq1_loss = ',eq1.detach().numpy())\n",
    "    print('bc1_loss = ',bc1.detach().numpy())\n",
    "    print('bc2_loss = ',bc2.detach().numpy())\n",
    "\n",
    "def interface_condition(f, del_t, k2, T, interface_index, T_r, del_x, N_colpts, N_tot, dTdx, d2Tdx2):\n",
    "    \n",
    "#     print(\"Double Derivative = \", d2Tdx2[interface_index - N_colpts - 1][0])\n",
    "#     f_new = f + del_t*k2*( d2Tdx2[interface_index - N_colpts - 1][0] )\n",
    "#     f_new = f + del_t*k2*( T[interface_index - N_colpts - 1][0] - T_r )/del_x**2\n",
    "#     f_new = f + del_t*k2*( -dTdx[interface_index - N_colpts - 1][0])/del_x\n",
    "    f_new = f + del_t*k2*( -dTdx[interface_index + int(f*N_colpts - N_colpts/2)][0])/del_x\n",
    "    \n",
    "    if(f_new>1):\n",
    "        T_new = (f_new - 1)*k1/k2 + T_r\n",
    "        T[interface_index][0] = T_new\n",
    "        print(\"Interface changed, temperature at the interface =  \", T[interface_index][0])     \n",
    "        interface_index += N_colpts + 1\n",
    "        f_new = 0\n",
    "#         f_new = f_new-1\n",
    "#         f_new = 0\n",
    "#         f_new = del_t*k2*( T_new - T_r )/del_x**2\n",
    "    \n",
    "    for j in range(interface_index + int(f_new*N_colpts - N_colpts/2), N_tot):\n",
    "        T[j][0] = T_r\n",
    "        \n",
    "    return torch.FloatTensor(T), interface_index, f_new\n",
    "    \n",
    "def lamb_analytical(k1, k2):\n",
    "    x = []\n",
    "    er = []\n",
    "    cnt = 0\n",
    "    for i in np.arange(0.1, 5, 0.001):\n",
    "        x.append(i)\n",
    "        er.append(math.erf(x[-1]))\n",
    "        cnt = cnt+1\n",
    "\n",
    "    x = np.array(x)\n",
    "    er = np.array(er)\n",
    "    y =[]\n",
    "    y = np.exp(-x*x)/(er*math.sqrt(math.pi))-x*k1/k2\n",
    "\n",
    "    for i in range(1,cnt):\n",
    "        if(y[i]*y[i-1]<0):\n",
    "            lam = x[i]\n",
    "            break\n",
    "    \n",
    "    return lam\n",
    "\n",
    "def analytical(N_x_test, x_test, t_test, T_r, k1, k2, T_l):\n",
    "\n",
    "    x_test = x_test.detach().numpy()\n",
    "    y_an = np.zeros((N_x_test, 1))\n",
    "    lam = lamb_analytical(k1, k2)\n",
    "    s = np.sqrt(k1*t_test)*2*lam\n",
    "    \n",
    "    for j in range(N_x_test):\n",
    "        if(x_test[j]<s):\n",
    "            y_an[j] = T_l - T_l*math.erf( x_test[j]/( 2*np.sqrt(k1*t_test) ) )/ math.erf(lam) \n",
    "        else:\n",
    "            y_an[j] = T_r\n",
    "            \n",
    "    y_an = np.reshape(y_an, (N_x_test, 1))\n",
    "    \n",
    "    return y_an, s\n",
    "    \n",
    "def train_model(model, optimiser1, epochs, T_r, T_l, k1, k2, N_x, x_l, x_r, N_t, N_bc, accuracy_cap, N_x_test, del_t, s_initial, del_x):\n",
    "    \n",
    "    loss_store = []\n",
    "    T_store_pred = []\n",
    "    s_store_pred = []\n",
    "    T_store_an = []\n",
    "    s_store_an = []\n",
    "    t_store = []\n",
    "    mse = nn.MSELoss(reduction='sum')\n",
    "    model.train()  \n",
    "    \n",
    "    N_gridpts = int((x_r - x_l)/del_x + 1)\n",
    "    print(\"gridpoints = \",N_gridpts)\n",
    "    N_colpts = 13\n",
    "    N_tot = (N_colpts + 1)*(N_gridpts - 1) + 1 \n",
    "    print(\"N_tot = \", N_tot)\n",
    "    \n",
    "    t_test = 0\n",
    "    t_store.append(0)\n",
    "    s_store_an.append(0)\n",
    "    \n",
    "    #Initial conditions\n",
    "    interface_index = N_colpts + 1\n",
    "    x_train = x_train_data(N_tot, x_l, x_r)\n",
    "    T_prev = initial_temp(N_tot, T_l, T_r)\n",
    "    s_prev = np_to_torch( x_train[interface_index].detach().numpy() )\n",
    "    f_liq = 0\n",
    "    for i in range(N_t):\n",
    "        \n",
    "        t_test = t_test + del_t\n",
    "        t_store.append(t_test)\n",
    "        print(\"t = \", t_test)\n",
    "        print(\" \")\n",
    "        N_eq1 = interface_index + 1\n",
    "        \n",
    "        if i>2:\n",
    "            epochs = 1501\n",
    "            \n",
    "        print(\"epochs = \", epochs)\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            #Backpropogation and optimisation\n",
    "            loss, eq1, bc1, bc2 = get_loss(x_train, k1, k2, T_l, T_r, x_l, x_r, T_prev, del_t, t_test, x_train[interface_index + int(f_liq*N_colpts - N_colpts/2)], N_eq1)\n",
    "            optimiser1.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser1.step()  \n",
    "            loss_store.append(loss.detach().numpy())\n",
    "            \n",
    "            if epoch%2000==0:\n",
    "                print_loss(epoch, loss, eq1, bc1, bc2)\n",
    "                print(\"\")\n",
    "\n",
    "#             if loss<accuracy_cap :\n",
    "#                 print(\"loss limit attained, epoch = \", epoch)\n",
    "#                 break\n",
    "                    \n",
    "        # Store the results after each time step\n",
    "        T_prev, dTdx_prev, d2Tdx2_prev,_ = model(x_train, s_prev)\n",
    "        T_prev, interface_index, f_liq = interface_condition(f_liq, del_t, k2, T_prev.detach().numpy(), interface_index, T_r, del_x, N_colpts, N_tot, dTdx_prev.detach().numpy(), d2Tdx2_prev.detach().numpy())\n",
    "        s_prev = torch.ones(1,1)*x_train[interface_index + int(f_liq*N_colpts - N_colpts/2)][0].detach().numpy().item()\n",
    "        print(\"f_liq = \", f_liq)\n",
    "        print(\"interface_PINN = \", s_prev[0][0].detach().numpy())\n",
    "        T_an, s_an = analytical(N_tot, x_train, t_test, T_r, k1, k2, T_l)\n",
    "        print(\"interface_Analytical = \", s_an)\n",
    "        \n",
    "        T_store_pred.append(T_prev.detach().numpy())\n",
    "        T_store_an.append(T_an)\n",
    "        s_store_pred.append(s_prev.detach().numpy())\n",
    "        s_store_an.append(s_an)\n",
    "        \n",
    "        T_prev = torch.FloatTensor(T_store_pred[-1]).clone().detach().requires_grad_(False)\n",
    "        s_prev = torch.FloatTensor(s_store_pred[-1]).clone().detach().requires_grad_(True)\n",
    "        \n",
    "        print(\"broke inner loop\")\n",
    "        print(\"\")\n",
    "\n",
    "    return loss_store, T_store_pred, T_store_an, x_train, s_store_pred, s_store_an, t_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa5eeff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=1, out_features=3, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=3, out_features=3, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=3, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Total trainable parameters in the model: 34\n",
      "gridpoints =  21\n",
      "N_tot =  281\n",
      "t =  0.001\n",
      " \n",
      "epochs =  50001\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (283) must match the size of tensor b (281) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20436/1200619824.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;31m# Training model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mloss_store\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_store_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_store_an\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_np\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_an\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimiser1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_bc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_cap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_x_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdel_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms_initial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdel_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mtime_elapsed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20436/615932567.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimiser1, epochs, T_r, T_l, k1, k2, N_x, x_l, x_r, N_t, N_bc, accuracy_cap, N_x_test, del_t, s_initial, del_x)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[1;31m#Backpropogation and optimisation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meq1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbc1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbc2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdel_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minterface_index\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_liq\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mN_colpts\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mN_colpts\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_eq1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m             \u001b[0moptimiser1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20436/615932567.py\u001b[0m in \u001b[0;36mget_loss\u001b[1;34m(x_train, k1, k2, T_l, T_r, x_l, x_r, T_prev, del_t, t_test, s, N_eq1)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdTdx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md2Tdx2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTs\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[0meq1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mT_prev\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdel_t\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mk1\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0md2Tdx2\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_eq1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[0mbc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mx_l\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mT_l\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[0mbc2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw3\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mT_r\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mTs\u001b[0m \u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (283) must match the size of tensor b (281) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "N_x = 3001\n",
    "N_bc = 30\n",
    "N_t = 150\n",
    "del_t = 0.001\n",
    "del_x = 0.01\n",
    "x_l = 0\n",
    "x_r = 0.2\n",
    "T_r = 0\n",
    "T_l = 1\n",
    "t_i = 0\n",
    "accuracy_cap = 0.0004\n",
    "N_x_test = 251\n",
    "s_initial = 0.0015\n",
    "\n",
    "# Neural network params\n",
    "layer_size = [1, 3, 3, 3, 1]\n",
    "\n",
    "# material params\n",
    "k1 = 0.01\n",
    "k2 = 0.005\n",
    "\n",
    "# Training data and initial data\n",
    "model = ANN(layer_size)\n",
    "print(model)\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total trainable parameters in the model:\", total_trainable_params)\n",
    "\n",
    "# # Setup Loss function and Optimiser\n",
    "lr = 2e-4\n",
    "epochs = 50001\n",
    "optimiser1 = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training model\n",
    "start = time.time()\n",
    "loss_store, T_store_pred, T_store_an, x_test_np, s_pred, s_an, t = train_model(model, optimiser1, epochs, T_r, T_l, k1, k2, N_x, x_l, x_r, N_t, N_bc, accuracy_cap, N_x_test, del_t, s_initial, del_x)\n",
    "end = time.time()\n",
    "time_elapsed = end - start\n",
    "print(\"time elapsed = \", time_elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb512346",
   "metadata": {},
   "source": [
    "# Results Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6502e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 149\n",
    "i = 0\n",
    "k = 301\n",
    "\n",
    "plt.plot(x_test_np[i:k].detach().numpy(), T_store_pred[j][i:k])\n",
    "plt.plot(x_test_np[i:k].detach().numpy(), T_store_an[j][i:k])\n",
    "Title = \"Stefan 1-phase using Time stepping, \" + \"time = \" + str(\"{:.3f}\".format((j+1)*del_t)) + \", delta_t = \" + str(\"{:.3f}\".format(del_t))\n",
    "plt.title(Title)\n",
    "plt.legend([\"PINN\", \"Analytical\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4b708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = []\n",
    "for i in range(len(s_pred)):\n",
    "    tp.append(s_pred[i][0])\n",
    "\n",
    "i = 0\n",
    "j = 149\n",
    "plt.plot(t[i:j], tp[i:j])\n",
    "plt.plot(t[i:j], s_an[i:j])\n",
    "plt.legend([\"PINN\", \"Analytical\"])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Interface position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21effa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_store)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
