{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4ac27d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.init as init\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8819113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_to_torch(arr):\n",
    "    \n",
    "    arr = torch.FloatTensor(arr)\n",
    "    arr = arr.unsqueeze(-1)\n",
    "    arr = arr.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def x_train_data(N_x, x_l, x_r, N_bc, N_ic, N_t, null):\n",
    "\n",
    "    mse = nn.MSELoss(reduction='sum')\n",
    "    x_train = np.geomspace(x_l+0.001, x_r, N_x)\n",
    "    x_train = np.tile(x_train, N_t)\n",
    "    x_bc1 = np.zeros(N_bc)\n",
    "    x_bc2 = np.ones(N_bc)*x_r\n",
    "    x_ic = np.random.uniform(low=x_l, high=x_r, size=(N_ic,))\n",
    "    x_train = np.concatenate((x_train,x_bc1,x_bc2,x_ic),0)\n",
    "    x_train = np_to_torch(x_train)\n",
    "    N_xl = mse( torch.where(x_train == x_l,1,0), null ).detach().numpy().item()\n",
    "    N_xr = mse( torch.where(x_train == x_r,1,0), null ).detach().numpy().item()\n",
    "    \n",
    "    return x_train, N_xl, N_xr\n",
    "\n",
    "def t_train_data(t_i,t_f,N_t,N_x,N_bc,N_ic):\n",
    "    \n",
    "    t_train = np.linspace(t_i, t_f, N_t)\n",
    "    t_train = np.repeat(t_train, N_x)\n",
    "    t_bc1 = np.random.uniform(low=t_i, high=t_f, size=(N_bc,))\n",
    "    t_bc2 = np.random.uniform(low=t_i, high=t_f, size=(N_bc,))\n",
    "    t_ic = np.zeros(N_ic)\n",
    "    t_train = np.concatenate((t_train,t_bc1,t_bc2,t_ic),0)\n",
    "    t_train = np_to_torch(t_train)\n",
    "\n",
    "    return t_train\n",
    "            \n",
    "def xavier_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0.05)\n",
    "    \n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, layer_size):\n",
    "        super(ANN, self).__init__()\n",
    "        \n",
    "        # Fully conected model\n",
    "        modules = []\n",
    "        for i in range(len(layer_size) - 1):\n",
    "            modules.append(nn.Linear(layer_size[i], layer_size[i+1]))  \n",
    "            modules.append(nn.Tanh())\n",
    "\n",
    "        self.fc = nn.Sequential(*modules)\n",
    "        self.fc.apply(xavier_init)\n",
    "        \n",
    "    def forward(self, x_train, t_train):\n",
    "        op = self.fc( torch.cat((x_train, t_train), 1) )\n",
    "        op_x = torch.autograd.grad(op, x_train, grad_outputs=torch.ones_like(op), create_graph=True)[0]\n",
    "        op_x2 = torch.autograd.grad(op_x, x_train, grad_outputs=torch.ones_like(op_x), create_graph=True)[0]\n",
    "        op_t = torch.autograd.grad(op, t_train, grad_outputs=torch.ones_like(op), create_graph=True)[0]\n",
    "        return op, op_t, op_x2\n",
    "    \n",
    "def get_loss(x_train, t_train, k1, N_tot, N_ic, T_l, T_r, N_xl, N_xr, x_l, x_r, T_ini, null):\n",
    "    \n",
    "    mse = nn.MSELoss(reduction='sum')\n",
    "    \n",
    "    w1 = 1\n",
    "    w2 = 1\n",
    "    w3 = 1\n",
    "    w4 = 1\n",
    "    T, dTdt, d2Tdx2 = model(x_train, t_train)\n",
    "    eq1 = w1*mse(dTdt, k1*d2Tdx2)/(N_tot)\n",
    "    ic = w2*( mse( torch.mul( torch.where(t_train == 0,1,0), (T - T_ini) ), null ) )/(N_ic)\n",
    "    bc1 = w3*mse( torch.mul(torch.where(x_train == x_l,1,0),(T - T_l)), null )/(N_xl)\n",
    "    bc2 = w4*mse( torch.mul(torch.where(x_train == x_r,1,0),(T - T_r)), null )/(N_xr)\n",
    "    loss = eq1 + bc1 + bc2 + ic\n",
    "    \n",
    "    return loss, eq1, bc1, bc2, ic\n",
    "\n",
    "def print_loss(epoch, loss, eq1, bc1, bc2, ic):\n",
    "    print('epoch = ',epoch)\n",
    "    print('loss = ',loss.detach().numpy())\n",
    "    print('eq1_loss = ',eq1.detach().numpy())\n",
    "    print('bc1_loss = ',bc1.detach().numpy())\n",
    "    print('bc2_loss = ',bc2.detach().numpy())\n",
    "    print('ic_loss = ',ic.detach().numpy())\n",
    "    \n",
    "def L2_err(N_x_test, N_t_test, x_test, t_test, y_an, model):\n",
    "    \n",
    "    x_test = np_to_torch(x_test)\n",
    "    t_test = np_to_torch(t_test)\n",
    "    y_pred,_,_  = model(x_test, t_test)\n",
    "    y_pred = y_pred.detach().numpy()\n",
    "\n",
    "    L2_err =  np.sum((y_an - y_pred)**2)/(N_x_test*N_t_test)\n",
    "    return L2_err\n",
    "\n",
    "def analytical(N_x_test, N_t_test, x_l, x_r, t_f):\n",
    "    \n",
    "    x_test = np.linspace(x_l, x_r, N_x_test)\n",
    "    x_test = np.tile(x_test, N_t_test)\n",
    "    t_test = np.linspace(0, t_f, N_t_test)\n",
    "    t_test = np.repeat(t_test, N_x_test)\n",
    "\n",
    "    i = 1\n",
    "    y_an = 0\n",
    "    for i in range(1, 49, 1): \n",
    "        y_an += -2*T_l*(-1)**(i+1)/(i*np.pi)*np.sin(i*np.pi*(1 - x_test))*np.exp(-i**2*np.pi**2*k1*t_test)   \n",
    "    y_an += T_l*(1 - x_test)\n",
    "    \n",
    "    y_an = y_an.reshape(N_x_test*N_t_test,1)\n",
    "    \n",
    "    return y_an, x_test, t_test\n",
    "    \n",
    "    \n",
    "def train_model(model, optimiser1, epochs, T_r, T_l, k1, N_x, x_l, x_r, N_t, N_bc, N_ic, t_arr, t_i, T_ini, accuracy_cap,  N_x_test, N_t_test):\n",
    "    \n",
    "    loss_store = []\n",
    "    mse = nn.MSELoss(reduction='sum')\n",
    "    model.train()  \n",
    "    \n",
    "    start = time.time()\n",
    "    y_an1, x_test1, t_test1 = analytical(N_x_test, N_t_test, x_l, x_r, t_arr[-1])\n",
    "    for j in range(len(t_arr)-1):\n",
    "        \n",
    "        if(j==0):\n",
    "            epochs = 15000\n",
    "        else:\n",
    "            epochs = 2000\n",
    "            \n",
    "        N_t_fin = int( ( t_arr[j+1] - t_arr[j] )*N_t )\n",
    "        N_bc_fin = int( ( t_arr[j+1] - t_arr[j] )*N_bc )\n",
    "        N_ic_fin = int( ( t_arr[j+1] - t_arr[j] )*N_ic )\n",
    "        print(\"N_t_fin = \", N_t_fin)\n",
    "            \n",
    "        # Training data    \n",
    "        N_tot = N_x*N_t_fin + 2*N_bc_fin + N_ic_fin\n",
    "        null = torch.zeros(N_tot).unsqueeze(-1)\n",
    "        x_train, N_xl, N_xr = x_train_data(N_x, x_l, x_r, N_bc_fin, N_ic_fin, N_t_fin, null)\n",
    "        t_train = t_train_data(0,t_arr[j+1],N_t_fin,N_x,N_bc_fin,N_ic_fin)\n",
    "        N_ic_fin = mse( torch.where(t_train == t_i,1,0) , null).detach().numpy().item()\n",
    "        print(\"x_train = \", x_train.shape)\n",
    "        print(\"t_train = \", t_train.shape)\n",
    "        print(\"\")\n",
    "        \n",
    "        # Test Data\n",
    "        N_t_test_2 = int( N_t_test*( t_arr[j+1] - t_arr[j] ) )\n",
    "        y_an2, x_test2, t_test2 = analytical(N_x_test, N_t_test_2, x_l, x_r, t_arr[j+1])\n",
    "\n",
    "        # Adam optimiser loop\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            #Backpropogation and optimisation\n",
    "            loss, eq1, bc1, bc2, ic = get_loss(x_train, t_train, k1, N_tot, N_ic_fin,T_l, T_r, N_xl, N_xr, x_l, x_r, T_ini, null)\n",
    "            optimiser1.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser1.step()  \n",
    "            loss_store.append(loss.detach().numpy())\n",
    "\n",
    "            L2_norm_err_1 = L2_err(N_x_test, N_t_test, x_test1, t_test1, y_an1, model)\n",
    "#             L2_norm_err_2 = L2_err(N_x_test, N_t_test_2, x_test2, t_test2, y_an2, model)\n",
    "            \n",
    "            if epoch%400==0:\n",
    "                print_loss(epoch, loss, eq1, bc1, bc2, ic)\n",
    "                print(\"L2_err_1= \", L2_norm_err_1 )\n",
    "#                 print(\"L2_err_2= \", L2_norm_err_2 )\n",
    "                print(\"\")\n",
    "                \n",
    "            if epoch%1000==0:\n",
    "                print(\"time = \", time.time() - start)\n",
    "                print(\"\")\n",
    "            \n",
    "            \n",
    "            if L2_norm_err_1<accuracy_cap: \n",
    "                print(\"loss limit attained, epoch = \", epoch)\n",
    "                print(\"\")\n",
    "                break\n",
    "            \n",
    "        print(\"broke inner loop\")\n",
    "        print(\"\")\n",
    "        \n",
    "#         if(L2_norm_err_1<accuracy_cap):\n",
    "#             break\n",
    "        \n",
    "    end = time.time()\n",
    "\n",
    "    return end-start, loss_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fa5eeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN(\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=35, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=35, out_features=35, bias=True)\n",
      "    (3): Tanh()\n",
      "    (4): Linear(in_features=35, out_features=35, bias=True)\n",
      "    (5): Tanh()\n",
      "    (6): Linear(in_features=35, out_features=35, bias=True)\n",
      "    (7): Tanh()\n",
      "    (8): Linear(in_features=35, out_features=35, bias=True)\n",
      "    (9): Tanh()\n",
      "    (10): Linear(in_features=35, out_features=35, bias=True)\n",
      "    (11): Tanh()\n",
      "    (12): Linear(in_features=35, out_features=1, bias=True)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n",
      "Total trainable parameters in the model: 6441\n",
      "N_t_fin =  101\n",
      "x_train =  torch.Size([3838, 1])\n",
      "t_train =  torch.Size([3838, 1])\n",
      "\n",
      "epoch =  0\n",
      "loss =  1.1152796\n",
      "eq1_loss =  0.02119454\n",
      "bc1_loss =  1.0460241\n",
      "bc2_loss =  0.04341016\n",
      "ic_loss =  0.0046507083\n",
      "L2_err_1=  0.1059186302547811\n",
      "\n",
      "time =  0.14086389541625977\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9044/340818185.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# Training model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mtime_elapsed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_store\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_bc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_ic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_ini\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_cap\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_x_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_t_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"time elapsed = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_elapsed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9044/561299491.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimiser1, epochs, T_r, T_l, k1, N_x, x_l, x_r, N_t, N_bc, N_ic, t_arr, t_i, T_ini, accuracy_cap, N_x_test, N_t_test)\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[0mloss_store\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m             \u001b[0mL2_norm_err_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL2_err\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_x_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_t_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_test1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_an1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;31m#             L2_norm_err_2 = L2_err(N_x_test, N_t_test_2, x_test2, t_test2, y_an2, model)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9044/561299491.py\u001b[0m in \u001b[0;36mL2_err\u001b[1;34m(N_x_test, N_t_test, x_test, t_test, y_an, model)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0mt_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp_to_torch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[0mL2_err\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_an\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_x_test\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mN_t_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_x = 35\n",
    "N_bc = 101\n",
    "N_ic = 101\n",
    "N_t = 101\n",
    "x_l = 0\n",
    "x_r = 1\n",
    "t_i = 0\n",
    "T_r = 0\n",
    "T_l = 1\n",
    "T_ini = 0\n",
    "accuracy_cap = 0.0001\n",
    "N_x_test = 100\n",
    "N_t_test = 100\n",
    "t_arr = [0, 1]\n",
    "\n",
    "# Neural network params\n",
    "layer_size = [2, 35, 35, 35, 35, 35, 35, 1]\n",
    "\n",
    "# material params\n",
    "k1 = 0.05\n",
    "\n",
    "# # Training data and initial data\n",
    "model = ANN(layer_size)\n",
    "print(model)\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total trainable parameters in the model:\", total_trainable_params)\n",
    "\n",
    "# # Setup Loss function and Optimiser\n",
    "# lr = 6e-5\n",
    "lr = 1e-4\n",
    "epochs = 15000\n",
    "optimiser = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# Training model\n",
    "time_elapsed, loss_store = train_model(model, optimiser, epochs, T_r, T_l, k1, N_x, x_l, x_r, N_t, N_bc, N_ic, t_arr, t_i, T_ini, accuracy_cap, N_x_test, N_t_test)\n",
    "print(\"time elapsed = \", time_elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890d895a",
   "metadata": {},
   "source": [
    "# Results Plotter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5645f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_an, x_test, t_test = analytical(N_x_test, N_t_test, x_l, x_r, 1)\n",
    "x_test = np_to_torch(x_test)\n",
    "t_test = np_to_torch(t_test)\n",
    "y_pred, dTdt, d2Tdx2 = model(x_test, t_test)\n",
    "x_test = x_test.detach().numpy()\n",
    "y_pred = y_pred.detach().numpy()\n",
    "# \n",
    "j = 90\n",
    "\n",
    "plt.plot(x_test[j*N_t_test:(j+1)*N_t_test], y_pred[j*N_t_test:(j+1)*N_t_test])\n",
    "plt.plot(x_test[j*N_t_test:(j+1)*N_t_test], y_an[j*N_t_test:(j+1)*N_t_test])\n",
    "plt.legend([\"PINN\", \"Analytical\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91243fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e9749e69d0>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "j = 0\n",
    "y_an, x_test, t_test = analytical(N_x_test, N_t_test, x_l, x_r, 1)\n",
    "plt.plot(x_test[j*N_t_test:(j+1)*N_t_test], y_an[j*N_t_test:(j+1)*N_t_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc790078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00],\n",
       "       [ 1.40325479e-01],\n",
       "       [-1.78601539e-01],\n",
       "       [-3.68885874e-02],\n",
       "       [ 9.64206950e-02],\n",
       "       [ 2.24677590e-02],\n",
       "       [-6.50536154e-02],\n",
       "       [-1.79388232e-02],\n",
       "       [ 4.85522276e-02],\n",
       "       [ 1.59468512e-02],\n",
       "       [-3.83320755e-02],\n",
       "       [-1.48759376e-02],\n",
       "       [ 3.13356798e-02],\n",
       "       [ 1.42115439e-02],\n",
       "       [-2.62090982e-02],\n",
       "       [-1.37497120e-02],\n",
       "       [ 2.22626959e-02],\n",
       "       [ 1.33968281e-02],\n",
       "       [-1.91089750e-02],\n",
       "       [-1.31051490e-02],\n",
       "       [ 1.65137589e-02],\n",
       "       [ 1.28483167e-02],\n",
       "       [-1.43273471e-02],\n",
       "       [-1.26108591e-02],\n",
       "       [ 1.24496713e-02],\n",
       "       [ 1.23832444e-02],\n",
       "       [-1.08113667e-02],\n",
       "       [-1.21593727e-02],\n",
       "       [ 9.36288479e-03],\n",
       "       [ 1.19352221e-02],\n",
       "       [-8.06792748e-03],\n",
       "       [-1.17080814e-02],\n",
       "       [ 6.89932742e-03],\n",
       "       [ 1.14760952e-02],\n",
       "       [-5.83637364e-03],\n",
       "       [-1.12379844e-02],\n",
       "       [ 4.86302412e-03],\n",
       "       [ 1.09928687e-02],\n",
       "       [-3.96668002e-03],\n",
       "       [-1.07401513e-02],\n",
       "       [ 3.13732621e-03],\n",
       "       [ 1.04794404e-02],\n",
       "       [-2.36691630e-03],\n",
       "       [-1.02104973e-02],\n",
       "       [ 1.64892480e-03],\n",
       "       [ 9.93319841e-03],\n",
       "       [-9.78015733e-04],\n",
       "       [-9.64750915e-03],\n",
       "       [ 3.49793785e-04],\n",
       "       [ 9.35346495e-03],\n",
       "       [ 2.39384970e-04],\n",
       "       [-9.05115703e-03],\n",
       "       [-7.92558813e-04],\n",
       "       [ 8.74072189e-03],\n",
       "       [ 1.31227382e-03],\n",
       "       [-8.42233336e-03],\n",
       "       [-1.80067305e-03],\n",
       "       [ 8.09619641e-03],\n",
       "       [ 2.25956732e-03],\n",
       "       [-7.76254242e-03],\n",
       "       [-2.69049186e-03],\n",
       "       [ 7.42162531e-03],\n",
       "       [ 3.09475197e-03],\n",
       "       [-7.07371852e-03],\n",
       "       [-3.47346015e-03],\n",
       "       [ 6.71911256e-03],\n",
       "       [ 3.82756649e-03],\n",
       "       [-6.35811288e-03],\n",
       "       [-4.15788356e-03],\n",
       "       [ 5.99103823e-03],\n",
       "       [ 4.46510713e-03],\n",
       "       [-5.61821913e-03],\n",
       "       [-4.74983332e-03],\n",
       "       [ 5.23999664e-03],\n",
       "       [ 5.01257290e-03],\n",
       "       [-4.85672123e-03],\n",
       "       [-5.25376332e-03],\n",
       "       [ 4.46875181e-03],\n",
       "       [ 5.47377877e-03],\n",
       "       [-4.07645483e-03],\n",
       "       [-5.67293866e-03],\n",
       "       [ 3.68020346e-03],\n",
       "       [ 5.85151476e-03],\n",
       "       [-3.28037684e-03],\n",
       "       [-6.00973717e-03],\n",
       "       [ 2.87735942e-03],\n",
       "       [ 6.14779941e-03],\n",
       "       [-2.47154022e-03],\n",
       "       [-6.26586256e-03],\n",
       "       [ 2.06331225e-03],\n",
       "       [ 6.36405879e-03],\n",
       "       [-1.65307191e-03],\n",
       "       [-6.44249417e-03],\n",
       "       [ 1.24121837e-03],\n",
       "       [ 6.50125096e-03],\n",
       "       [-8.28152997e-04],\n",
       "       [-6.54038935e-03],\n",
       "       [ 4.14278833e-04],\n",
       "       [ 6.55994880e-03],\n",
       "       [ 0.00000000e+00]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_an[j*N_t_test:(j+1)*N_t_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2843e3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class weightedTanh(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         ex = torch.exp(0.5*input)\n",
    "#         return 1-2*((ex-1)/(ex+1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e9a68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Training:\n",
    "#     def __init__(self, model, optimiser1, optimiser2, epochs, T_r, T_l, k1, N_x, x_l, x_r, N_t, N_bc, N_ic, t_arr, t_i, T_ini):\n",
    "        \n",
    "#         self.model = model\n",
    "#         self.epochs = epochs\n",
    "#         self.T_r = T_r\n",
    "#         self.T_l = T_l\n",
    "#         self.k1 = k1\n",
    "#         self.N_x = N_x\n",
    "#         self.x_l = x_l\n",
    "#         self.x_r = x_r\n",
    "#         self.N_t = N_t\n",
    "#         self.N_bc = N_bc\n",
    "#         self.N_ic = N_ic\n",
    "#         self.t_arr = t_arr\n",
    "#         self.t_i = t_i\n",
    "#         self.T_ini = T_ini\n",
    "#         self.w1 = 1\n",
    "#         self.w2 = 1\n",
    "#         self.w3 = 1\n",
    "#         self.w4 = 1\n",
    "#         self.N_tot = self.N_x*self.N_t + 2*self.N_bc + self.N_ic\n",
    "#         self.null = torch.zeros(self.N_tot, 1)\n",
    "#         self.mse = nn.MSELoss(reduction='sum')\n",
    "        \n",
    "#     def x_train_data(self):\n",
    "        \n",
    "#         x_train = np.linspace(self.x_l, self.x_r, self.N_x)\n",
    "#         x_train = np.tile(x_train, N_t)\n",
    "#         x_bc1 = np.zeros(self.N_bc)\n",
    "#         x_bc2 = np.ones(self.N_bc)*self.x_r\n",
    "#         x_ic = np.random.uniform(low=self.x_l, high=self.x_r, size=(self.N_ic,))\n",
    "#         x_train = np.concatenate((x_train,x_bc1,x_bc2,x_ic),0)\n",
    "#         x_train = torch.FloatTensor(x_train)\n",
    "#         x_train = x_train.unsqueeze(-1)\n",
    "#         x_train = x_train.clone().detach().requires_grad_(True)\n",
    "#         N_xl = self.mse( torch.where(x_train == self.x_l,1,0), self.null ).detach().numpy().item()\n",
    "#         N_xr = self.mse( torch.where(x_train == self.x_r,1,0), self.null ).detach().numpy().item()\n",
    "    \n",
    "#         return x_train, N_xl, N_xr, a, b\n",
    "    \n",
    "#     def t_train_data(self, t_start, t_end):\n",
    "        \n",
    "#         t_train = np.linspace(t_start, t_end, self.N_t)\n",
    "#         t_train = np.repeat(t_train, self.N_x)\n",
    "#         t_bc1 = np.random.uniform(low = t_start, high= t_end, size=(self.N_bc,))\n",
    "#         t_bc2 = np.random.uniform(low = t_start, high= t_end, size=(self.N_bc,))\n",
    "#         t_ic = np.zeros(self.N_ic)\n",
    "#         t_train = np.concatenate((t_train,t_bc1,t_bc2,t_ic),0)\n",
    "#         t_train = torch.FloatTensor(t_train)\n",
    "#         t_train = t_train.unsqueeze(-1)\n",
    "#         t_train = t_train.clone().detach().requires_grad_(True)\n",
    "    \n",
    "#         return t_train, \n",
    "    \n",
    "#     def get_loss(x_train, t_train, k1, N_tot, N_ic, a, b, T_l, T_r, N_xl, N_xr, x_l, x_r, T_ini, null):\n",
    "\n",
    "#         T, dTdt, d2Tdx2 = model(x_train, t_train)\n",
    "#         eq1 = self.w1*mse(dTdt, self.k1*d2Tdx2)/(self.N_tot)\n",
    "#         ic = self.w2*( mse( torch.mul(a, T - T_l), null ) + mse( torch.mul(b, T - T_ini), null ) )/(N_ic)\n",
    "#         bc1 = w3*mse( torch.mul(torch.where(x_train == x_l,1,0),(T - T_l)), null )/(N_xl)\n",
    "#         bc2 = w4*mse( torch.mul(torch.where(x_train == x_r,1,0),(T - T_r)), null )/(N_xr)\n",
    "#         loss = eq1 + bc1 + bc2 + ic\n",
    "    \n",
    "#         return loss, eq1, bc1, bc2, ic\n",
    "    \n",
    "#     def abcd(self):\n",
    "#         print(self.t_arr[0])\n",
    "#         print(self.t_arr[1])\n",
    "#         t_train = self.t_train_data(self.t_arr[0], self.t_arr[1])\n",
    "#         return t_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d82fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# boom = Training(model, optimiser1, optimiser2, epochs, T_r, T_l, k1, N_x, x_l, x_r, N_t, N_bc, N_ic, t_arr, t_i, T_ini)\n",
    "# t_train = boom.abcd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
