{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7466475",
   "metadata": {},
   "source": [
    "# dy/dx + 2x = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e3c1947a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n",
      "9500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15988/2715822178.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m \u001b[0mwts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mx_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_neural_network_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mN1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mN1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mN1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'NN'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15988/2715822178.py\u001b[0m in \u001b[0;36mtrain_neural_network_batch\u001b[1;34m()\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;31m#             _, l = sess.run([optimizer,cost], feed_dict={x_ph:x})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m             \u001b[0mu1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx_ph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m             \u001b[0mv1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0my_ph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mu1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;31m#             _, l = sess.run([optimizer,cost], feed_dict={x_ph:v1})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 970\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    971\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    972\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1191\u001b[0m     \u001b[1;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1193\u001b[1;33m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[0;32m   1194\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[0;32m   1195\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1373\u001b[1;33m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[0;32m   1374\u001b[0m                            run_metadata)\n\u001b[0;32m   1375\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1378\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1379\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1380\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1381\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1382\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1362\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1363\u001b[1;33m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[0;32m   1364\u001b[0m                                       target_list, run_metadata)\n\u001b[0;32m   1365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1454\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1455\u001b[0m                           run_metadata):\n\u001b[1;32m-> 1456\u001b[1;33m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[0;32m   1457\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1458\u001b[0m                                             run_metadata)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import math, random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import special\n",
    "\n",
    "# Define the number of outputs and the learning rate\n",
    "n_input = 1\n",
    "n_nodes_hl1 = 50\n",
    "n_nodes_hl2 = 50\n",
    "n_nodes_hl3 = 50\n",
    "n_nodes_hl4 = 50\n",
    "n_nodes_hl5 = 50\n",
    "n_nodes_hl6 = 50\n",
    "n_nodes_hl7 = 50\n",
    "n_nodes_hl8 = 50\n",
    "n_output = 1\n",
    "learn_rate = 0.0002\n",
    "\n",
    "# Boundary Conditions\n",
    "A = 0\n",
    "\n",
    "# training data\n",
    "N = 9000\n",
    "N1 = 500 # Number of datapoints to describe B.C's\n",
    "a = 0\n",
    "b = 1\n",
    "diff = (b-a)/N\n",
    "x = []\n",
    "x1 = []\n",
    "for i in range(N):\n",
    "    x1.append(a+i*diff) \n",
    "    x.append(random.uniform(0, 1))\n",
    "    if(i==N-1):\n",
    "        for j in range(N1):\n",
    "            x.append(1)\n",
    "  \n",
    "x1 = np.array(x1)\n",
    "x1 = x1.reshape((N,1))\n",
    "N = N + N1            \n",
    "x = np.array(x)\n",
    "x = x.reshape((N,1))\n",
    "\n",
    "# Placeholders\n",
    "x_ph = tf.placeholder('float', [None, 1],name='input')\n",
    "y_ph = tf.placeholder('float', [None, 1],name='input')\n",
    "\n",
    "# number of epochs\n",
    "n_epochs = 500\n",
    "\n",
    "# Define standard deviation for initialising weights and biases from normal distribution.\n",
    "hl_sigma = 0.02\n",
    "\n",
    "def neural_network_model(data):\n",
    "    \n",
    "    data = tf.cast(data, tf.float32)\n",
    "    hidden_1_layer = {'weights': tf.Variable(tf.random.normal([n_input, n_nodes_hl1],stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl1], stddev=hl_sigma))}\n",
    "    hidden_2_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl1, n_nodes_hl2], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl2], stddev=hl_sigma))}\n",
    "    hidden_3_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl2, n_nodes_hl3], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl3], stddev=hl_sigma))}\n",
    "    hidden_4_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl3, n_nodes_hl4], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl4], stddev=hl_sigma))}\n",
    "    hidden_5_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl4, n_nodes_hl5], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl5], stddev=hl_sigma))}\n",
    "    hidden_6_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl5, n_nodes_hl6], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl6], stddev=hl_sigma))}\n",
    "    hidden_7_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl6, n_nodes_hl7], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl7], stddev=hl_sigma))}\n",
    "    hidden_8_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl7, n_nodes_hl8], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl8], stddev=hl_sigma))}\n",
    "    output_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl8, n_output], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_output], stddev=hl_sigma))}\n",
    "           \n",
    "    l1 = tf.add(tf.matmul(data, hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.tanh(l1)   \n",
    "    l2 = tf.add(tf.matmul(l1, hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.tanh(l2)\n",
    "    l3 = tf.add(tf.matmul(l2, hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.tanh(l3)\n",
    "    l4 = tf.add(tf.matmul(l3, hidden_4_layer['weights']), hidden_4_layer['biases'])\n",
    "    l4 = tf.nn.tanh(l4)\n",
    "    l5 = tf.add(tf.matmul(l4, hidden_5_layer['weights']), hidden_5_layer['biases'])\n",
    "    l5 = tf.nn.tanh(l5)\n",
    "    l6 = tf.add(tf.matmul(l5, hidden_6_layer['weights']), hidden_6_layer['biases'])\n",
    "    l6 = tf.nn.tanh(l6)\n",
    "    l7 = tf.add(tf.matmul(l6, hidden_7_layer['weights']), hidden_7_layer['biases'])\n",
    "    l7 = tf.nn.tanh(l7)\n",
    "    l8 = tf.add(tf.matmul(l7, hidden_8_layer['weights']), hidden_8_layer['biases'])\n",
    "    l8 = tf.nn.tanh(l8)\n",
    "    output = tf.add(tf.matmul(l8, output_layer['weights']), output_layer['biases'], name='output')\n",
    "    return tf.concat([tf.reshape(hidden_1_layer['weights'], [50]),tf.reshape(hidden_2_layer['weights'], [2500])], 0),output\n",
    "\n",
    "def train_neural_network_batch():\n",
    "    wts, u = neural_network_model(x_ph)  \n",
    "    dudx = tf.gradients(u, x_ph)\n",
    "    wts, v = neural_network_model(y_ph)\n",
    "    \n",
    "    cost = tf.reduce_mean(tf.square(dudx + 2*x_ph) + tf.square(tf.cast(tf.equal(x_ph,1), tf.float32)*(u-A)) )\n",
    "    optimizer = tf.train.AdamOptimizer(learn_rate).minimize(cost)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "#             _, l = sess.run([optimizer,cost], feed_dict={x_ph:x})\n",
    "            u1 = sess.run(u, feed_dict={x_ph:x})        \n",
    "            v1 = sess.run(v, feed_dict={y_ph:u1})\n",
    "            _, l = sess.run([optimizer,cost], feed_dict={x_ph:v1})\n",
    "            print(len(v1))\n",
    "#             print(l)\n",
    "#             y = u.eval(session=sess)\n",
    "            \n",
    "#             if(epoch % 100 == 0):\n",
    "#                 print('loss:-',l,', epoch:-',epoch)\n",
    "#                 print(type(u))\n",
    "#                 print(sess.run(wts))\n",
    "\n",
    "        # Validation\n",
    "        return sess.run(wts), sess.run(tf.squeeze(u),{x_ph:x1}), x1\n",
    "    \n",
    "                \n",
    "wts, y_pred ,x_pred = train_neural_network_batch()\n",
    "y_pred = y_pred.reshape(N-N1,1)\n",
    "plt.plot(x_pred[0:N-N1], y_pred[0:N-N1], label ='NN')\n",
    "plt.plot(x_pred[0:N-N1], -x_pred[0:N-N1]*x_pred[0:N-N1]+1, label ='Analytical')\n",
    "plt.title(\"dT/dx + 2x = 0\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7aa11ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[253.72934 261.653   254.63487 ... 254.88412 262.72888 261.74814]\n",
      " [254.19106 262.34357 256.26352 ... 256.995   266.9292  269.97113]\n",
      " [241.87312 248.87424 245.28119 ... 252.10767 250.17346 256.80096]\n",
      " ...\n",
      " [252.92992 258.8196  252.55267 ... 262.6327  259.4683  265.95425]\n",
      " [254.6813  257.7068  251.2738  ... 255.58766 259.11414 262.31842]\n",
      " [247.67007 253.18756 246.97809 ... 259.94043 259.9941  258.73502]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.compat.v1.placeholder(tf.float32, shape=(1024, 1024))\n",
    "y = tf.matmul(x, x)\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "#     print(sess.run(y))  # ERROR: will fail because x was not fed.\n",
    "\n",
    "    rand_array = np.random.rand(1024, 1024)\n",
    "    print(sess.run(y, feed_dict={x: rand_array}))  # Will succeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ef542d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41061c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor to array\n",
    "a = tf.constant([[1, 2], [3, 4]])\n",
    "a.eval(session=tf.compat.v1.Session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4fcf735c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02144775 0.72002786]\n",
      " [0.39256777 0.39280851]]\n",
      "[[0.2831197  0.29827604]\n",
      " [0.16262364 0.43695822]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# tensor to array\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "a = tf.compat.v1.placeholder(tf.float32, shape=(2, 2))\n",
    "b = tf.matmul(a, a)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x = np.random.rand(2, 2)\n",
    "    print(x)\n",
    "    c = sess.run(b, feed_dict={a:x})\n",
    "    print(c)\n",
    "    print(type(c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a140aaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wt = wts.reshape(50,1)\n",
    "wts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f2de90",
   "metadata": {},
   "source": [
    "# d2y/dx2 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4a0915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import math, random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import special\n",
    "\n",
    "# Define the number of outputs and the learning rate\n",
    "n_input = 1\n",
    "n_nodes_hl1 = 50\n",
    "n_nodes_hl2 = 50\n",
    "n_nodes_hl3 = 50\n",
    "n_nodes_hl4 = 50\n",
    "n_nodes_hl5 = 50\n",
    "n_nodes_hl6 = 50\n",
    "n_nodes_hl7 = 50\n",
    "n_nodes_hl8 = 50\n",
    "n_output = 1\n",
    "learn_rate = 0.0002\n",
    "\n",
    "# Boundary Conditions\n",
    "A = 1\n",
    "B = 0\n",
    "\n",
    "# training data\n",
    "N = 9000\n",
    "N1 = 500 # Number of datapoints to describe B.C's\n",
    "a = 0\n",
    "b = 1\n",
    "diff = (b-a)/N\n",
    "x = []\n",
    "x1 = []\n",
    "for i in range(N):\n",
    "    x1.append(a+i*diff) \n",
    "    x.append(random.uniform(0, 1))\n",
    "    if(i==N-1):\n",
    "        for j in range(N1):\n",
    "            x.append(0)\n",
    "            x.append(1)\n",
    "            \n",
    "x1 = np.array(x1)\n",
    "x1 = x1.reshape((N,1))\n",
    "N = N + 2*N1            \n",
    "x = np.array(x)\n",
    "x = x.reshape((N,1))\n",
    "\n",
    "# Placeholders\n",
    "x_ph = tf.placeholder('float', [None, 1],name='input')\n",
    "y_ph = tf.placeholder('float')\n",
    "\n",
    "# number of epochs\n",
    "n_epochs = 5000\n",
    "\n",
    "# Define standard deviation for initialising weights and biases from normal distribution.\n",
    "hl_sigma = 0.02\n",
    "\n",
    "def neural_network_model(data):\n",
    "    \n",
    "    data = tf.cast(data, tf.float32)\n",
    "    hidden_1_layer = {'weights': tf.Variable(tf.random.normal([n_input, n_nodes_hl1],stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl1], stddev=hl_sigma))}\n",
    "    hidden_2_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl1, n_nodes_hl2], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl2], stddev=hl_sigma))}\n",
    "    hidden_3_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl2, n_nodes_hl3], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl3], stddev=hl_sigma))}\n",
    "    hidden_4_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl3, n_nodes_hl4], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl4], stddev=hl_sigma))}\n",
    "    hidden_5_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl4, n_nodes_hl5], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl5], stddev=hl_sigma))}\n",
    "    hidden_6_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl5, n_nodes_hl6], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl6], stddev=hl_sigma))}\n",
    "    hidden_7_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl6, n_nodes_hl7], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl7], stddev=hl_sigma))}\n",
    "    hidden_8_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl7, n_nodes_hl8], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl8], stddev=hl_sigma))}\n",
    "    output_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl8, n_output], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_output], stddev=hl_sigma))}\n",
    "           \n",
    "    l1 = tf.add(tf.matmul(data, hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.tanh(l1)   \n",
    "    l2 = tf.add(tf.matmul(l1, hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.tanh(l2)\n",
    "    l3 = tf.add(tf.matmul(l2, hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.tanh(l3)\n",
    "    l4 = tf.add(tf.matmul(l3, hidden_4_layer['weights']), hidden_4_layer['biases'])\n",
    "    l4 = tf.nn.tanh(l4)\n",
    "    l5 = tf.add(tf.matmul(l4, hidden_5_layer['weights']), hidden_5_layer['biases'])\n",
    "    l5 = tf.nn.tanh(l5)\n",
    "    l6 = tf.add(tf.matmul(l5, hidden_6_layer['weights']), hidden_6_layer['biases'])\n",
    "    l6 = tf.nn.tanh(l6)\n",
    "    l7 = tf.add(tf.matmul(l6, hidden_7_layer['weights']), hidden_7_layer['biases'])\n",
    "    l7 = tf.nn.tanh(l7)\n",
    "    l8 = tf.add(tf.matmul(l7, hidden_8_layer['weights']), hidden_8_layer['biases'])\n",
    "    l8 = tf.nn.tanh(l8)\n",
    "    output = tf.add(tf.matmul(l8, output_layer['weights']), output_layer['biases'], name='output')\n",
    "    return hidden_1_layer['weights'], output\n",
    "\n",
    "def train_neural_network_batch():\n",
    "    wts, u = neural_network_model(x_ph)  \n",
    "    dudx = tf.gradients(u, x_ph)\n",
    "    dudx2 = tf.gradients(tf.gradients(u, x_ph), x_ph)\n",
    "    \n",
    "    \n",
    "    cost = tf.reduce_mean(tf.square(dudx2) +  tf.square(tf.cast(tf.equal(x_ph,0), tf.float32)*(u-A)) + tf.square(tf.cast(tf.equal(x_ph,1), tf.float32)*(u-B)) )\n",
    "    optimizer = tf.train.AdamOptimizer(learn_rate).minimize(cost)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            _, l = sess.run([optimizer,cost], feed_dict={x_ph:x})\n",
    "            \n",
    "            if(epoch % 100 == 0):\n",
    "                print('loss:-',l,', epoch:-',epoch)\n",
    "                if(l<=0.00005):\n",
    "                    break\n",
    "\n",
    "        # Validation\n",
    "        return wts, sess.run(tf.squeeze(u),{x_ph:x1}), x1\n",
    "    \n",
    "                \n",
    "wts, y_pred ,x_pred = train_neural_network_batch()\n",
    "y_pred = y_pred.reshape(N-2*N1,1)\n",
    "plt.plot(x_pred[0:N-2*N1], y_pred[0:N-2*N1], label ='NN')\n",
    "plt.plot(x_pred[0:N-2*N1], x_pred[0:N-2*N1]*(B-A) + A, label ='Analytical')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfd7963",
   "metadata": {},
   "source": [
    "# k*d2T/dx2 + s = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ac698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import math, random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import special\n",
    "\n",
    "# Define the number of outputs and the learning rate\n",
    "n_input = 1\n",
    "n_nodes_hl1 = 50\n",
    "n_nodes_hl2 = 50\n",
    "n_nodes_hl3 = 50\n",
    "n_nodes_hl4 = 50\n",
    "n_nodes_hl5 = 50\n",
    "n_nodes_hl6 = 50\n",
    "n_nodes_hl7 = 50\n",
    "n_nodes_hl8 = 50\n",
    "n_output = 1\n",
    "learn_rate = 0.0006\n",
    "\n",
    "# Boundary Conditions\n",
    "A = 0\n",
    "B = 1\n",
    "C = 2.0\n",
    "\n",
    "# training data\n",
    "N = 9000\n",
    "N1 = 900 # Number of datapoints to describe B.C's\n",
    "a = 0\n",
    "b = 1\n",
    "diff = (b-a)/N\n",
    "x = []\n",
    "for i in range(N):\n",
    "    x.append(a+i*diff)  \n",
    "    if(i==N-1):\n",
    "        for j in range(N1):\n",
    "            x.append(0)\n",
    "            x.append(1)\n",
    "            \n",
    "N = N + 2*N1            \n",
    "x = np.array(x)\n",
    "x = x.reshape((N,1))\n",
    "\n",
    "# Placeholders\n",
    "x_ph = tf.placeholder('float', [None, 1],name='input')\n",
    "y_ph = tf.placeholder('float')\n",
    "\n",
    "# number of epochs\n",
    "n_epochs = 5000\n",
    "\n",
    "# Define standard deviation for initialising weights and biases from normal distribution.\n",
    "hl_sigma = 0.02\n",
    "me = 0.0\n",
    "def neural_network_model(data):\n",
    "    \n",
    "    data = tf.cast(data, tf.float32)\n",
    "    hidden_1_layer = {'weights': tf.Variable(tf.random.normal([n_input, n_nodes_hl1], mean = me, stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl1], mean = me, stddev=hl_sigma))}\n",
    "    hidden_2_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl1, n_nodes_hl2], mean = me, stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl2], mean = me, stddev=hl_sigma))}\n",
    "    hidden_3_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl2, n_nodes_hl3], mean = me, stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl3], mean = me, stddev=hl_sigma))}\n",
    "    hidden_4_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl3, n_nodes_hl4], mean = me, stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl4], mean = me, stddev=hl_sigma))}\n",
    "    hidden_5_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl4, n_nodes_hl5], mean = me, stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl5], mean = me, stddev=hl_sigma))}\n",
    "    hidden_6_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl5, n_nodes_hl6], mean = me, stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl6], mean = me, stddev=hl_sigma))}\n",
    "    hidden_7_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl6, n_nodes_hl7], mean = me, stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl7], mean = me, stddev=hl_sigma))}\n",
    "    hidden_8_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl7, n_nodes_hl8], mean = me, stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl8], mean = me, stddev=hl_sigma))}\n",
    "    output_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl8, n_output], mean = me, stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_output], mean = me, stddev=hl_sigma))}\n",
    "           \n",
    "    l1 = tf.add(tf.matmul(data, hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.tanh(l1)   \n",
    "    l2 = tf.add(tf.matmul(l1, hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.tanh(l2)\n",
    "    l3 = tf.add(tf.matmul(l2, hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.tanh(l3)\n",
    "    l4 = tf.add(tf.matmul(l3, hidden_4_layer['weights']), hidden_4_layer['biases'])\n",
    "    l4 = tf.nn.tanh(l4)\n",
    "    l5 = tf.add(tf.matmul(l4, hidden_5_layer['weights']), hidden_5_layer['biases'])\n",
    "    l5 = tf.nn.tanh(l5)\n",
    "    l6 = tf.add(tf.matmul(l5, hidden_6_layer['weights']), hidden_6_layer['biases'])\n",
    "    l6 = tf.nn.tanh(l6)\n",
    "    l7 = tf.add(tf.matmul(l6, hidden_7_layer['weights']), hidden_7_layer['biases'])\n",
    "    l7 = tf.nn.tanh(l7)\n",
    "    l8 = tf.add(tf.matmul(l7, hidden_8_layer['weights']), hidden_8_layer['biases'])\n",
    "    l8 = tf.nn.tanh(l8)\n",
    "    output = tf.add(tf.matmul(l8, output_layer['weights']), output_layer['biases'], name='output')\n",
    "    return output\n",
    "\n",
    "def train_neural_network_batch():\n",
    "    u = neural_network_model(x_ph)  \n",
    "    dudx = tf.gradients(u, x_ph)\n",
    "    dudx2 = tf.gradients(tf.gradients(u, x_ph), x_ph)\n",
    "    \n",
    "    \n",
    "    cost = tf.reduce_mean(tf.square(dudx2 + tf.constant(C)) +  tf.square(tf.cast(tf.equal(x_ph,0), tf.float32)*(u-A)) + tf.square(tf.cast(tf.equal(x_ph,1), tf.float32)*(u-B)) )\n",
    "    optimizer = tf.train.AdamOptimizer(learn_rate).minimize(cost)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            _, l = sess.run([optimizer,cost], feed_dict={x_ph:x})\n",
    "            \n",
    "            if(epoch % 100 == 0):\n",
    "                print('loss:-',l,', epoch:-',epoch)\n",
    "                if(l<=0.0006):\n",
    "                    break\n",
    "\n",
    "        # Validation\n",
    "        return sess.run(tf.squeeze(u),{x_ph:x}), x\n",
    "    \n",
    "                \n",
    "y_pred ,x_pred = train_neural_network_batch()\n",
    "y_pred = y_pred.reshape(N,1)\n",
    "plt.plot(x_pred[0:N-2*N1], y_pred[0:N-2*N1], label ='NN')\n",
    "plt.plot(x_pred[0:N-2*N1], -C*x_pred[0:N-2*N1]*x_pred[0:N-2*N1]/2 + (B - A + C/2)*x_pred[0:N-2*N1] + A, label ='Analytical')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9196a360",
   "metadata": {},
   "source": [
    "# dT/dt - (alpha)*(d2T/dX2) = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db46572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import math, random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import special\n",
    "\n",
    "# Define the number of outputs and the learning rate\n",
    "n_input = 2\n",
    "n_nodes_hl1 = 30\n",
    "n_nodes_hl2 = 30\n",
    "n_nodes_hl3 = 30\n",
    "n_nodes_hl4 = 30\n",
    "n_nodes_hl5 = 30\n",
    "n_nodes_hl6 = 30\n",
    "n_nodes_hl7 = 30\n",
    "n_nodes_hl8 = 30\n",
    "n_output = 1\n",
    "learn_rate = 0.0002\n",
    "\n",
    "# Boundary Conditions\n",
    "A = 0\n",
    "B = 1\n",
    "C = 0.1\n",
    "T = 1\n",
    "\n",
    "# training data\n",
    "N = 9000\n",
    "N1 = 100 # Number of datapoints to describe B.C's\n",
    "a = 0\n",
    "b = 1\n",
    "diff = (b-a)/N\n",
    "x = []\n",
    "t = []\n",
    "x1 = []\n",
    "t1 = []\n",
    "for i in range(N):\n",
    "    x1.append(a+i*diff) \n",
    "    t1.append(T)\n",
    "    x.append(random.uniform(0, 1))\n",
    "    t.append(random.uniform(0, 1))\n",
    "    if(i==N-1):\n",
    "        for j in range(N1):\n",
    "            x.append(0)\n",
    "            t.append(random.uniform(0, 1))\n",
    "            x.append(1)\n",
    "            t.append(random.uniform(0, 1))\n",
    "            x.append(random.uniform(0, 1))\n",
    "            t.append(0)\n",
    "            \n",
    "x1 = np.array(x1)\n",
    "x1 = x1.reshape((N,1))\n",
    "t1 = np.array(t1)\n",
    "t1 = t1.reshape((N,1))\n",
    "N = N + 3*N1            \n",
    "x = np.array(x)\n",
    "x = x.reshape((N,1))\n",
    "t = np.array(t)\n",
    "t = t.reshape((N,1))\n",
    "\n",
    "# Placeholders\n",
    "x_ph = tf.placeholder('float', [None, 1],name='input')\n",
    "t_ph = tf.placeholder('float', [None, 1],name='input')\n",
    "y_ph = tf.placeholder('float')\n",
    "\n",
    "# number of epochs\n",
    "n_epochs = 10000\n",
    "\n",
    "# Define standard deviation for initialising weights and biases from normal distribution.\n",
    "hl_sigma = 0.1\n",
    "\n",
    "def neural_network_model(x2, t2):\n",
    "    data = tf.concat([x2, t2], 1)\n",
    "    data = tf.cast(data, tf.float32)\n",
    "    hidden_1_layer = {'weights': tf.Variable(tf.random.normal([n_input, n_nodes_hl1],stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl1], stddev=hl_sigma))}\n",
    "    hidden_2_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl1, n_nodes_hl2], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl2], stddev=hl_sigma))}\n",
    "    hidden_3_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl2, n_nodes_hl3], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl3], stddev=hl_sigma))}\n",
    "    hidden_4_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl3, n_nodes_hl4], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl4], stddev=hl_sigma))}\n",
    "    hidden_5_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl4, n_nodes_hl5], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl5], stddev=hl_sigma))}\n",
    "    hidden_6_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl5, n_nodes_hl6], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl6], stddev=hl_sigma))}\n",
    "    hidden_7_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl6, n_nodes_hl7], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl7], stddev=hl_sigma))}\n",
    "    hidden_8_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl7, n_nodes_hl8], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl8], stddev=hl_sigma))}\n",
    "    output_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl8, n_output], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_output], stddev=hl_sigma))}\n",
    "           \n",
    "    l1 = tf.add(tf.matmul(data, hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.tanh(l1)   \n",
    "    l2 = tf.add(tf.matmul(l1, hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.tanh(l2)\n",
    "    l3 = tf.add(tf.matmul(l2, hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.tanh(l3)\n",
    "    l4 = tf.add(tf.matmul(l3, hidden_4_layer['weights']), hidden_4_layer['biases'])\n",
    "    l4 = tf.nn.tanh(l4)\n",
    "    l5 = tf.add(tf.matmul(l4, hidden_5_layer['weights']), hidden_5_layer['biases'])\n",
    "    l5 = tf.nn.tanh(l5)\n",
    "    l6 = tf.add(tf.matmul(l5, hidden_6_layer['weights']), hidden_6_layer['biases'])\n",
    "    l6 = tf.nn.tanh(l6)\n",
    "    l7 = tf.add(tf.matmul(l6, hidden_7_layer['weights']), hidden_7_layer['biases'])\n",
    "    l7 = tf.nn.tanh(l7)\n",
    "    l8 = tf.add(tf.matmul(l7, hidden_8_layer['weights']), hidden_8_layer['biases'])\n",
    "    l8 = tf.nn.tanh(l8)\n",
    "    output = tf.add(tf.matmul(l8, output_layer['weights']), output_layer['biases'], name='output')\n",
    "    return output\n",
    "\n",
    "def train_neural_network_batch():\n",
    "    u = neural_network_model(x_ph,t_ph)  \n",
    "    dudx = tf.gradients(u, x_ph)\n",
    "    dudx2 = tf.gradients(tf.gradients(u, x_ph), x_ph)\n",
    "    dudt = tf.gradients(u, t_ph)\n",
    "    cost = tf.reduce_mean(tf.square(dudt-tf.constant(C)*dudx2) + tf.square(tf.cast(tf.equal(x_ph,0), tf.float32)*(u-A)) + tf.square(tf.cast(tf.equal(x_ph,1), tf.float32)*(u-B)) + tf.square(tf.cast(tf.equal(t_ph,0), tf.float32)*(u-A)))\n",
    "    optimizer = tf.train.AdamOptimizer(learn_rate).minimize(cost)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            _, l = sess.run([optimizer,cost], feed_dict={x_ph:x, t_ph:t})\n",
    "            \n",
    "            if(epoch % 100 == 0):\n",
    "                print('loss:-',l,', epoch:-',epoch)\n",
    "                if(l<=0.00005):\n",
    "                    break\n",
    "\n",
    "        # Validation\n",
    "        return sess.run(tf.squeeze(u),{x_ph:x1, t_ph:t1}), x1\n",
    "    \n",
    "# Run the code                              \n",
    "y_pred ,x_pred = train_neural_network_batch()\n",
    "y_pred = y_pred.reshape(N-3*N1,1)\n",
    "\n",
    "# Analytical\n",
    "y1 = []\n",
    "T = 0.4\n",
    "i = 1\n",
    "y1 = -2*(-1)**(i+1)/(i*np.pi)*np.sin(i*np.pi*x1)*np.exp(-i**2*np.pi**2*C*T)\n",
    "for i in range(2, 49, 1):\n",
    "    y1 += -2*(-1)**(i+1)/(i*np.pi)*np.sin(i*np.pi*x1)*np.exp(-i**2*np.pi**2*C*T)   \n",
    "y1 += x1\n",
    "\n",
    "# Plot\n",
    "plt.plot(x_pred[0:N-3*N1], y_pred[0:N-3*N1], label ='NN')\n",
    "plt.plot(x1, y1, label ='Analytical')\n",
    "plt.title(\"dT/dt - (c)*(d2T/dx2) = 0, c = 0.1, t = 1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b86e412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as  pd\n",
    "final = pd.DataFrame(list(zip(x_pred[0:N-3*N1], y_pred[0:N-3*N1])),columns =['Length','Temperature'])\n",
    "final\n",
    "final.to_excel(\"1-D_Unsteady_Conduction_t=1_A=0_B=1_ini=0.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f9aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import math, random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import special\n",
    "#import tensorflow_probability as tfp\n",
    "\n",
    "# Define the number of outputs and the learning rate\n",
    "n_input = 2\n",
    "n_nodes_hl1 = 30\n",
    "n_nodes_hl2 = 30\n",
    "n_nodes_hl3 = 30\n",
    "n_nodes_hl4 = 30\n",
    "n_nodes_hl5 = 30\n",
    "n_nodes_hl6 = 30\n",
    "n_nodes_hl7 = 30\n",
    "n_nodes_hl8 = 30\n",
    "n_output = 1\n",
    "learn_rate = 0.0002\n",
    "\n",
    "# Boundary Conditions\n",
    "A = 0\n",
    "B = 1\n",
    "C = 0.15\n",
    "T = 0.2\n",
    "\n",
    "# training data\n",
    "N = 9000\n",
    "N1 = 100 # Number of datapoints to describe B.C's\n",
    "a = 0\n",
    "b = 1\n",
    "diff = (b-a)/N\n",
    "x = []\n",
    "t = []\n",
    "x1 = []\n",
    "t1 = []\n",
    "for i in range(N):\n",
    "    x1.append(a+i*diff) \n",
    "    t1.append(T)\n",
    "    x.append(random.uniform(0, 1))\n",
    "    t.append(random.uniform(0, 1))\n",
    "    if(i==N-1):\n",
    "        for j in range(N1):\n",
    "            x.append(0)\n",
    "            t.append(random.uniform(0, 1))\n",
    "            x.append(1)\n",
    "            t.append(random.uniform(0, 1))\n",
    "            x.append(random.uniform(0, 1))\n",
    "            t.append(0)\n",
    "            \n",
    "x1 = np.array(x1)\n",
    "x1 = x1.reshape((N,1))\n",
    "t1 = np.array(t1)\n",
    "t1 = t1.reshape((N,1))\n",
    "N = N + 3*N1            \n",
    "x = np.array(x)\n",
    "x = x.reshape((N,1))\n",
    "t = np.array(t)\n",
    "t = t.reshape((N,1))\n",
    "\n",
    "# Placeholders\n",
    "x_ph = tf.placeholder('float', [None, 1],name='input')\n",
    "t_ph = tf.placeholder('float', [None, 1],name='input')\n",
    "y_ph = tf.placeholder('float')\n",
    "\n",
    "# number of epochs\n",
    "n_epochs = 7000\n",
    "\n",
    "# Define standard deviation for initialising weights and biases from normal distribution.\n",
    "hl_sigma = 0.1\n",
    "\n",
    "def neural_network_model(x2, t2):\n",
    "    data = tf.concat([x2, t2], 1)\n",
    "    data = tf.cast(data, tf.float32)\n",
    "    hidden_1_layer = {'weights': tf.Variable(tf.random.normal([n_input, n_nodes_hl1],stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl1], stddev=hl_sigma))}\n",
    "    hidden_2_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl1, n_nodes_hl2], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl2], stddev=hl_sigma))}\n",
    "    hidden_3_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl2, n_nodes_hl3], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl3], stddev=hl_sigma))}\n",
    "    hidden_4_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl3, n_nodes_hl4], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl4], stddev=hl_sigma))}\n",
    "    hidden_5_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl4, n_nodes_hl5], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl5], stddev=hl_sigma))}\n",
    "    hidden_6_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl5, n_nodes_hl6], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl6], stddev=hl_sigma))}\n",
    "    hidden_7_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl6, n_nodes_hl7], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl7], stddev=hl_sigma))}\n",
    "    hidden_8_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl7, n_nodes_hl8], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_nodes_hl8], stddev=hl_sigma))}\n",
    "    output_layer = {'weights': tf.Variable(tf.random.normal([n_nodes_hl8, n_output], stddev=hl_sigma)),\n",
    "                      'biases': tf.Variable(tf.random.normal([n_output], stddev=hl_sigma))}\n",
    "           \n",
    "    l1 = tf.add(tf.matmul(data, hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    l1 = tf.nn.tanh(l1)   \n",
    "    l2 = tf.add(tf.matmul(l1, hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.tanh(l2)\n",
    "    l3 = tf.add(tf.matmul(l2, hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.tanh(l3)\n",
    "    l4 = tf.add(tf.matmul(l3, hidden_4_layer['weights']), hidden_4_layer['biases'])\n",
    "    l4 = tf.nn.tanh(l4)\n",
    "    l5 = tf.add(tf.matmul(l4, hidden_5_layer['weights']), hidden_5_layer['biases'])\n",
    "    l5 = tf.nn.tanh(l5)\n",
    "    l6 = tf.add(tf.matmul(l5, hidden_6_layer['weights']), hidden_6_layer['biases'])\n",
    "    l6 = tf.nn.tanh(l6)\n",
    "    l7 = tf.add(tf.matmul(l6, hidden_7_layer['weights']), hidden_7_layer['biases'])\n",
    "    l7 = tf.nn.tanh(l7)\n",
    "    l8 = tf.add(tf.matmul(l7, hidden_8_layer['weights']), hidden_8_layer['biases'])\n",
    "    l8 = tf.nn.tanh(l8)\n",
    "    output = tf.add(tf.matmul(l8, output_layer['weights']), output_layer['biases'], name='output')\n",
    "    return output\n",
    "\n",
    "def train_neural_network_batch():\n",
    "    u = neural_network_model(x_ph,t_ph)  \n",
    "    dudx = tf.gradients(u, x_ph)\n",
    "    dudx2 = tf.gradients(tf.gradients(u, x_ph), x_ph)\n",
    "    dudt = tf.gradients(u, t_ph)\n",
    "    cost = tf.reduce_mean(tf.square(dudt-tf.constant(C)*dudx2) + tf.square(tf.cast(tf.equal(x_ph,0), tf.float32)*(u-A)) + tf.square(tf.cast(tf.equal(x_ph,1), tf.float32)*(u-A)) + tf.square(tf.cast(tf.equal(t_ph,0), tf.float32)*(u-B)))\n",
    "    optimizer = tf.train.AdamOptimizer(learn_rate).minimize(cost)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            _, l = sess.run([optimizer,cost], feed_dict={x_ph:x, t_ph:t})\n",
    "            \n",
    "            if(epoch % 100 == 0):\n",
    "                print('loss:-',l,', epoch:-',epoch)\n",
    "                if(l<=0.0005):\n",
    "                    break\n",
    "\n",
    "        # Validation\n",
    "        return sess.run(tf.squeeze(u),{x_ph:x1, t_ph:t1}), x1\n",
    "    \n",
    "                          \n",
    "y_pred ,x_pred = train_neural_network_batch()\n",
    "y_pred = y_pred.reshape(N-3*N1,1)\n",
    "\n",
    "# Analytical Solution\n",
    "y1 = []\n",
    "i = 1\n",
    "y1 = (4/np.pi)*np.sin(i*np.pi*x1)*np.exp(-i**2*np.pi**2*C*T)\n",
    "for i in range(3, 21, 2):\n",
    "    y1 += (4/np.pi)*np.sin(i*np.pi*x1)*np.exp(-i**2*np.pi**2*C*T)\n",
    "\n",
    "# Plot \n",
    "plt.plot(x_pred[0:N-3*N1], y_pred[0:N-3*N1], label ='NN')\n",
    "plt.plot(x1, y1, label ='Analytical')\n",
    "s1 = \"dT/dt - (c)*(d2T/dx2) = 0, c = \"\n",
    "s2 = str(C)\n",
    "s3 = \", t = \"\n",
    "s4 = str(T)\n",
    "plt.title(s1+s2+s3+s4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d88687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = (N-3*N1)/2\n",
    "y_pred[4500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e503d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5dd658",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = []\n",
    "T = 1\n",
    "i = 1\n",
    "y1 = -2*(-1)**(i+1)/(i*np.pi)*np.sin(i*np.pi*x1)*np.exp(-i**2*np.pi**2*C*T)\n",
    "for i in range(2, 49, 1):\n",
    "    y1 += -2*(-1)**(i+1)/(i*np.pi)*np.sin(i*np.pi*x1)*np.exp(-i**2*np.pi**2*C*T)\n",
    "    \n",
    "y1 += x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b12411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_pred[0:N-3*N1], y_pred[0:N-3*N1], label ='NN')\n",
    "plt.plot(x1, y1, label ='Analytical')\n",
    "s1 = \"dT/dt - (c)*(d2T/dx2) = 0, c = \"\n",
    "s2 = str(C)\n",
    "s3 = \", t = \"\n",
    "s4 = str(T)\n",
    "plt.title(s1+s2+s3+s4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f860ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df322f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "y1 = (4/np.pi)*np.sin(i*np.pi*x1)*np.exp(-i**2*np.pi**2*C*T)\n",
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4331654",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "y1 += (4/np.pi)*np.sin(i*np.pi*x1)*np.exp(-i**2*np.pi**2*C*T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3312e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9e00aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
