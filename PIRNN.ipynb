{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd415869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e92d1d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data(N_x, x_l, x_r, N_t, t_i, t_f, del_t, sel_x):\n",
    "\n",
    "    x_train = np.ones((N_x,1,1))\n",
    "    for i in range(N_x):\n",
    "        x_train[i][0][0] = t_i + del_x*i\n",
    "        \n",
    "    t_train = np.ones((N_x,1,1))\n",
    "    null = np.zeros((N_x,1,1))\n",
    "    x_train = torch.FloatTensor(x_train)\n",
    "    x_train = x_train.clone().detach().requires_grad_(True)\n",
    "    t_train = torch.FloatTensor(t_train)\n",
    "    null = torch.FloatTensor(null)\n",
    "    null = null.clone().detach().requires_grad_(True)\n",
    "    return x_train, t_train, null\n",
    "\n",
    "def ini_temp(N_x, T_r, T_l):\n",
    "    u_train = np.zeros(N_x)\n",
    "    u_train[0] = T_l\n",
    "    u_train[-1] = T_r\n",
    "    u_train = torch.FloatTensor(u_train)\n",
    "    u_train = u_train.unsqueeze(1)\n",
    "    u_train = u_train.clone().detach().requires_grad_(True)\n",
    "    return u_train\n",
    "    \n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, rnn_size, rnn_layers, hidden_layers):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        \n",
    "        # RNN model\n",
    "        self.rnn = nn.GRU(input_size, rnn_size, rnn_layers)\n",
    "        \n",
    "        # Fully conected model\n",
    "        modules = []\n",
    "        for i in range(hidden_layers):\n",
    "            if(i == 0):\n",
    "                modules.append(nn.Linear(rnn_size, hidden_size))\n",
    "            else:\n",
    "                modules.append(nn.Linear(hidden_size, hidden_size)) \n",
    "            modules.append(nn.Tanh())\n",
    "            \n",
    "        if(hidden_layers == 0):\n",
    "            hidden_size = rnn_size \n",
    "            \n",
    "        modules.append(nn.Linear(hidden_size, output_size))\n",
    "        modules.append(nn.Tanh())\n",
    "        self.fc = nn.Sequential(*modules)\n",
    "        \n",
    "        # initialise the weights\n",
    "        for layer in self.fc.modules():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.weight.data.normal_(mean=0, std=0.2)\n",
    "\n",
    "        for layer in range(rnn_layers):\n",
    "            for weight in self.rnn._all_weights[layer]:       \n",
    "                if \"weight\" in weight:\n",
    "                    nn.init.xavier_normal_(getattr(self.rnn,weight))\n",
    "\n",
    "    def forward(self, x_train, t_train, t_i, N_t, del_t, N_x):\n",
    "        \n",
    "        self.h0 = torch.zeros(rnn_layers, 1, rnn_size)\n",
    "        \n",
    "        T = torch.zeros((N_t, N_x, 1))\n",
    "        T_xx = torch.zeros((N_t, N_x, 1))\n",
    "        T_t = torch.zeros((N_t, N_x, 1))\n",
    "        for i in range(N_t):\n",
    "            t_train2 = t_train*(t_i + i*del_t)\n",
    "            t_train3 = t_train2.clone().detach().requires_grad_(True)\n",
    "            X = torch.cat((x_train, t_train3), 2)\n",
    "            rnn_output, hidden = self.rnn(X, self.h0)\n",
    "#             print(t_train3)\n",
    "            \n",
    "            op = self.fc( rnn_output[:, -1, :])\n",
    "            op_x = torch.autograd.grad(op, x_train, grad_outputs=torch.ones_like(op), create_graph=True)[0]\n",
    "            op_t = torch.autograd.grad(op, t_train3, grad_outputs=torch.ones_like(op), create_graph=True)[0]\n",
    "            op_xx = torch.autograd.grad(op_x, x_train, grad_outputs=torch.ones_like(op_x), create_graph=True)[0]\n",
    "            op_t = torch.squeeze(op_t, 2)\n",
    "            op_xx = torch.squeeze(op_xx, 2)\n",
    "            \n",
    "            self.h0 = hidden\n",
    "            T[i] = op\n",
    "            T_xx[i] = op_xx\n",
    "            T_t[i] = op_t\n",
    "            \n",
    "        return T, T_xx, T_t, x_train, t_train\n",
    "            \n",
    "def train_model(model, optimiser, epochs, T_ini, null, N_t, T_l, T_r, x_l, x_r, c1, x_train, t_train, N_x):\n",
    "    loss_str = []\n",
    "    iters = []\n",
    "    mse = nn.MSELoss()\n",
    "    model.train()  # Set the model in training model\n",
    "    for epoch in range(epochs):\n",
    "#         print(epoch)\n",
    "        T, T_xx, T_t, x_train2, t_train2 = model(x_train, t_train, t_i, N_t, del_t, N_x)\n",
    "        optimiser.zero_grad() \n",
    "        \n",
    "        eq = 0\n",
    "        bc1 = 0\n",
    "        bc2 = 0\n",
    "        ic = 0\n",
    "        for i in range(N_t):\n",
    "            eq += mse(T_t[i] , c1*T_xx[i])\n",
    "            bc1 += mse( torch.mul(torch.where(x_train2 == x_l,1,0),(T[i] - T_l)), null ) \n",
    "            bc2 += mse( torch.mul(torch.where(x_train2 == x_r,1,0),(T[i] - T_r)), null ) \n",
    "            ic += mse( torch.mul(torch.where(t_train2 == t_i,1,0),(T[i] - T_ini)), null) \n",
    "                \n",
    "        loss = (eq + bc1 + bc2 + ic)/N_t\n",
    "        loss.backward()   \n",
    "        optimiser.step()\n",
    "        loss_str.append(loss.detach().numpy())\n",
    "        iters.append(epoch)\n",
    "        \n",
    "        if epoch%10 == 0:\n",
    "            print('epoch = ',epoch,', loss = ',loss.detach().numpy())\n",
    "            \n",
    "    return iters, loss_str\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94edb0dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNN(\n",
      "  (rnn): GRU(2, 3)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=1, bias=True)\n",
      "    (1): Tanh()\n",
      "  )\n",
      ")\n",
      "Total trainable parameters in the model: 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\loss.py:520: UserWarning: Using a target size (torch.Size([41, 1, 1])) that is different to the input size (torch.Size([41, 41, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =  0 , loss =  0.032342635\n",
      "epoch =  10 , loss =  0.022675488\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1672/3467932216.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0moptimiser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1500\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0miters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_ini\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnull\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_l\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1672/267055105.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, optimiser, epochs, T_ini, null, N_t, T_l, T_r, x_l, x_r, c1, x_train, t_train, N_x)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0meq\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbc1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbc2\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mN_t\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m         \u001b[0moptimiser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mloss_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sanjeet\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training data parameters and B.C\n",
    "N_x = 35\n",
    "N_t = 35\n",
    "x_l = 0\n",
    "x_r = 1\n",
    "t_i = 0\n",
    "t_f = 0.8\n",
    "del_t = (t_f - t_i)/(N_t - 1)\n",
    "del_x = (x_r - x_l)/(N_x - 1)\n",
    "T_l = 1.0\n",
    "T_r = 0.0\n",
    "\n",
    "# Neural network params\n",
    "input_size = 2\n",
    "rnn_size = 3\n",
    "hidden_size = 6\n",
    "output_size = 1\n",
    "hidden_layers = 0\n",
    "rnn_layers = 1\n",
    "\n",
    "# Equation Parameters\n",
    "c1 = 0.1\n",
    "\n",
    "# Training Data\n",
    "x_train, t_train, null = train_data(N_x, x_l, x_r, N_t, t_i, t_f, del_t, del_x)\n",
    "T_ini = ini_temp(N_x, T_r, T_l)\n",
    "model = SimpleRNN(input_size, hidden_size, output_size, rnn_size, rnn_layers, hidden_layers)\n",
    "print(model)\n",
    "total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total trainable parameters in the model:\", total_trainable_params)\n",
    "\n",
    "# Training model\n",
    "learning_rate = 2e-3\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "epochs = 1500\n",
    "iters, loss_str = train_model(model, optimiser, epochs, T_ini, null, N_t, T_l, T_r, x_l, x_r, c1, x_train, t_train, N_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71856041",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
